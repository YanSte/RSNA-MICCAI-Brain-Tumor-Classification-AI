{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yannicksteph/rsna-miccai-brain-tumor-classification?scriptVersionId=131379270\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/yannicksteph/rsna-miccai-brain-tumor-classification\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{"_uuid":"8155ab2b-ec95-4b01-a716-a8fcc889a16e","_cell_guid":"7f6cd229-f6a5-494a-9433-a5058896a9d0","_kg_hide-input":false,"_kg_hide-output":false,"trusted":true}},{"cell_type":"markdown","source":"# üöß Work in progress üöß","metadata":{"_uuid":"6c8d67e3-01dd-444c-b849-56476650f77e","_cell_guid":"26a5ce13-f409-44ed-a82b-a32eba83f26d","trusted":true}},{"cell_type":"markdown","source":"**<center><font size=5>RSNA-MICCAI Brain Tumor Classification</font></center>**\n\n<center><img src=\"https://lingualab.ca/fr/project/language-recovery-psa/featured_hu67ab33455cf328a3b8dbb37d23762824_484672_720x0_resize_lanczos_2.png\" alt=\"equal-2495950-1920\" border=\"0\" width=\"700\"></center>\n\n***\n\n**Table of Contents**\n- <a href='#overview'>1. Project overview and objectives</a> \n    - <a href='#contributors'>1.1. Contributors</a>\n    - <a href='#dataset_overview'>1.2. Data overview</a>\n    - <a href='#definitions'>1.3. Imports, Methods, Paths, Reading definitions</a>\n- <a href='#exploratory_data'>2. Exploratory Data</a>\n    - <a href='#exploratory_data_2_1'>2.1. MRI  scans folders</a>\n    - <a href='#exploratory_data_2_2'>2.2. MRI slides scans previews</a>\n    - <a href='#exploratory_data_2_3'>2.3. MRI and MGMT values previews</a>\n    - <a href='#exploratory_data_2_4'>2.4. Summary</a>\n- <a href='#brain_segmentation'>3. Brain segmentation</a>\n    - <a href='#brain_segmentation_3_1'>3.1. Importance of Brain Segmentation</a>\n    - <a href='#brain_segmentation_3_2'>3.2. Selection Criteria</a>\n    - <a href='#brain_segmentation_3_3'>3.3. Contributions of the Library</a>\n    - <a href='#brain_segmentation_3_4'>3.4. Summary</a>\n- <a href='#utilizing_unet_4'>4. Utilizing the U-Net for Brain MRI Model and RadiomicsShape2D Class</a>\n    - <a href='#utilizing_unet_4_1'>4.1. Architecture</a>\n    - <a href='#utilizing_unet_4_2'>4.2. Usage Instructions</a>\n    - <a href='#utilizing_unet_4_3'>4.3. RadiomicsShape2D Class</a>\n    - <a href='#utilizing_unet_4_4'>4.4. Usage Instructions</a>\n- <a href='#dataset_creation_5'>5. Dataset creation</a>\n- <a href='#exploratory_dataset'>6. Exploratory Dataset</a>\n    - <a href='#exploratory_dataset_6_1'>6.1. Summary</a>\n- <a href='#analysis'>7. Exploratory Dataset</a>\n    - <a href='#analysis_univariee_7_2'>7.2. univariate analysis</a> \n\n# üöß TODO TEAM ADD OTHER SECTION ‚ö†Ô∏è\n\n***","metadata":{"_uuid":"c43415ef-3df5-4fd1-ac0b-bd8d4df441cc","_cell_guid":"48cb0110-daeb-473e-b870-67237f936874","_kg_hide-input":true,"trusted":true}},{"cell_type":"markdown","source":"# <a id='overview'>1. Project overview and objectives</a>\n\n### Overview:\n\nA malignant brain tumor is a life-threatening condition, specifically glioblastoma, which is the most common and has the poorest prognosis among adult brain cancers, with a median survival of less than a year. The presence of a specific genetic sequence called MGMT promoter methylation in the tumor has been identified as a favorable prognostic factor and a strong predictor of chemotherapy responsiveness.\n\nCurrently, the genetic analysis of cancer requires a surgical procedure to obtain a tissue sample, followed by a time-consuming process of determining the genetic characteristics of the tumor, which can take several weeks. Depending on the results and the chosen initial treatment, additional surgeries may be necessary. Developing an accurate method to predict the genetic profile of the cancer solely through imaging (known as radiogenomics) would potentially reduce the number of surgeries and allow for a more tailored therapy approach.\n\nThe Radiological Society of North America (RSNA) and the Medical Image Computing and Computer Assisted Intervention Society (MICCAI Society) have collaborated to enhance the diagnosis and treatment planning for glioblastoma patients.\n\n### Competition:\n\nIn this competition, participants are tasked with using MRI (magnetic resonance imaging) scans to train and test a model that can predict the genetic subtype of glioblastoma by detecting the presence of MGMT promoter methylation.\n\nSuccessful outcomes from this competition could significantly contribute to less invasive diagnoses and treatments for brain cancer patients. Introducing new and personalized treatment strategies before surgery holds the potential to improve the management, survival rates, and overall prospects of individuals affected by brain cancer.\n\n### Acknowledgments:\n\nThe Radiological Society of North America (RSNA¬Æ) is a non-profit organization representing 31 radiologic subspecialties from 145 countries worldwide. RSNA promotes excellence in patient care and healthcare delivery through education, research, and technological innovation.\n\nRSNA provides high-quality educational resources, publishes five top peer-reviewed journals, hosts the world's largest radiology conference, and is dedicated to shaping the future of the profession through the RSNA Research & Education (R&E) Foundation, which has funded $66 million in grants since its establishment. Additionally, RSNA actively supports and facilitates research in medical imaging artificial intelligence (AI) by sponsoring ongoing AI challenge competitions.\n\nThe Medical Image Computing and Computer Assisted Intervention Society (MICCAI Society) is committed to advancing research, education, and practice in the field of medical image computing, computer-assisted interventions, biomedical imaging, and medical robotics. The society achieves this objective by organizing high-quality international conferences, workshops, tutorials, and publications that promote the exchange and dissemination of advanced knowledge, expertise, and experiences produced by leading institutions, scientists, physicians, and educators worldwide.\n\nA complete list of acknowledgments can be found on this page.\n\n[RSNA-MICCAI Brain Tumor Radiogenomic Classification](https://www.kaggle.com/competitions/rsna-miccai-brain-tumor-radiogenomic-classification/data?select=train_labels.csv)\n\n## <a id='contributors'>1.1. Contributors</a>\n\n- [David Goudard](https://www.kaggle.com/goudgoud)\n- [Louis-Marie Renaud](https://www.kaggle.com/louismarierenaud)\n- [Yannick Stephan](https://github.com/YanSteph)\n\n## <a id='dataset_overview'>1.2. Data overview</a>\n\nThe dataset we will be working with consists of MRI data provided by the Radiological Society of North America (RSNA¬Æ) and the Medical Imaging Computation and Computer Assistance Society (MICCAI Society). The images are provided in DICOM format and are accompanied by a CSV file containing radiomic features extracted from the images.\n\nThe **\"train/\"** directory contains the training files with the **\"train_labels.csv\"**\n\nThe **\"test/\"** directory contains the tests files for the competition with the **\"sample_submission.csv\"**\n\nThe files are mpMRI scans, this includes:\n- Fluid Attenuated Inversion Recovery (FLAIR)\n    * What it is: These are images that detect brain abnormalities, such as edema and inflammatory lesions. These images are sensitive to the detection of anomalies related to inflammatory and infectious diseases of the central nervous system.\n    * What it highlights: It helps to detect anomalies in the brain that might not be visible in other MRI sequences.\n    * These images allow for the detection of brain abnormalities related to inflammatory and infectious diseases of the central nervous system.\n- T1-weighted pre-contrast (T1w)\n    * What it is: These are images that highlight soft tissues, such as muscles and nerves, and are useful for visualizing normal brain structures.\n    * What it highlights: It allows the visualization of the normal brain structures and also helps in the detection of tumors and lesions.\n    * These images allow for the detection of brain tumors and lesions.\n- T1-weighted post-contrast (T1Gd)\n    * What it is: These are images that use a contrast agent to detect vascular anomalies, such as tumors and lesions, which are more visible after contrast agent administration.\n    * What it highlights: It enhances the visibility of vascular anomalies, such as tumors and lesions, making it easier to detect them.\n    * These images allow for the detection of vascular anomalies, such as tumors and lesions.\n- T2-weighted (T2)\n    * What it is: These images detect abnormalities related to demyelination, such as multiple sclerosis, as well as brain tumors and lesions.\n    * What it highlights: It helps in the detection of anomalies related to cerebrospinal fluid, such as cysts and brain tumors.\n    * These images allow for the detection of anomalies related to demyelination, brain tumors, lesions, and cerebrospinal fluid.","metadata":{"_uuid":"cf119e7a-ef70-4976-a413-3aec8573f6b4","_cell_guid":"8466a8d1-2048-42c7-99fe-bae11bceadeb","execution":{"iopub.status.busy":"2023-05-14T19:19:16.412499Z","iopub.execute_input":"2023-05-14T19:19:16.41328Z","iopub.status.idle":"2023-05-14T19:19:17.264665Z","shell.execute_reply.started":"2023-05-14T19:19:16.413115Z","shell.execute_reply":"2023-05-14T19:19:17.263011Z"},"trusted":true}},{"cell_type":"markdown","source":"## <a id='definitions'>1.3. Imports, Methods, Paths, Reading definitions</a>","metadata":{"_uuid":"4f991b68-9d67-4116-acaf-c3778c7e77cb","_cell_guid":"4e26ccc0-13bc-43c4-ab66-d921146fa7f1","trusted":true}},{"cell_type":"markdown","source":"### Imports","metadata":{"_uuid":"a4dcfa2e-2aa9-4ea3-a8d9-4e154b2d332e","_cell_guid":"63e63b99-efd0-4e72-b03a-69cdd668b237","trusted":true}},{"cell_type":"code","source":"# Operating System and File System\nimport os \n\n# Data Manipulation and Analysis\nimport numpy as np  \nimport pandas as pd\n\n# Data Visualization\nimport matplotlib.pyplot as plt \n# Animation Matplotlib\nimport matplotlib.animation as anim\nimport seaborn as sns\n\n# Warnings\nimport warnings  # For suppressing warnings\n\n# JSON Handling\nimport json  # For working with JSON data\n\n# Encoding and Decoding Binary Data\nimport base64  # For encoding and decoding binary data\n\n# Interactive Widgets and Display\nimport ipywidgets as widgets  # For creating interactive widgets in Jupyter Notebook\nfrom IPython.display import HTML, display  # For displaying HTML content\nfrom IPython.display import Image as show_gif # GIF\n\n# Deep Learning Framework\nimport torch  # For working with PyTorch deep learning framework\n\n# DICOM File Handling\nimport pydicom  # For reading DICOM files\nfrom pydicom import dcmread  # For reading DICOM files\n\n# Image Processing and Filtering\nimport SimpleITK as sitk  # For image filtering\nfrom PIL import Image  # For image processing using the Python Imaging Library (PIL)\n\n# Machine Learning and Data Splitting\nfrom sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n\nfrom scipy import stats\nfrom statsmodels.stats.diagnostic import lilliefors\nimport statsmodels.api as sm\n\n# Additional Libraries\nimport glob # Fetch data recusif\n!pip install pyradiomics > /dev/null  # Installing the pyradiomics library for radiomics feature extraction\nimport radiomics  # For extracting radiomics features from medical images","metadata":{"_uuid":"a70cd560-0d31-48a6-93e6-43a171aba651","_cell_guid":"540d4ede-cfb8-4dcb-89f3-e4ae8122fa34","_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-05-28T22:45:33.637437Z","iopub.execute_input":"2023-05-28T22:45:33.638009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configure","metadata":{"_uuid":"0fc9720b-0c24-4785-9e99-d1f43a7b80be","_cell_guid":"79534d1b-26e1-4a4c-a314-e81402a24afd","trusted":true}},{"cell_type":"code","source":"# ------------\n# Configuration\n# ------------\n\n# Show all columns\npd.set_option('display.max_columns', None)\n# Suppressing Warnings\nwarnings.filterwarnings('ignore')\n\n# Define \"reader\"\n# Read serie of image files into a SimpleTK image\nsitk_reader = sitk.ImageSeriesReader()\nsitk_reader.LoadPrivateTagsOn()\n\n# ------------\n# Paths\n# ------------\n\npath_rsna_brain_tumor_classification = \"../input/rsna-miccai-brain-tumor-radiogenomic-classification/\"\n\ntrain_path = path_rsna_brain_tumor_classification + \"train/\"\ntrain_label_file = path_rsna_brain_tumor_classification + '/train_labels.csv'\n\n# @TODO define path like genere\npath = \"../input/rsna-miccai-brain-tumor-segmentation-pytorch-unet/\"\ndataset_path = \"/kaggle/input/rsna-miccai-brain-tumor-segmentation-pytorch-unet/rsna_miccai_brain_tumor_brain_segmentation_pytorch_unet.csv\"\n#path + \"rsna_miccai_brain_tumor_brain_segmentation_pytorch_unet.csv/\"\n\n# ------------\n# Segmentation\n# ------------\n\n# Load mateuszbuda/brain-segmentation-pytorch, U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI\nsegmentation_model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', \n                                    'unet', \n                                    in_channels=3, \n                                    out_channels=1, \n                                    init_features=32, \n                                    pretrained=True, \n                                    trust_repo=False)\n\n# ------------\n# Dataset\n# ------------\n\n# Dataset of the project, explanation in next section.\ndataset = pd.read_csv(train_label_file)\nsamp_subm = pd.read_csv(path_rsna_brain_tumor_classification + 'sample_submission.csv')","metadata":{"_uuid":"d8be3601-d445-4247-85ff-165eea20d384","_cell_guid":"c45968e1-81d5-44f9-936c-27df7d3839e2","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdin","text":"The repository mateuszbuda_brain-segmentation-pytorch does not belong to the list of trusted repositories and as such cannot be downloaded. Do you trust this repository and wish to add it to the trusted list of repositories (y/N)? y\n"}]},{"cell_type":"markdown","source":"üëÜ Repository *mateuszbuda_brain-segmentation-pytorch*  will ask permission, tap *y*.","metadata":{"_uuid":"e8136709-10e8-4209-bba4-438136dccd60","_cell_guid":"621b91b4-8cce-4200-89fc-a41da7500ef6","trusted":true}},{"cell_type":"markdown","source":"### Methods","metadata":{"_uuid":"32a5f72d-eb46-4475-8746-b3fc08c11161","_cell_guid":"dfb68ae0-85f4-4db7-9594-3096dde490c9","trusted":true}},{"cell_type":"code","source":"# ===============================================\n# Images methods\n# ===============================================\n\ndef get_processed_image(patient_id):\n    \"\"\"\n    Retrieves and processes the images for a given patient, grouping them for segmentation.\n\n    Args:\n        patient_id (str): The ID of the patient (BraTS21ID).\n\n    Returns:\n        numpy.ndarray: A processed image composed of the different images of the patient.\n    \"\"\"\n    # SEGMENTATION MODEL LIMITED TO 3 LAYERS\n    # T2W SKIPPED\n    patient_id = int(patient_id)\n\n    # Paths for image sequences\n    t1w_path = f'{train_path}/{str(patient_id).zfill(5)}/T1w'\n    flair_path = f'{train_path}/{str(patient_id).zfill(5)}/FLAIR'\n    t1wce_path = f'{train_path}/{str(patient_id).zfill(5)}/T1wCE'\n    #t2w_path = f'{train_path}/{str(patient_id).zfill(5)}/T2w'\n\n    # Retrieve image sequences\n    t1w_image = sequence_filenames(t1w_path)\n    flair_image = sequence_filenames(flair_path)\n    t1wce_image = sequence_filenames(t1wce_path)\n    #t2w_image = sequence_filenames(t2w_path)\n\n    # Resampling\n    re_sampled_flair = re_sample_image(flair_image, t1w_image)\n    re_sampled_t1wce = re_sample_image(t1wce_image, t1w_image)\n    #re_sampled_t2w = re_sample_image(t2w_image, t1w_image)\n\n    # Normalization\n    t1w_array = normalize(sitk.GetArrayFromImage(t1w_image))\n    flair_array = normalize(sitk.GetArrayFromImage(re_sampled_flair))\n    t1wce_array = normalize(sitk.GetArrayFromImage(re_sampled_t1wce))\n    #t2w_array = normalize(sitk.GetArrayFromImage(re_sampled_t2w))\n\n    sequence_stacked = np.stack([t1w_array, flair_array, t1wce_array]) #, t2w_array])\n\n    central_slice = t1w_array.shape[0] // 2\n    rvb = sequence_stacked[:, central_slice, :, :].transpose(1, 2, 0)\n    image = Image.fromarray((rvb * 255).astype(np.uint8))\n    return np.array([np.moveaxis(np.array(image.resize((256, 256))), -1, 0)])\n\n\ndef sequence_filenames(path) :\n    \"\"\"\n    Retrieves a sequence of images for a given directory.\n\n    Args:\n        path (str): The path to the directory containing the DICOM data set.\n\n    Returns:\n        SimpleITK.Image: A sequence of images corresponding to the DICOM files in the directory.\n\n    Raises:\n        FileNotFoundError: If the specified path does not exist.\n    \"\"\"\n    filenames = sitk_reader.GetGDCMSeriesFileNames(path)\n    sitk_reader.SetFileNames(filenames)\n    image = sitk_reader.Execute()\n    \n    return image    \n\ndef normalize(dataset) :\n    \"\"\"\n    Normalizes the data obtained from the images.\n\n    Args:\n        dataset (numpy.ndarray): The dataset to be normalized.\n\n    Returns:\n        numpy.ndarray: The normalized dataset.\n    \"\"\"\n    return (dataset - np.min(dataset)) / (np.max(dataset) - np.min(dataset))\n\n\ndef re_sample_image(image, ref_img):\n    \"\"\"\n    Resamples the image to match the dimensions and properties of the reference image.\n\n    Args:\n        image (SimpleITK.Image): The image to be resampled.\n        ref_img (SimpleITK.Image): The reference image used for resampling.\n\n    Returns:\n        SimpleITK.Image: The resampled image.\n    \"\"\"\n    re_sampler = sitk.ResampleImageFilter()\n    re_sampler.SetReferenceImage(ref_img)\n    re_sampler.SetDefaultPixelValue(image.GetPixelIDValue())\n    re_sampler.SetInterpolator(sitk.sitkLinear)\n    re_sampler.SetOutputSpacing(ref_img.GetSpacing())\n    re_sampler.SetOutputDirection(ref_img.GetDirection())\n    re_sampler.SetOutputOrigin(ref_img.GetOrigin())\n    re_sampler.SetSize(ref_img.GetSize())\n    re_sampler.SetTransform(sitk.AffineTransform(image.GetDimension()))\n    re_sampled_image = re_sampler.Execute(image)\n    \n    return re_sampled_image\n\ndef segmentation_process(image_resized):\n    \"\"\"\n    Obtains the segmented image.\n\n    Args:\n        image_resized (numpy.ndarray): The resized image.\n\n    Returns:\n        numpy.ndarray: The segmented image.\n    \"\"\"\n    segmentation = segmentation_model(torch.Tensor(image_resized))\n    return segmentation\n    \n# ===============================================\n# Dataset creation methods\n# ===============================================\n\ndef init_dataset_radiomics() :\n    \"\"\"\n    Initializes the DataFrame structures for radiomics data acquisition.\n\n    Returns:\n        None\n    \"\"\"\n    global df_shapes\n    global df_textures\n    global df_first_orders_features\n    \n    df_shapes_columns = ['ID','BraTS21ID','MeshSurface','PixelSurface','Perimeter','PerimeterSurfaceRatio','Sphericity',\n                              'SphericalDisproportion','MaximumDiameter','MajorAxisLength','MinorAxisLenth','Elongation']\n    df_shapes = pd.DataFrame(columns=df_shapes_columns) \n\n\n    df_textures_columns = ['ID','Autocorrelation', 'ClusterProminence', 'ClusterShade', 'ClusterTendency', 'Contrast', \n                           'Correlation', 'DifferenceAverage', 'DifferenceEntropy', 'DifferenceVariance', 'Id', 'Idm', \n                           'Idmn', 'Idn', 'Imc1', 'Imc2', 'InverseVariance', 'JointAverage', 'JointEnergy', 'JointEntropy', \n                           'MCC', 'MaximumProbability', 'SumAverage', 'SumEntropy', 'SumSquares']\n    df_textures = pd.DataFrame(columns=df_textures_columns) \n\n\n    df_first_orders_features_columns=['ID','10Percentile', '90Percentile', 'Energy', 'Entropy', 'InterquartileRange', 'Kurtosis', \n                                      'Maximum', 'MeanAbsoluteDeviation', 'Mean', 'Median', 'Minimum', 'Range', 'RobustMeanAbsoluteDeviation', \n                                      'RootMeanSquared', 'Skewness', 'TotalEnergy', 'Uniformity', 'Variance']\n    df_first_orders_features = pd.DataFrame(columns=df_first_orders_features_columns) \n    \n\ndef add_patient_data(ID,img_resized,segmentation) :\n    \"\"\"\n    Adds data from the specified patient's images to the analysis dataset.\n\n    Args:\n        ID (int): The ID of the patient.\n        img_resized (numpy.ndarray): Resized image of the patient.\n        segmentation (torch.Tensor): Segmentation of the patient's image.\n\n    Returns:\n        None\n    \"\"\"\n    global df_shapes\n    global df_textures\n    global df_first_orders_features\n    \n    # shape\n    results = radiomics.shape2D.RadiomicsShape2D(\n        sitk.GetImageFromArray(img_resized), \n        sitk.GetImageFromArray(np.array([\n            segmentation[0][0].detach().cpu().numpy() > 0.5\n        ]).astype(np.uint8)),\n        force2D=True\n    )\n    \n    shape2D = {}\n    shape2D['ID'] = int(ID)\n    shape2D['BraTS21ID'] = int(ID)\n    shape2D['MeshSurface'] = results.getMeshSurfaceFeatureValue()\n    shape2D['PixelSurface'] = results.getPixelSurfaceFeatureValue()\n    shape2D['Perimeter'] = results.getPerimeterFeatureValue()\n    shape2D['PerimeterSurfaceRatio'] = results.getPerimeterSurfaceRatioFeatureValue()\n    shape2D['Sphericity'] = results.getSphericityFeatureValue()\n    shape2D['SphericalDisproportion'] = results.getSphericalDisproportionFeatureValue()\n    shape2D['MaximumDiameter'] = results.getMaximumDiameterFeatureValue()\n    shape2D['MajorAxisLength'] = results.getMajorAxisLengthFeatureValue()\n    shape2D['MinorAxisLenth'] = results.getMinorAxisLengthFeatureValue()\n    shape2D['Elongation'] = results.getElongationFeatureValue()\n    \n    df_shapes=df_shapes.append(shape2D,ignore_index=True)\n    \n    # GLCM\n    results=radiomics.glcm.RadiomicsGLCM(\n        sitk.GetImageFromArray(img_resized[0,0,:,:].reshape(1, 256, 256)), \n        sitk.GetImageFromArray(np.array([\n            segmentation[0][0].detach().cpu().numpy() > 0.5\n        ]).astype(np.uint8)),\n        force2D=True\n    )\n\n    results.enableAllFeatures()\n    res = results.execute()\n    res['ID']=int(ID)\n\n    df_textures=df_textures.append(res,ignore_index=True)\n    \n    # First-orders features\n    results =  radiomics.firstorder.RadiomicsFirstOrder(\n        sitk.GetImageFromArray(img_resized[0,0,:,:].reshape(1, 256, 256)), \n        sitk.GetImageFromArray(np.array([\n            segmentation[0][0].detach().cpu().numpy() > 0.5\n        ]).astype(np.uint8)),\n        force2D=True\n    )\n\n    results.enableAllFeatures()\n    res = results.execute()\n    res['ID']=int(ID)\n\n    df_first_orders_features=df_first_orders_features.append(res,ignore_index=True)\n    \n    \n# ===============================================\n# Show methods\n# ===============================================\n\ndef show_segmentation(title, img_src, segmentation):\n    \"\"\"\n    Displays the resized source images and the segmentation image in a single line.\n\n    Args:\n        title (str): Global title for the plot.\n        img_src (numpy.ndarray): Resized source images.\n        segmentation (torch.Tensor): Segmentation image.\n\n    Returns:\n        None\n    \"\"\"\n    titles = ['T1w', 'FLAIR', 'T1wce'] #,'T2w'\n\n    # Create the main figure\n    fig = plt.figure()\n\n    # Adjust top margin for the main figure\n    fig.subplots_adjust(top=0.85)\n\n    # Set the global title\n    fig.suptitle(title, y=0.75)\n\n    # Iterate over the source images\n    for i in range(3):\n        ax = fig.add_subplot(1, 4, 1+i)\n        ax.imshow(img_src[0, i])\n        ax.set_title(titles[i])\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    # Add the segmentation image\n    ax_segmentation = fig.add_subplot(1, 4, 4)\n    ax_segmentation.imshow(segmentation.detach().numpy()[0, 0])\n    ax_segmentation.set_title('Segmentation')\n    ax_segmentation.set_xticks([])\n    ax_segmentation.set_yticks([])\n\n    plt.tight_layout()\n    plt.show()\n    \ndef show_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):\n    \"\"\"\n    Displays a download link for a DataFrame as a CSV file.\n\n    Args:\n        df (pandas.DataFrame): The DataFrame to be downloaded.\n        title (str): The title of the download link (default: \"Download CSV file\").\n        filename (str): The name of the downloaded file (default: \"data.csv\").\n\n    Returns:\n        None\n    \"\"\"\n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload, title=title, filename=filename)\n    display(HTML(html))\n\n# ===============================================\n# Analysis\n# ===============================================\n\ndef filter_correlation_matrix(correlation_matrix, correlation_threshold):\n    \"\"\"\n    Filters a correlation matrix by keeping only the absolute values greater than or equal to the correlation threshold.\n\n    Args:\n        correlation_matrix (pd.DataFrame): The correlation matrix.\n        correlation_threshold (float): The correlation threshold for filtering the matrix.\n\n    Returns:\n        pd.DataFrame: The filtered correlation matrix.\n\n    \"\"\"\n    filtered_correlation_matrix = correlation_matrix[abs(correlation_matrix) >= correlation_threshold]\n\n    return filtered_correlation_matrix\n\ndef find_highly_correlated_groups(correlation_matrix, correlation_threshold = 0.8, filter_duplicated_group = True, convert_indices_to_column_names = True):\n    \"\"\"\n    Finds groups of highly correlated variables from a correlation matrix.\n\n    Args:\n        correlation_matrix (pd.DataFrame): The correlation matrix.\n        correlation_threshold (float): The correlation threshold to consider as highly correlated.\n        filter_duplicated_group (bool): Indicates whether to filter out duplicated values in the correlated groups.\n        convert_indices_to_column_names (bool): Indicates whether to convert indices to column names.\n\n    Returns:\n        List[List[str]]: A list of groups, where each group contains the names of variables that are highly correlated.\n\n    \"\"\"\n    n = correlation_matrix.shape[0]  # Number of variables in the correlation matrix\n    groups_correlated = []  # List to store the correlated groups\n    \n    # Retrieve column names\n    column_names = correlation_matrix.columns\n    \n    # Traverse each variable\n    for i in range(n):\n        if column_names[i] not in [column_names[v] for g in groups_correlated for v in g]:  # Check if the variable has already been added to a group\n            group = [i]  # Create a new group containing the current variable (i)\n            for j in range(i+1, n):\n                if column_names[j] not in [column_names[v] for g in groups_correlated for v in g]:  # Check if the variable has already been added to a group\n                    correlation = correlation_matrix.iloc[i, j]  # Retrieve the correlation between variables i and j\n                    if abs(correlation) >= correlation_threshold:  # Strong correlation condition (adjust as needed)\n                        group.append(j)  # Add variable j to the group\n            \n            groups_correlated.append(group)  # Add the group to the list of correlated groups\n    \n    # Filter out duplicated values in the correlated groups\n    if filter_duplicated_group:\n        filtered_groups_correlated = []\n        for group in groups_correlated:\n            filtered_group = list(set(group))  # Convert to a set to eliminate duplicates, then convert back to a list\n            filtered_groups_correlated.append(filtered_group)\n        groups_correlated = filtered_groups_correlated # Reset by new one\n    \n    # Convert indices to column names\n    if convert_indices_to_column_names:\n        groups_correlated = [[column_names[i] for i in group] for group in groups_correlated]\n    \n    return groups_correlated\n\ndef fonc_test_normality(df,graphic=True) : \n\n    describe = df.describe()\n    \n    for col in df.columns :\n\n        describe.loc['skewness',col] = stats.skew(df[col])\n        describe.loc['kurtosis', col] = stats.kurtosis(df[col],fisher=False)#Vrai kurtosis\n        describe.loc['excess_kurtosis',col] = stats.kurtosis(df[col])#Vrai kurtosis\n        shapiro_test =  stats.shapiro(df[col])[1]\n        describe.loc['shapiro_test',col] = shapiro_test\n        describe.loc['normalite',col] = 'Oui' if shapiro_test > 0.05 else 'Non'\n\n        if graphic : \n            figure_size = (6, 2)\n\n            fig = plt.figure(figsize=figure_size)\n\n            plt.subplot(1,3,1)\n            sns.histplot(df[col], kde=True)\n            plt.title('Histogramme de {}'.format(col),fontsize=8)\n            plt.xlabel('Valeur',fontsize=7)\n            plt.ylabel('Fr√©quence',fontsize=7)\n            plt.plot(describe.loc['mean',col],0, marker=\"o\", color=\"red\")\n\n            plt.subplot(1,3,2)\n            plt.boxplot(x=df[col])\n            plt.title('Boxplot de {}'.format(col),fontsize=8)\n            #plt.xlabel('Valeur')\n\n            plt.subplot(1,3,3)\n            stats.probplot(df[col], plot=plt)\n            plt.title('Q-Q plot pour {}'.format(col),fontsize=8)\n            plt.xlabel('Quantile th√©orique',fontsize=7)\n            plt.ylabel('Valeurs ordonn√©es',fontsize=7)\n            plt.tight_layout()\n            plt.show()\n\n    return describe","metadata":{"_uuid":"a054ac17-8847-419e-87f0-8b1ff3cefd32","_cell_guid":"f0842638-3396-4074-93e5-59be1be65db1","_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>","metadata":{"_uuid":"abcfe1be-c252-4a1e-8a35-dac03f103531","_cell_guid":"a355a450-f0d3-41c7-9b64-abfeb399333e","trusted":true}},{"cell_type":"markdown","source":"# <a id='exploratory_data'>2. Exploratory Data</a>","metadata":{"_uuid":"ce7ad6e9-b195-4658-93da-6317fb1711ab","_cell_guid":"dffc5a8f-76bd-40dd-80c5-40226460db89","trusted":true}},{"cell_type":"markdown","source":"The **\"train/\"** directory contains the training files for the competition. Each top-level directory represents a subject, and the **\"train_labels.csv\"** file contains the corresponding targets for each subject, indicating the presence of MGMT promoter methylation.\n\n‚ÑπÔ∏è **Note:** However, report on main contest page, there are unexpected problems with the following three cases in the training dataset: [00109, 00123, 00709].","metadata":{"_uuid":"7cb37020-31e3-429b-a0ca-3d3357531b14","_cell_guid":"0a55358e-1db4-41df-8b78-cc7aceca8346","trusted":true}},{"cell_type":"code","source":"print('Samples of train folder:', len(dataset))","metadata":{"_uuid":"0cb6c1fc-4944-48ed-ad4c-62f5461304e5","_cell_guid":"9b6a1cda-bdcc-4bf6-89cb-8162cab12171","_kg_hide-output":false,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head(10)","metadata":{"_uuid":"f69b32cc-090b-44a8-b0e2-451d8a60425a","_cell_guid":"0e658141-9f84-421f-b485-2cf1f9541460","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The \"train_labels.csv\" file.","metadata":{"_uuid":"025fbd90-5700-4486-9d40-de2fb356ce0a","_cell_guid":"b0d49dc5-2d01-489e-864c-be5bc206f8c7","trusted":true}},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nsns.countplot(data=dataset, x=\"MGMT_value\")\nplt.title(\"Distribution of MGMT values\")\nplt.xlabel(\"MGMT Value\")\nplt.xticks([0, 1], [\"Not Present\", \"Present\"])\nplt.show()","metadata":{"_uuid":"8bc181c3-b212-4d36-a7dd-e8e3dd0dc26c","_cell_guid":"94df6d8c-1ce6-4aec-ab4e-4ebefc86c99b","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **\"test/\"** directory contains the test files. For each subject in the test data, there is no file containing the methylation targets, so these values must be predicted. The **\"sample_submission.csv\"** file is an example of a correctly formatted submission file, with MGMT values of **0.5** for each subject.\n\nOverall, the task of the competition is to predict the presence of MGMT promoter methylation for each subject in the test data.\n\n‚ÑπÔ∏è **Note:** We deduce that we have to separate the sets of given train into part two part train and test for training.","metadata":{"_uuid":"01ea4e87-7ddf-4481-a102-13b166c98850","_cell_guid":"56259e0c-defc-4083-a107-b3459f259e7a","trusted":true}},{"cell_type":"code","source":"samp_subm.head(1)","metadata":{"_uuid":"37c36bba-0c63-41cc-b72c-4945fd5c13a4","_cell_guid":"fade83bb-e3fd-4c2c-8ce2-b96fa0a07c34","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='exploratory_data_2_1'>2.1. MRI  scans folders</a>","metadata":{"_uuid":"56c39e93-1cd5-42a9-bac0-da6760e19265","_cell_guid":"a18e3d83-ea73-42bd-b072-25e89f4ec38b","trusted":true}},{"cell_type":"code","source":"# Extract first train sample\nfirst_folder = str(dataset.loc[0, 'BraTS21ID']).zfill(5) + \"/\"\n\n# Folders content\nprint(\n    \"Folders content for all patients:\", \n        json.dumps(os.listdir(train_path + first_folder), indent=4)\n)","metadata":{"_uuid":"502ad0b4-4807-4785-a347-b35c5fb3ec10","_cell_guid":"a351b8f2-382a-4d52-bf5c-149b36b4ff83","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the first Dataset of the patient, we will explore the images contained in ['T2w', 'T1wCE', 'T1w', 'FLAIR'] of the first patient.","metadata":{"_uuid":"93f6c36f-c3e2-4419-a133-d1b498c91a48","_cell_guid":"93d3c95c-0629-442f-9407-d3a033d56ad6","trusted":true}},{"cell_type":"code","source":"print('Number of FLAIR images:', len(os.listdir(train_path + first_folder +'FLAIR')))\nprint('Number of T1w images:', len(os.listdir(train_path + first_folder + 'T1w')))\nprint('Number of T1wCE images:', len(os.listdir(train_path + first_folder + 'T1wCE')))\nprint('Number of T2w images:', len(os.listdir(train_path + first_folder + 'T2w')))","metadata":{"_uuid":"73ab2792-c52a-47f2-bbdd-8a1262d4e187","_cell_guid":"1af9897d-cc0e-4f88-97e2-c674b9f05a8c","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='exploratory_data_2_2'>2.2. MRI slides scans previews</a>","metadata":{}},{"cell_type":"code","source":"image_path = \"https://github.com/YanSteph/RSNA-MICCAI-Brain-Tumor-Classification-AI/blob/main/img/scan1.png?raw=true\"\nhtml_code = f'<img src=\"{image_path}\" style=\"width: 700px;\" />'\ndisplay(HTML(html_code))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='exploratory_data_2_3'>2.3. MRI and MGMT values previews</a>","metadata":{}},{"cell_type":"code","source":"patient_without_mgmt = dataset.loc[dataset[\"MGMT_value\"] == 0]\npatient_with_mgmt = dataset.loc[dataset[\"MGMT_value\"] == 1]\n\npatient_with_mgmt_folder_ids = patient_with_mgmt[\"BraTS21ID\"].iloc[:3]\npatient_without_mgmt_folder_ids = patient_without_mgmt[\"BraTS21ID\"].iloc[:3]\n\nfor patient_with_mgmt_folder_id, patient_without_mgmt_folder_id in zip(patient_with_mgmt_folder_ids, patient_without_mgmt_folder_ids):\n    img_resized = get_processed_image(patient_with_mgmt_folder_id)\n    segmentation = segmentation_process(img_resized)\n    show_segmentation(\"Patient with MGMT\",img_resized,segmentation)\n\n    img_resized = get_processed_image(patient_without_mgmt_folder_id)\n    segmentation = segmentation_process(img_resized)\n    show_segmentation(\"Patient without MGMT\",img_resized,segmentation)","metadata":{"_uuid":"611591c4-ff56-42de-854b-c93ea7def75d","_cell_guid":"3452c76d-7b92-4222-aad3-e4a557ead4f0","_kg_hide-input":true,"_kg_hide-output":false,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='exploratory_data_2_4'>2.4. Summary</a>\n\n* We deduce that we have to separate the sets of given train into part two part train and test for training.\n* Report on main contest page, there are unexpected problems with the following three cases in the training dataset: [00109, 00123, 00709], we exclude this data.\n* Exclusion of \"/test\" folder.","metadata":{"_uuid":"c37b30df-6505-4513-b591-03879fd116c5","_cell_guid":"6babb453-7342-4a9d-9988-71763ffa1c13","trusted":true}},{"cell_type":"markdown","source":"----","metadata":{"_uuid":"952659d7-2e85-4c66-bd90-3e15c89735a6","_cell_guid":"53a384a0-72ba-4993-949e-b5414ef164c3","trusted":true}},{"cell_type":"markdown","source":"# <a id='brain_segmentation'>3. Brain segmentation</a>\nThe \"mateuszbuda_brain-segmentation-pytorch_unet\" library was chosen to facilitate brain segmentation from medical images in our project. This section outlines the reasons behind selecting this library and its contribution to achieving accurate brain segmentation results.\n\nSource: [mateuszbuda_brain-segmentation-pytorch_unet on PyTorch Hub](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/)\n\n## <a id='brain_segmentation_3_1'>3.1. Importance of Brain Segmentation</a>\nBrain segmentation is a crucial task in medical imaging as it enables the extraction of precise information about different regions or classes within the brain structures. Accurate segmentation plays a vital role in various medical applications, including tumor detection, anatomical analysis, and treatment planning.\n\n## <a id='brain_segmentation_3_2'>3.2. Selection Criteria</a>\nThe \"mateuszbuda_brain-segmentation-pytorch_unet\" library was chosen based on the following criteria:\n\n* Exceptional Performance: The library has demonstrated exceptional performance in brain segmentation tasks, providing accurate and reliable results.\nUnet Architecture: It is specifically designed based on the Unet neural network architecture, which has a proven track record of success in biomedical image segmentation.\n* User-Friendly Nature: The library offers a user-friendly interface and efficient implementation, allowing researchers and practitioners to easily integrate it into their projects.\n\n## <a id='brain_segmentation_3_3'>3.3. Contributions of the Library</a>\nThe utilization of the \"mateuszbuda_brain-segmentation-pytorch_unet\" library significantly contributed to the acquisition of accurate brain segmentation data for our dataset. By leveraging the library's capabilities, we were able to efficiently segment medical images and extract valuable information for further analysis and research purposes.\n\n## <a id='brain_segmentation_3_4'>3.4. Summary</a>\n\nIn summary, the selection of the \"mateuszbuda_brain-segmentation-pytorch_unet\" library was based on its exceptional performance, user-friendly nature, and utilization of the powerful Unet architecture. This library played a pivotal role in achieving accurate brain segmentation results and provided a solid foundation for our project's objectives.\n\n# <a id='utilizing_unet_4'>4. Utilizing the U-Net for Brain MRI Model and RadiomicsShape2D Class</a>\nU-Net for Brain MRI Model\nTo accomplish tumor segmentation, we will employ the U-Net for Brain MRI model. This section provides an overview of the model's architecture and usage instructions.\n\n## <a id='utilizing_unet_4_1'>4.1. Architecture</a>\nThe U-Net for Brain MRI model features a U-shaped architecture with branch connections and consists of four levels of blocks. Each block comprises two convolution layers with batch normalization, ReLU activation function, and an encoding part with a max pooling layer. The decoding part utilizes up-convolution. The number of convolution filters varies across the model's levels, ranging from 32 to 256.\n\n## <a id='utilizing_unet_4_2'>4.2. Usage Instructions</a>\nTo utilize the U-Net for Brain MRI model, follow these steps:\n\nProvide an input brain MRI image with three channels corresponding to pre-contrast, FLAIR, and post-contrast sequences.\nScale the image to a size of 256x256 pixels.\nNormalize the image using the z-score method per volume.\nThe pre-trained U-Net model produces a single-channel probability map indicating anomalous regions in the input image. By applying an appropriate threshold to this probability map, it can be converted into a binary segmentation mask.\n\n## <a id='utilizing_unet_4_3'>4.3. RadiomicsShape2D Class</a>\nTo perform shape analysis and extract relevant features, we will utilize the \"radiomics.shape2D.RadiomicsShape2D\" class. This class provides functionalities for analyzing the shape characteristics of segmented regions in medical images.\n\n## <a id='utilizing_unet_4_4'>4.4. Usage Instructions</a>\nTo utilize the RadiomicsShape2D class, follow these steps:\n\nProvide the segmented regions or masks obtained from the U-Net model.\nInstantiate the RadiomicsShape2D class.\nUse the available methods and functions to extract shape features, such as volume, surface area, compactness\n\nSource: [Radiomics.shape2D.RadiomicsShape2D ](https://pyradiomics.readthedocs.io/en/latest/features.html#module-radiomics.shape2D)\n\n# <a id='dataset_creation_5'>5. Dataset Preparation</a>\n\nIn order to proceed, we need to partition the given training dataset into two sets: training and testing. \n\nHowever, it is important to note that there are certain issues with three specific cases in the training dataset, namely [00109, 00123, 00709], as reported on the main contest page. \n\nTherefore, we will exclude these cases from the dataset.","metadata":{"_uuid":"a20c3a29-eca0-495e-92fb-1c71edf542f7","_cell_guid":"ab110a59-81b9-4b08-a3ac-7324d4de4ba6","trusted":true}},{"cell_type":"code","source":"# Flag to skip brain segmentation with PyTorch UNet\n# If set to True, we will import the dataset that has already been generated\nskip_brain_segmentation_pytorch_unet = True\n\n# model_segmentation, load mateuszbuda/brain-segmentation-pytorch, U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI","metadata":{"_uuid":"d644804a-04f0-4084-81e3-35721c325d0b","_cell_guid":"524351c1-3938-4b68-b6f8-5d16897221ae","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üëÜ Flag to skip brain segmentation with PyTorch UNet, default set to *True* else will import the dataset that has already been generated","metadata":{"_uuid":"86ee57dd-6bbb-40d1-8760-5fa1c60491b9","_cell_guid":"f4dad8a0-7500-4e34-a902-2aea75bb82fa","trusted":true}},{"cell_type":"code","source":"if skip_brain_segmentation_pytorch_unet:\n    dataset = pd.read_csv(dataset_path)\n    \nelse:\n    loader = widgets.IntProgress(min=0, max=len(dataset), description='Loading:')\n    display(loader)\n    \n    # Empty creation of datasets\n    init_dataset_radiomics()\n\n    for i in dataset.BraTS21ID :\n        loader.value += 1\n        img_resized = get_processed_image(i)\n        segmentation = segmentation_process(img_resized)\n        add_patient_data(i, img_resized, segmentation)\n\n    # Join the 3 datasets\n    df_shapes = df_shapes.set_index('ID')\n    df_textures = df_textures.set_index('ID')\n    df_first_orders_features = df_first_orders_features.set_index('ID')\n\n    df = df_shapes.join(df_textures).join(df_first_orders_features)\n\n    # Define 'BraTS21ID' column as integer IDs\n    df['BraTS21ID'] = df['BraTS21ID'].astype(int)\n    \n    # Merge the old dataset with the new one\n    dataset = pd.merge(dataset, df, left_on='BraTS21ID', right_on='BraTS21ID')\n    dataset.rename(columns={'BraTS21ID': 'ID'}, inplace=True)\n    \n# Patient BraTS21ID now is ID, and ID of Dataset\ndataset = dataset.set_index('ID')\nshow_download_link(dataset)","metadata":{"_uuid":"5857f597-d16f-49a1-b403-b6493d3420b3","_cell_guid":"2b7c92a1-1b27-41b2-8ad4-4ff7e34b4332","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>","metadata":{"_uuid":"2d471aa3-4010-4b05-8b0e-8dc0fe675841","_cell_guid":"3abe78a7-b994-487c-b21e-4d64938c7226","trusted":true}},{"cell_type":"markdown","source":"# <a id='exploratory_dataset'>6. Exploratory Dataset</a>\n\nThese features provide information about various properties of brain MRI images, such as shape, texture, and grayscale statistics. They are commonly used for analysis and classification of medical images to aid in the detection and characterization of brain pathologies.\n\nHere is the requested list of features extracted from brain MRI images:\n\n**Shape Features:**\n\n* **ID:** Sample identifier.\n* **MeshSurface:** Mesh surface representing the object's surface using a three-dimensional mesh.\n* **PixelSurface:** Surface in pixels representing the object's surface using pixels.\n* **Perimeter:** Perimeter of the object, which is the length of the line surrounding the object.\n* **PerimeterSurfaceRatio:** Ratio of perimeter to surface, which can provide an indication of the object's shape.\n* **Sphericity:** Sphericity, measuring how closely the object resembles a perfect sphere.\n* **SphericalDisproportion:** Spherical disproportion, measuring the difference between the object's shape and a perfect sphere.\n* **MaximumDiameter:** Maximum diameter, which is the greatest distance between two points of the object.\n* **MajorAxisLength:** Major axis length, which is the length of the object's principal axis.\n* **MinorAxisLength:** Minor axis length, which is the length of the object's secondary axis.\n* **Elongation:** Elongation, measuring the stretching of the object.\n\n**Texture Features:**\n\n* **ID:** Sample identifier.\n* **Autocorrelation:** Autocorrelation, measuring the similarity between grayscale levels of an image at different positions.\n* **ClusterProminence:** Cluster prominence, measuring the asymmetry and regularity of pixel values within a cluster.\n* **ClusterShade:** Cluster shade, measuring the symmetry of pixel values within a cluster.\n* **ClusterTendency:** Cluster tendency, measuring the similarity of pixel values within a cluster.\n* **Contrast:** Contrast, measuring the brightness differences between neighboring pixels.\n* **Correlation:** Correlation, measuring the linear relationship between grayscale levels of an image in different directions.\n* **DifferenceAverage:** Difference average, measuring the average differences between grayscale levels of neighboring pixels.\n* **DifferenceEntropy:** Difference entropy, measuring the amount of information in the differences between grayscale levels of neighboring pixels.\n* **DifferenceVariance:** Difference variance, measuring the variability of differences between grayscale levels of neighboring pixels.\n* **Id, Idm, Idmn, Idn, Imc1, Imc2:** These texture-specific features are computed from gray-level co-occurrence matrices and measure different properties of the distribution of gray levels in the image.\n* **InverseVariance:** Inverse variance, measuring the reciprocity of gray-level variance in the image.\n* **JointAverage:** Joint average, measuring the average gray levels in neighborhood relationships.\n* **JointEnergy:** Joint energy, measuring the sum of squared joint gray level values.\n* **JointEntropy:** Joint entropy, measuring the amount of information contained in joint gray level values.\n* **MCC:** Maximum correlation coefficient, measuring the maximum correlation between grayscale levels of an image in different directions.\n* **MaximumProbability:** Maximum probability, measuring the maximum probability of joint grayscale values.\n* **SumAverage:** Sum average, measuring the average sum of joint gray level values.\n* **SumEntropy:** Sum entropy, measuring the amount of information contained in the sums of joint gray level values.\n* **SumSquares:** Sum squares, measuring the sum of squared joint gray level values.\n\n**First-Order Features:**\n\n* **ID:** Sample identifier.\n* **MGMT_value:** Presence of MGMT.\n* **10Percentile:** 10th percentile, representing the value below which 10% of the pixels are found.\n* **90Percentile:** 90th percentile, representing the value below which 90% of the pixels are found.\n* **Energy:** Energy, measuring the sum of squared grayscale levels of pixels.\n* **Entropy:** Entropy, measuring the amount of information contained in the grayscale levels of the image.\n* **InterquartileRange:** Interquartile range, which is the difference between the 75th and 25th percentiles, providing","metadata":{"_uuid":"04405e51-35ba-4be3-9e61-5d5f2c0df59e","_cell_guid":"103c1032-10dd-403d-9502-cd048fce8444","trusted":true}},{"cell_type":"code","source":"dataset.head()","metadata":{"_uuid":"24525fae-c34a-4914-967f-312e2c5d9674","_cell_guid":"f05295bd-c9cf-4662-bd19-f9cced69d860","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{"_uuid":"c5009d57-d406-47b6-abba-f561be4b2e7a","_cell_guid":"617a5376-3ce1-4d08-98f0-094c007aee0e","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.describe()","metadata":{"_uuid":"651fabba-eb72-42ce-b9c0-0c09aba469cc","_cell_guid":"58d2c1b1-5cc4-45a9-afe1-fc9f0e457c3b","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the first row for patients with MGMT_value = 1\npatient_with_mgmt = dataset.loc[dataset[\"MGMT_value\"] == 1].mean()\n\n# Select the first row for patients with MGMT_value = 0\npatient_without_mgmt = dataset.loc[dataset[\"MGMT_value\"] == 0].mean()\n\n# Transpose the dataframes\npatient_with_mgmt = patient_with_mgmt.T\npatient_without_mgmt = patient_without_mgmt.T\n\n# Concatenate the transposed dataframes horizontally\npatient_explore = pd.concat([patient_with_mgmt, patient_without_mgmt], axis=1)\n\n# Rename the columns\npatient_explore.columns = ['Patient MGMT 1 (Mean)', 'Patient MGMT 0 (Mean)']\n\nfrom colorama import Fore, Style\n\n# Obtenir les 52 derni√®res lignes du DataFrame patient_explore\npatient_tail = patient_explore.tail(52)\n\n    \n# Get the last 52 rows of the dataframe\npatient_explore.tail(52)","metadata":{"_uuid":"6af3a4dd-e02b-4098-9b75-83313d85b38b","_cell_guid":"3667fd9e-1cbe-4349-b866-67d31b5e19b6","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='exploratory_dataset_6_1'>6.1. Summary</a>\n\n* No null values\n* The variable \"MGMT_value\" has a mean of **0.524786** and a standard deviation of **0.499813**, indicating a bimodal distribution with *slight positive skewness.*\n* The `[\"MeshSurface\", \"PixelSurface\"]` surface variables appear to have a considerable range, with values ranging from **336.5 to 6565.**\n* The variable `[\"Perimeter\"]` has a mean of **450.439610** and a standard deviation (std) of **360.639463**, indicating a relatively high dispersion of values. A relatively high spread of values means that object boundaries vary. The individual values can be far from the mean, which induces a great variability in the sizes and shapes of the objects present in the images.\n* The `[\"Sphericity\", \"SphericalDisproportion\", \"Elongation\"]` shape variables have mean values around **0.4 to 0.6**, indicating a generally non-spherical shape of the depicted objects. (Brain)\n* The variable `[\"Mean\"]`  has of **879.400708** and a standard deviation of **1101.421810**, indicating a relatively high variability in the mean values of the images. This suggests that the images show great diversity in their average values, resulting from different characteristics of the objects and the capture conditions.\n* The `[\"Minimum\"]` variable has a minimum value of 0, suggesting the possible presence of outliers or black pixels in the images.  \n    * (See in previous summary, patients:`[00109, 00123, 00709]`)","metadata":{"_uuid":"9ff867c7-8d3b-43cd-aa7c-55cf7249f09d","_cell_guid":"b5f40700-b346-4bfe-a03f-584efa89cb6f","trusted":true}},{"cell_type":"markdown","source":"----","metadata":{"_uuid":"acd8330c-307e-4d7f-a840-792df4ae9fc4","_cell_guid":"d75739f2-5122-4f84-8f68-8a68c8275b68","trusted":true}},{"cell_type":"markdown","source":"# <a id='analysis'>7. Analysis</a>","metadata":{"_uuid":"1bee88f4-c132-489d-8528-7a9c1e756147","_cell_guid":"33897382-92cb-4bfa-9596-3cb97766350f","trusted":true}},{"cell_type":"markdown","source":"# ‚ö†Ô∏è TODO A faire ‚ö†Ô∏è","metadata":{"_uuid":"6bbb098f-31f7-42bc-80ff-92776aa0e7c3","_cell_guid":"b234bcbd-c806-4732-8c12-c9995c4d44f3","trusted":true}},{"cell_type":"markdown","source":">","metadata":{"_uuid":"037b1078-05a9-4d9d-91ec-ce64096eab23","_cell_guid":"170e6525-4be4-4c9d-b211-00736cdc17c1","trusted":true}},{"cell_type":"markdown","source":"### Correlation Matrix","metadata":{"_uuid":"aed523d9-fbca-462f-8f38-b8b0b48ba3ef","_cell_guid":"8c971fe9-4f24-466c-bfd9-262070bef4f6","trusted":true}},{"cell_type":"code","source":"correlation_matrix = dataset.corr()\n\nfig, ax = plt.subplots(figsize=(50, 50))\nsns.heatmap(correlation_matrix, ax=ax, annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Matrix\")\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\nplt.show()","metadata":{"_uuid":"eab5c682-c80c-4b2a-aa9d-5fd6a014f792","_cell_guid":"0dd60262-c98a-487c-8c3c-8b318f4d0ab2","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Matrix with Strong correlation\n\nAs a rule of correlation threshold:\n* 0.00-0.19: very weak.\n* 0.20-0.39: weak.\n* 0.40-0.59: moderate.\n* 0.60-0.79: strong.\n* 0.80-1.00: very strong.\n\nWe will use 0.7, strong.","metadata":{"_uuid":"3a6e9877-0721-4e05-a37f-d64f3c684d5c","_cell_guid":"5e99b576-dfb0-48f6-adc9-087c36052a6e","trusted":true}},{"cell_type":"code","source":"correlation_threshold = 0.7\n# Filtrer by correlation threshold\nfiltered_correlation_matrix = filter_correlation_matrix(correlation_matrix, correlation_threshold)\n\nfig, ax = plt.subplots(figsize=(50, 50))\nsns.heatmap(filtered_correlation_matrix, ax=ax, annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Matrix\")\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\nplt.show()","metadata":{"_uuid":"bf162c4e-f362-4dd7-a672-be3ec44a2bab","_cell_guid":"9d973433-ad03-4e4f-9a89-5099692adefe","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TODO","metadata":{"_uuid":"de54763a-89e5-4066-9063-aa7d60ca12bb","_cell_guid":"7969b5f6-3c7e-49e3-b3f1-8435045bdc2f","trusted":true}},{"cell_type":"markdown","source":"### Correlation group with threshold strength\n\nFor correlation threshold equal to **0.7, strong.**","metadata":{"_uuid":"8b96d02c-4925-40cc-8a32-b9196357c066","_cell_guid":"8f48eb35-6786-46eb-a8c8-98f919b96aca","trusted":true}},{"cell_type":"code","source":"correlation_threshold = 0.7\ngroups_correlated_threshold_07 = find_highly_correlated_groups(correlation_matrix, correlation_threshold)\n\nfor group_correlated in groups_correlated_threshold_07:\n    if len(group_correlated) > 1:\n        print(group_correlated)","metadata":{"_uuid":"02a4d4c7-4ca9-42cb-8798-a0fdca0998ec","_cell_guid":"089f30f9-583a-4047-ae38-92f8441cc8c2","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For correlation threshold equal to **0.99, very strong.**","metadata":{"_uuid":"be7f914a-6220-44c2-b1bd-d91f87a84142","_cell_guid":"c63feb5d-ff5e-4775-9acd-caf49768ee69","trusted":true}},{"cell_type":"code","source":"correlation_threshold = 0.99\ngroups_correlated_threshold_099 = find_highly_correlated_groups(correlation_matrix, correlation_threshold)\n\nfor group_correlated in groups_correlated_threshold_099:\n    if len(group_correlated) > 1:\n        print(group_correlated)","metadata":{"_uuid":"59b43b2d-b9d9-4795-9b78-9b3d37eabd30","_cell_guid":"092ba183-c45f-4031-94a8-3a2c13d9a3da","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='analysis_univariee_7_2'>7.2. Analyse univari√©e</a>\n\nTest de normalit√© utilis√© : Shapiro-Wilk : Puissant et pr√©cis, recommand√© pour les √©chantillons de petite taille\n\nTeste le skewness : mesure l'assym√©trie d'une s√©rie (0 si suit loi normale). Lorsque la Skewness est √©gal √† 0, le dataset est sym√©trique. Mais cette mesure nous renseigne aussi sur le type d‚Äôasym√©trie.\n\nTeste le kurtosis : mesure l'applatissement (vaut 3 si loi normale de Laplace) mais on utilise aussi l'exc√©dent de Kurtosis. Si le Kurtosis est sup√©rieur √† 3, alors l‚Äôensemble de donn√©es est leptokurtique, c‚Äôest-√†-dire que les queues sont plus √©paisses que la normale. Cela indique un regroupement d‚Äôoutliers.\n\nSi le Kurtosis est inf√©rieur √† 3, alors l‚Äôensemble de donn√©es est platykurtique, c‚Äôest-√†-dire que les queues sont plus fines que la normale. Cela indique un exc√®s n√©gatif d‚Äôoutlier. En d‚Äôautres termes, la plupart des donn√©es ont tendance √† se rassembler autour de la moyenne.\n\nLorsque le Kurtosis est √©gal √† 3, alors l‚Äôensemble de donn√©es est m√©sokurtique, c‚Äôest-√†-dire que les queues sont les m√™mes que dans une distribution normale.\n\n‚ÑπÔ∏è Graphiques utilis√©s :\n\n    histogramme avec courbe de densit√© de probabilit√© (le point rouge indique la moyenne)\n    boxplot\n    QQ plot (diagramme Quantile-Quantile) : permet d'√©valuer la pertinence de l'ajustement d'une distribution donn√©e √† un mod√®le th√©orique.","metadata":{"_uuid":"20e792be-3e26-4ae1-9ce5-03546d191b9b","_cell_guid":"b6f7bf10-6ea9-4a1b-8af3-3b04145491d2","trusted":true}},{"cell_type":"markdown","source":"<h3>Dataset complet</h3>","metadata":{"_uuid":"64e171db-bd86-4bf4-b2d8-dd71a6c4b084","_cell_guid":"cd1ff73c-0800-4963-beed-bc5f5f6bc5ae","trusted":true}},{"cell_type":"code","source":"describe=fonc_test_normality(dataset.drop('MGMT_value',axis=1))\n\nprint(\"Description dataset complet\")\ndescribe","metadata":{"_uuid":"0bf8cd07-a8da-449a-be5b-5d01f90c2192","_cell_guid":"6e05016e-7d08-42bd-bab9-22e5b857023a","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"‚ÑπÔ∏è **Note** : Le jeu de donn√©es montre que les les valeurs cibles sont homog√®nes.\n\nAucune des variables ne r√©pond au test de Shapiro-Wilk indiquant que les variables explicatives ne suivent pas une loi normale.\nVariables ayant un skewness correct : Sphericity, Elongation, Idn, Range,\nVariables ayant un bon kurtosis : MajorAxisLength, Elongation, DifferenceAverage, DifferenceEntropy, Idn, JointAverage, SumAverage, Mean,\nEn analysant les graphiques, variables semblant bonnes sur le Q-Q plot sans trop de valeurs extr√™mes : Elongation, DifferenceAverage, DifferenceEntropy, JointEntropy, SumEntropy, Entropy, Maximum, Range\nMais cela ne signifie pas que le dataset soit mauvais, il permet de faire ressortir les variables avec des valeurs extr√™mes.\nElongation, Idn et Range semblent √™tre homog√®nes.","metadata":{"_uuid":"05b6f6de-c30f-4565-bc00-79cde71d5534","_cell_guid":"e61b3978-d3d8-4f59-a975-d2fc7425b62c","trusted":true}},{"cell_type":"markdown","source":"<h3>M√™mes calculs mais que pour les MGMT_value = 1</h3>","metadata":{"_uuid":"db138125-2a8d-4210-ba3f-a364959a7d8e","_cell_guid":"4139877e-4698-4dba-9b46-0d122d88c10e","trusted":true}},{"cell_type":"code","source":"describe_MGMT_1=fonc_test_normality(dataset[dataset.MGMT_value == 1].drop('MGMT_value',axis=1),graphic=False)","metadata":{"_uuid":"41035425-6563-4fbc-84af-2d0fa13cb539","_cell_guid":"6e0075a9-a206-4f9f-a59f-9ccba3a1e877","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Description du dataset pour MGMT_value = 1\")\ndescribe_MGMT_1","metadata":{"_uuid":"c36d2151-6ac6-464f-b80c-6693f0bf0a0f","_cell_guid":"7496e822-6606-4620-bfb4-35ae62f2fbf2","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Comparaison des kurtosis et skewness</h3>","metadata":{"_uuid":"2614bc83-d08f-4d5c-9245-02340233a350","_cell_guid":"2aa5e04b-b6e2-4afa-ab84-71a03e9968cb","trusted":true}},{"cell_type":"code","source":"print(\"complet\")\ndescribe.loc[['skewness','excess_kurtosis'],:]","metadata":{"_uuid":"84ea6ff1-fe8b-438d-9284-6cf1521ade9b","_cell_guid":"75f2cc0d-3253-4a10-8197-7740426ea5c7","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"MGMT = 1\")\ndescribe_MGMT_1.loc[['skewness','excess_kurtosis'],:]","metadata":{"_uuid":"643448b1-8041-46a7-8571-7d1cf3e9959d","_cell_guid":"bd0a635c-245d-4477-9197-47e31ca10003","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"‚ÑπÔ∏è Certaines variables ont un meilleur skewness avec un MGMT √† 1 que par rapport au dataset global et inversement, ces m√™mes variables sont donc sensibles au marqueurs. De m√™me le Kurtosis est sensible aussi. Suivant les variables, la corr√©lation est positive ou n√©gative entre skewness et kurtosis.","metadata":{"_uuid":"a6a5d24c-3314-4212-b374-c751c95b17d5","_cell_guid":"d477cb6e-6757-4f6a-bb7e-ff4da4b52786","trusted":true}},{"cell_type":"code","source":"print(\"Par variable, variation avec MGMT_value=1 par rapport au dataset global\")\nfor col in describe.columns :\n    skew = (describe_MGMT_1.loc['skewness',col]*100/describe.loc['skewness',col])-100\n    kurt = (describe_MGMT_1.loc['excess_kurtosis',col]*100/describe.loc['excess_kurtosis',col])-100\n    print (f\"{col:<32} : skewness {skew:>8.2f}%, excess_kurtosis {kurt:>8.2f}%\")\n    \nT_describe = describe.drop(['Kurtosis'],axis=1).T\nT_MGMT_1  = describe_MGMT_1.drop(['Kurtosis','Maximum'],axis=1).T\nT_MGMT_0  = describe_MGMT_0.drop(['Kurtosis','Maximum'],axis=1).T\n\n\n#plt.plot(comparaisonNormalite.kurtosis.index, comparaisonNormalite.kurtosis.value)\n#plt.show()\nplt.figure(figsize=(12,6))\nT_describe[T_describe['kurtosis']<10]['kurtosis'].plot(label='Complet')\nT_MGMT_1[T_MGMT_1['kurtosis']<10]['kurtosis'].plot(label='MGMT_1')\nT_MGMT_0[T_MGMT_0['kurtosis']<10]['kurtosis'].plot(label='MGMT_0')\n\nplt.title('Comparaison des valeurs de Kurtosis')\nplt.legend()\nplt.show()","metadata":{"_uuid":"a5129cd9-5ea1-4a1d-b425-1ad5edba775b","_cell_guid":"6187c0d8-6013-4369-872c-7057b4305095","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"‚ÑπÔ∏è **Note** Une premi√®re conclusion pourrait √™tre qu'un gliocome dont la valeur du MGMT = 0 pr√©senterait des variables contenant plus d'outliers.\n\nIl y a des exceptions avec des variation semblant abb√©rantes comme l'excess de kurtosis pour la variable MajorAxisLength qui explose avec une augmentation de presque 1232%.\n\nCes variations pourrait indiquer les variables ayant un impact plus important par rapport √† la valeur cible. Par Exemple Sphericity √† moins d'outliers avec MGMT_value √† 1, une meilleur normalit√© d'Idn.","metadata":{"_uuid":"bdb06d18-14d2-4b45-b45c-ca6deb2fc5e9","_cell_guid":"00e941fa-ddd4-43f6-b778-4b8f575dc883","trusted":true}},{"cell_type":"markdown","source":"<h3>Exemple : comparaison de SphericalDisproportion, Entropy, Uniformity\nafin de v√©rifier la normalit√© en fonction de la valeur cible","metadata":{"_uuid":"2b854651-c37d-4d88-82c5-4a269c902f91","_cell_guid":"f38ba7a0-9711-4e12-a103-9d646f41caed","trusted":true}},{"cell_type":"code","source":"print(\"SphericalDisproportion pour MGMT_value=1\")\nfonc_test_normality(dataset[dataset.MGMT_value == 1][['SphericalDisproportion']],graphic=True)      \n\nprint(\"SphericalDisproportion pour MGMT_value=0\")\nfonc_test_normality(dataset[dataset.MGMT_value == 0][['SphericalDisproportion']],graphic=True)   \n\nprint(\"Entropy pour MGMT_value=1\")\nfonc_test_normality(dataset[dataset.MGMT_value == 1][['Entropy']],graphic=True)      \n\nprint(\"Entropy pour MGMT_value=0\")\nfonc_test_normality(dataset[dataset.MGMT_value == 0][['Entropy']],graphic=True)  \n\nprint(\"Uniformity pour MGMT_value=1\")\nfonc_test_normality(dataset[dataset.MGMT_value == 1][['Uniformity']],graphic=True)      \n\nprint(\"Uniformity pour MGMT_value=0\")\nfonc_test_normality(dataset[dataset.MGMT_value == 0][['Uniformity']],graphic=True)","metadata":{"_uuid":"fddf8d3b-5be8-426a-aae2-1b8fff640321","_cell_guid":"b5f0eb77-3bc8-4dc9-b240-f63904a2b41b","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>D√©tection des outliers</h3>\nUtilisation du IQR (interquartile range)","metadata":{"_uuid":"ecf1163a-0648-4c54-9811-8304c1aaf69b","_cell_guid":"043fa606-6c6d-44ec-a98d-3608c0c728ad","trusted":true}},{"cell_type":"code","source":"q1=dataset.quantile(0.25)\nq3=dataset.quantile(0.75)\n\nIQR=q3-q1\n\noutliers = dataset[((dataset<(q1-1.5*IQR)) | (dataset>(q3+1.5*IQR)))]\noutliers\n\noutliers_removed = outliers.dropna().reset_index()\nprint(outliers_removed)","metadata":{"_uuid":"071c27ac-80a1-48a9-a219-a53b8a0b34eb","_cell_guid":"f3e9b7ae-1c2f-4ebc-9db3-e37144743551","_kg_hide-input":false,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ----","metadata":{"_uuid":"fd06234a-a0ba-441f-8e75-743c49b53fa6","_cell_guid":"ca850e13-7546-4bc9-b988-1e62024c3663","trusted":true}},{"cell_type":"code","source":"# TODO\n# On se base sur l'une des variables quantitatives de chaque groupe retourn√© par la fonction find_highly_correlated_groups avec un seuil\n# de 0.7 afin d'avoir une matrice lisible.\n\nsns.pairplot(data=dataset[['MeshSurface', 'Elongation', 'Sphericity', 'Correlation', 'Energy', 'Autocorrelation', 'ClusterProminence', 'Entropy', 'RootMeanSquared', 'Skewness', 'Kurtosis']], diag_kind='kde')","metadata":{"_uuid":"aa5bcc71-6713-4627-a7dc-6a78ce1bd786","_cell_guid":"ff7776d5-b472-4774-9f58-fe34ca99dfd3","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO\n# Avec les m√™mes colonnes que celles utilis√©es pour le PairPlot, on r√©alise un Pair grid en s√©parant par couleur la variable explicative\n\ndf_filtered = dataset[['MGMT_value', 'MeshSurface', 'Elongation', 'Sphericity', 'Correlation', 'Energy', 'Autocorrelation', 'ClusterProminence', 'Entropy', 'RootMeanSquared', 'Skewness', 'Kurtosis']]\ndf_filtered['MGMT_value'] = df_filtered['MGMT_value'].apply(lambda x: \"Oui\" if x == 1 else \"Non\")\n\ng = sns.PairGrid(df_filtered, hue='MGMT_value')\ng.map(plt.scatter, alpha=.4)\ng.add_legend();","metadata":{"_uuid":"8d3ea589-314a-4d6b-9125-3e99671d4007","_cell_guid":"f03ccfc4-15c7-42f2-aa68-095ccbfd12e6","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üöß test 3","metadata":{"_uuid":"fddab6b8-6fe4-424e-9fcd-b3b4c7b72a51","_cell_guid":"0c8440cd-3ab9-4451-81f5-9a4996c503b9","trusted":true}},{"cell_type":"markdown","source":"MeshSurface                      : skewness    10.60%, excess_kurtosis    37.33%\nPixelSurface                     : skewness    10.46%, excess_kurtosis    36.77%\nPerimeter                        : skewness     1.65%, excess_kurtosis     2.27%\nPerimeterSurfaceRatio            : skewness     2.71%, excess_kurtosis     2.55%\nSphericity                       : skewness    85.81%, excess_kurtosis   -11.49%\nSphericalDisproportion           : skewness     8.05%, excess_kurtosis    65.98%\nMaximumDiameter                  : skewness    20.76%, excess_kurtosis   -18.67%\nMajorAxisLength                  : skewness    18.44%, excess_kurtosis -1754.60%\nMinorAxisLenth                   : skewness     8.21%, excess_kurtosis    52.19%\nElongation                       : skewness     6.41%, excess_kurtosis   -81.26%\nAutocorrelation                  : skewness     9.67%, excess_kurtosis    35.20%\nClusterProminence                : skewness    31.60%, excess_kurtosis    48.45%\nClusterShade                     : skewness    23.83%, excess_kurtosis    31.68%\nClusterTendency                  : skewness    36.08%, excess_kurtosis   105.35%\nContrast                         : skewness    -5.30%, excess_kurtosis    -1.71%\nCorrelation                      : skewness     2.16%, excess_kurtosis    63.42%\nDifferenceAverage                : skewness    -9.69%, excess_kurtosis   -37.73%\nDifferenceEntropy                : skewness    20.10%, excess_kurtosis   -42.82%\nDifferenceVariance               : skewness    -8.46%, excess_kurtosis    -3.92%\nId                               : skewness   -13.34%, excess_kurtosis    -9.83%\nIdm                              : skewness   -11.28%, excess_kurtosis   -14.69%\nIdmn                             : skewness     0.84%, excess_kurtosis    -7.63%\nIdn                              : skewness   -71.54%, excess_kurtosis   -40.69%\nImc1                             : skewness    41.19%, excess_kurtosis    68.27%\nImc2                             : skewness    11.52%, excess_kurtosis    15.82%\nInverseVariance                  : skewness   -29.58%, excess_kurtosis    -2.24%\nJointAverage                     : skewness    10.37%, excess_kurtosis   -49.45%\nJointEnergy                      : skewness     5.35%, excess_kurtosis    49.01%\nJointEntropy                     : skewness    24.60%, excess_kurtosis   -53.49%\nMCC                              : skewness    -0.55%, excess_kurtosis    55.98%\nMaximumProbability               : skewness     3.32%, excess_kurtosis   -16.05%\nSumAverage                       : skewness    10.37%, excess_kurtosis   -49.45%\nSumEntropy                       : skewness    20.02%, excess_kurtosis   -67.65%\nSumSquares                       : skewness    35.75%, excess_kurtosis   105.29%\n10Percentile                     : skewness    44.20%, excess_kurtosis   -12.22%\n90Percentile                     : skewness    -4.02%, excess_kurtosis     1.11%\nEnergy                           : skewness     6.95%, excess_kurtosis    11.98%\nEntropy                          : skewness    47.87%, excess_kurtosis   -70.16%\nInterquartileRange               : skewness    18.80%, excess_kurtosis    87.89%\nKurtosis                         : skewness   -66.52%, excess_kurtosis   -87.55%\nMaximum                          : skewness    15.54%, excess_kurtosis     9.56%\nMeanAbsoluteDeviation            : skewness     8.57%, excess_kurtosis    83.60%\nMean                             : skewness    16.10%, excess_kurtosis   -24.84%\nMedian                           : skewness     5.56%, excess_kurtosis    -7.40%\nMinimum                          : skewness   -18.25%, excess_kurtosis     4.98%\nRange                            : skewness    13.32%, excess_kurtosis   -10.82%\nRobustMeanAbsoluteDeviation      : skewness    14.10%, excess_kurtosis    92.15%\nRootMeanSquared                  : skewness    16.84%, excess_kurtosis   -32.18%\nSkewness                         : skewness   -58.03%, excess_kurtosis   -79.56%\nTotalEnergy                      : skewness     6.95%, excess_kurtosis    11.98%\nUniformity                       : skewness     9.75%, excess_kurtosis   110.36%\nVariance","metadata":{"_uuid":"c157997c-c7f8-484c-bcaa-eaf5380b717a","_cell_guid":"d2b23e01-b495-4a0f-ab27-4744cc5e1b0d","trusted":true}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport pandas as pd\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.widgets import Button\n\n\n# Assuming you have a DataFrame named 'df' with two columns 'x' and 'y'\n# You can replace these with the actual column names from your DataFrame\n\n# Create a new figure and axis\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Assign a constant value to the Z-coordinate\n\n\ncolors = ['r' if value == 1 else 'b' for value in dataset['MGMT_value']]\n\n\nimport numpy as np\nfrom sklearn.preprocessing import RobustScaler\n\n# Extract 'Entropy' and 'Elongation' columns from the dataset\nentropy_data = dataset['Entropy'].values.reshape(-1, 1)\nelongation_data = dataset['Elongation'].values.reshape(-1, 1)\n\n# ‚ö†Ô∏è no senbible au valeur aberante\nentropy_scaler = RobustScaler()\nelongation_scaler = RobustScaler()\n\n# Fit and transform the data using the scalers\nnormalized_entropy = entropy_scaler.fit_transform(entropy_data)\nnormalized_elongation = elongation_scaler.fit_transform(elongation_data)\n\n# Update the dataset with normalized values\nnew_dataset = dataset\nnew_dataset['Entropy'] = normalized_entropy\nnew_dataset['Elongation'] = normalized_elongation\nnew_dataset['MGMT_value2'] = dataset['MGMT_value']\n\n# Plot the scatter points with constant Z-coordinate\n# ‚ö†Ô∏è Separeation add z = new_dataset['MGMT_value2']\nz = 0\nax.scatter(new_dataset['Entropy'], new_dataset['Elongation'], z, c=colors)\n\n# Set labels and title\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_title('2D Scatter Plot in 3D Space')\n\n\n# Function to update the plot for each frame\ndef update(frame):\n    ax.view_init(elev=30, azim=frame)  # Set the azimuth angle for rotation\n\n# Create the animation\nanimation = FuncAnimation(fig, update, frames=range(0, 360, 5), interval=100)\n\nax.set_zscale('linear')  # Set linear scaling for the 'z' axis\nax.zaxis.set_tick_params(labelsize=12)  # Adjust the font size of the 'z' axis labels\n\n# Function to handle zoom button click event\ndef zoom(event):\n    ax.set_xlim(0, 10)  # Set the x-axis limits for zoom\n    ax.set_ylim(0, 20)  # Set the y-axis limits for zoom\n    ax.set_zlim(0, 30)  # Set the z-axis limits for zoom\n    fig.canvas.draw()  # Redraw the figure\n\n# Create a zoom button and position it outside the plot area\nzoom_button_ax = plt.axes([0.85, 0.05, 0.1, 0.075])\nzoom_button = Button(zoom_button_ax, 'Zoom')\nzoom_button.on_clicked(zoom)\n\n# Adjust the figure layout to make space for the zoom button\nplt.subplots_adjust(right=0.8)\n\n# Set the appropriate backend for animation\n#plt.rcParams['animation.html'] = 'jshtml'  # Use this for Jupyter Notebook\nplt.rcParams['animation.html'] = 'html5'  # Use this for other Python environments\n\n# Display the animation\nplt.close()  # Close the initial figure to prevent duplicate display\n# Show the plot\nanimation","metadata":{"_uuid":"db1e6310-54ae-40dd-94a3-e32e35b8da41","_cell_guid":"15169f4a-2197-44de-b237-d3c1a48845ea","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO\ndef create_kde_mgmt_pos_neg(df, x, y) : \n    \"\"\"\n    Displays 3 KDE plots that show the bivariate density using the columns x and y from a dataframe.\n\n    Args:\n        df: DataFrame, the dataframe whose 2 variables are going to be used for the KDE plot.\n        x, y: String, the two columns\n        \n    \"\"\"\n    fig = plt.figure(figsize=(21, 7))\n\n    plt.subplot(1,3,1)\n    plt.title(\"Densit√© bivari√©e de \"+y+\" par rapport √† \"+x)\n    sns.kdeplot(data=df, x=x, y=y, hue=\"MGMT_value\")\n\n    plt.subplot(1,3,2)\n    plt.title(\"Densit√© bivari√©e pour MGMT positif\")\n    sns.kdeplot(data=df[df[\"MGMT_value\"] == 1], x=x, y=y)\n\n    plt.subplot(1,3,3)\n    plt.title(\"Densit√© bivari√©e pour MGMT n√©gatif\")\n    sns.kdeplot(data=df[df[\"MGMT_value\"] == 0], x=x, y=y)\n    plt.show();\n\n    \n# On veut utiliser la densit√© bivari√© entre des variables li√©es √† la forme de la tumeur et la texture de la tumeur.\n# Cela permettrait de voir si une variable a une influence diff√©rente sur une autre variable si la valeur de MGMT est de 1 ou de 0\n\ncreate_kde_mgmt_pos_neg(dataset, \"Elongation\", \"Contrast\")\n\ncreate_kde_mgmt_pos_neg(dataset, \"MaximumDiameter\", \"Contrast\")\n\ncreate_kde_mgmt_pos_neg(dataset, \"Sphericity\", \"ClusterShade\")\n\ncreate_kde_mgmt_pos_neg(dataset, \"Elongation\", \"Idm\")\n\n# Ici, nous ne pouvons pas vraiment conclure, il ne semble pas y avoir de diff√©rence significative","metadata":{"_uuid":"75c9a40e-ccf5-42de-83bf-9a39ac6961ee","_cell_guid":"4bc7e1a1-5fb3-4a71-b53b-dc66c70243f5","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Etape 3/ Nettoyage et Pre-processing :","metadata":{"_uuid":"8ebdf8d3-3407-4ce0-8714-c9a779d9c894","_cell_guid":"9bd45cd4-b2d6-4305-93aa-471351b6543b","trusted":true}},{"cell_type":"code","source":"#TODO","metadata":{"_uuid":"e20931c0-77f1-4fda-8fd5-a73a3594c5d7","_cell_guid":"53ae312b-3a73-4912-9e2c-883eb9244038","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO","metadata":{"_uuid":"07adf01b-e58c-444b-9b8d-3ca4b4ffe276","_cell_guid":"027b76ca-b4fd-487e-a163-830677892134","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO","metadata":{"_uuid":"856b0a0d-6e4c-4743-8247-36b06fb328fe","_cell_guid":"344fc10d-bdaa-4f5e-a3d7-28173adb5eb9","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"01ac53d1-40dc-405e-8ca7-ad2df141a897","_cell_guid":"8c552b41-b9a4-4fbd-9ee7-8b20f40f302d","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}