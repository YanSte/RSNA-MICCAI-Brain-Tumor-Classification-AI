{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yannicksteph/179-rsna-miccai-brain-tumor-classification?scriptVersionId=133883185\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/yannicksteph/rsna-miccai-brain-tumor-classification\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{"_uuid":"db314319-d6c3-4adb-8c2e-45f4523acceb","_cell_guid":"41cf6070-bb6e-4ef4-93d1-bb3ca5e070b0","_kg_hide-input":false,"_kg_hide-output":false,"trusted":true}},{"cell_type":"markdown","source":"# üöß Work in progress üöß #","metadata":{"_uuid":"4d939ae4-ac76-45fb-9506-ba28a75af59d","_cell_guid":"ce89b0f8-6c29-42bb-aaba-2bab95dac7ce","trusted":true}},{"cell_type":"markdown","source":"**<center><font size=5>RSNA-MICCAI Brain Tumor Classification</font></center>**\n\n<center><img src=\"https://lingualab.ca/fr/project/language-recovery-psa/featured_hu67ab33455cf328a3b8dbb37d23762824_484672_720x0_resize_lanczos_2.png\" alt=\"equal-2495950-1920\" border=\"0\" width=\"700\"></center>\n\n****\n\n# üöß TODO TEAM / Na pas oublier de mettre les sections ‚ö†Ô∏è\n\n**Table of Contents**\n- <a href='#overview'>1. Project overview and objectives</a> \n    - <a href='#contributors'>1.1. Contributors</a>\n    - <a href='#dataset_overview'>1.2. Data overview</a>\n    - <a href='#definitions'>1.3. Imports, Methods, Paths, Reading definitions</a>\n- <a href='#exploratory_data'>2. Exploratory Data</a>\n    - <a href='#exploratory_data_2_1'>2.1. MRI  scans folders</a>\n    - <a href='#exploratory_data_2_2'>2.2. MRI slides scans previews</a>\n    - <a href='#exploratory_data_2_3'>2.3. MRI and MGMT values previews</a>\n- <a href='#brain_segmentation'>3. Brain segmentation</a>\n    - <a href='#brain_segmentation_3_1'>3.1. Importance of Brain Segmentation</a>\n    - <a href='#brain_segmentation_3_2'>3.2. Selection Criteria</a>\n    - <a href='#brain_segmentation_3_3'>3.3. Contributions of the Library</a>\n- <a href='#utilizing_unet_4'>4. Utilizing the U-Net for Brain MRI Model and RadiomicsShape2D Class</a>\n    - <a href='#utilizing_unet_4_1'>4.1. Architecture</a>\n    - <a href='#utilizing_unet_4_2'>4.2. Usage Instructions</a>\n    - <a href='#utilizing_unet_4_3'>4.3. RadiomicsShape2D Class</a>\n    - <a href='#utilizing_unet_4_4'>4.4. Usage Instructions</a>\n- <a href='#dataset_creation_5'>5. Dataset creation</a>\n- <a href='#exploratory_dataset'>6. Exploratory Dataset</a>\n- <a href='#analysis'>7. Analysis Dataset</a>\n    - <a href='#analysis_7_1'>7.1. Average Patient MGMT Comparison</a> \n    - <a href='#analysis_7_2'>7.2. Correlation Matrix</a> \n    - <a href='#analysis_7_3'>7.3. Univariate analysis</a>  \n    - <a href='#analysis_7_4'>7.4. Bivariate analysis</a>           \n- <a href='#clean_preprocessing_8'>8. Clean & Preprocessing</a>\n\n\n****","metadata":{"_uuid":"2118c681-7e49-48f6-89a0-ccdb8a0312f5","_cell_guid":"a966507d-9558-42ea-813a-d84643cc1bee","trusted":true}},{"cell_type":"markdown","source":"# <a id='overview'>1. Project overview and objectives</a>\n\n### Overview:\n\nA malignant brain tumor is a life-threatening condition, specifically glioblastoma, which is the most common and has the poorest prognosis among adult brain cancers, with a median survival of less than a year. The presence of a specific genetic sequence called MGMT promoter methylation in the tumor has been identified as a favorable prognostic factor and a strong predictor of chemotherapy responsiveness.\n\nCurrently, the genetic analysis of cancer requires a surgical procedure to obtain a tissue sample, followed by a time-consuming process of determining the genetic characteristics of the tumor, which can take several weeks. Depending on the results and the chosen initial treatment, additional surgeries may be necessary. Developing an accurate method to predict the genetic profile of the cancer solely through imaging (known as radiogenomics) would potentially reduce the number of surgeries and allow for a more tailored therapy approach.\n\nThe Radiological Society of North America (RSNA) and the Medical Image Computing and Computer Assisted Intervention Society (MICCAI Society) have collaborated to enhance the diagnosis and treatment planning for glioblastoma patients.\n\n### Competition:\n\nIn this competition, participants are tasked with using MRI (magnetic resonance imaging) scans to train and test a model that can predict the genetic subtype of glioblastoma by detecting the presence of MGMT promoter methylation.\n\nSuccessful outcomes from this competition could significantly contribute to less invasive diagnoses and treatments for brain cancer patients. Introducing new and personalized treatment strategies before surgery holds the potential to improve the management, survival rates, and overall prospects of individuals affected by brain cancer.\n\n### Acknowledgments:\n\nThe Radiological Society of North America (RSNA¬Æ) is a non-profit organization representing 31 radiologic subspecialties from 145 countries worldwide. RSNA promotes excellence in patient care and healthcare delivery through education, research, and technological innovation.\n\nRSNA provides high-quality educational resources, publishes five top peer-reviewed journals, hosts the world's largest radiology conference, and is dedicated to shaping the future of the profession through the RSNA Research & Education (R&E) Foundation, which has funded $66 million in grants since its establishment. Additionally, RSNA actively supports and facilitates research in medical imaging artificial intelligence (AI) by sponsoring ongoing AI challenge competitions.\n\nThe Medical Image Computing and Computer Assisted Intervention Society (MICCAI Society) is committed to advancing research, education, and practice in the field of medical image computing, computer-assisted interventions, biomedical imaging, and medical robotics. The society achieves this objective by organizing high-quality international conferences, workshops, tutorials, and publications that promote the exchange and dissemination of advanced knowledge, expertise, and experiences produced by leading institutions, scientists, physicians, and educators worldwide.\n\nA complete list of acknowledgments can be found on this page.\n\n[RSNA-MICCAI Brain Tumor Radiogenomic Classification](https://www.kaggle.com/competitions/rsna-miccai-brain-tumor-radiogenomic-classification/data?select=train_labels.csv)\n\n## <a id='contributors'>1.1. Contributors</a>\n\n- [David Goudard](https://www.kaggle.com/goudgoud)\n- [Louis-Marie Renaud](https://www.kaggle.com/louismarierenaud)\n- [Yannick Stephan](https://github.com/YanSteph)\n\n\n## <a id='contributors'>1.2. Note & Global</a>\n\n<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n   <div class=\"card-body\">\n      <h2 class=\"card-title\" style=\"color: white;\"><strong>üìù Note:</strong></h2>\n      <p style=\"color: white;\">\n         This \"Note\" block contains information and conclusions based on the analysis of previous results.<br>It provides important remarks and points to consider for proper interpretation of the data and conclusions drawn.\n      </p>\n   </div>\n</div>\n<br>\n<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n   <div class=\"card-body\">\n      <h2 class=\"card-title\" style=\"color: white;\"><strong>üõ† Global:</strong></h2>\n      <p style=\"color: white;\">\n         This \"Global\" block contains global information and settings relevant to the project.<br>It provides details on the configuration and global aspects to consider when analyzing data and developing models.\n      </p>\n   </div>\n</div>\n\n## <a id='contributors'>1.3. Abbreviations List</a>\n\n- **ADC** : Apparent Diffusion Coefficient\n- **ASL** : Arterial Spin Labelling\n- **AUC** : Area Under the Curve \n- **DSC** : Dynamic Susceptibility Contrast \n- **DTI** : Diffusion-Tensor Imaging \n- **DWI** : Diffusion-Weighted Imaging \n- **FLAIR** : Fluid-attenuated Inversion Recovery\n- **GLCM** : Gray Level Co-occurrence Matrix\n- **GLRLM** : Grey Level Run-Length Matrix \n- **MD** : Mean Diffusivity\n- **MNI** : Montreal Neurological Institute\n- **ROC** : Receiver Operating Characteristic\n- **ROI**: Region Of Interest","metadata":{"_uuid":"4339bd0b-a195-4387-8b52-b7a1e5e1e1e5","_cell_guid":"d486cb1b-8208-47e6-a774-527c33a1d615","execution":{"iopub.status.busy":"2023-05-14T19:19:16.412499Z","iopub.execute_input":"2023-05-14T19:19:16.41328Z","iopub.status.idle":"2023-05-14T19:19:17.264665Z","shell.execute_reply.started":"2023-05-14T19:19:16.413115Z","shell.execute_reply":"2023-05-14T19:19:17.263011Z"},"trusted":true}},{"cell_type":"markdown","source":"## <a id='definitions'>1.4. Imports, Methods, Paths, Reading definitions</a>","metadata":{"_uuid":"3482f72a-bcc3-4b15-b40a-0e0cdab1ad5a","_cell_guid":"5917792b-b306-4f91-b295-c0efa1c11a20","trusted":true}},{"cell_type":"markdown","source":"### Imports","metadata":{"_uuid":"5bd316fd-abe4-494d-be6d-879ae4fb4e0a","_cell_guid":"63f21d42-4549-41eb-a66a-cf76bdb7f60e","trusted":true}},{"cell_type":"code","source":"# Operating System and File System\nimport os \n\n# Basic\nimport math\nfrom enum import Enum\nfrom itertools import chain\n\n# Data Manipulation and Analysis\nimport numpy as np  \nimport pandas as pd\n\n# Data Visualization\nimport matplotlib.pyplot as plt \n# Animation Matplotlib\nimport matplotlib.animation as anim\nimport seaborn as sns\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Warnings\nimport warnings  # For suppressing warnings\n\n# JSON Handling\nimport json  # For working with JSON data\n\n# Encoding and Decoding Binary Data\nimport base64  # For encoding and decoding binary data\n\n# Interactive Widgets and Display\nimport ipywidgets as widgets  # For creating interactive widgets in Jupyter Notebook\nfrom IPython.display import HTML, display  # For displaying HTML content\nfrom IPython.display import Image as show_gif # GIF\n\n# Deep Learning Framework\nimport torch  # For working with PyTorch deep learning framework\n\n# DICOM File Handling\nimport pydicom  # For reading DICOM files\nfrom pydicom import dcmread  # For reading DICOM files\n\n# Image Processing and Filtering\nimport SimpleITK as sitk  # For image filtering\nfrom PIL import Image  # For image processing using the Python Imaging Library (PIL)\n\n# Machine Learning and Data Splitting\nfrom sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n\nfrom scipy import stats\nfrom statsmodels.stats.diagnostic import lilliefors\nimport statsmodels.api as sm\n\nimport itertools  \nfrom itertools import permutations\n\n# sklearn\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n# scipy\nimport scipy.cluster.hierarchy as sch\nfrom scipy.cluster.hierarchy import fcluster, linkage\n\n# Additional Libraries\nimport glob # Fetch data recusif\nimport random\n\n!pip install pyradiomics > /dev/null  # Installing the pyradiomics library for radiomics feature extraction\nimport radiomics  # For extracting radiomics features from medical images","metadata":{"_uuid":"7ee05952-0cc5-464f-ad60-40778f96252e","_cell_guid":"6fe6a9d6-9a01-4516-bd63-de3387ea5362","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-06-17T08:10:00.19086Z","iopub.execute_input":"2023-06-17T08:10:00.192219Z","iopub.status.idle":"2023-06-17T08:10:11.885271Z","shell.execute_reply.started":"2023-06-17T08:10:00.192177Z","shell.execute_reply":"2023-06-17T08:10:11.883998Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"### Paths","metadata":{}},{"cell_type":"code","source":"# ------------------------------------#    \n# Definition Version\n# ------------------------------------#    \n\nclass DatasetVersion(Enum):\n    V_2D = '2D'\n    V_3D = '3D'\n    \n# ------------------------------------#  \n# Paths\n# ------------------------------------#  \n    \n# ------------#\n# Folders\n# ------------# \nbase_folder_path = \"../input/rsna-miccai-brain-tumor-radiogenomic-classification/\"\ntrain_folder_path = f\"{base_folder_path}train/\"\ndataset_folder_path = \"../input/githut-rsna-miccai-brain-tumor-classification-ai/dataset/\"\n\n# ------------#\n# Train / files\n# ------------# \ntrain_labels_path = f\"{base_folder_path}train_labels.csv\"\nsubmission_labels_path = f\"{base_folder_path}sample_submission.csv\"\n\ndataset_3d_path = f\"{dataset_folder_path}3d_rsna_miccai_brain_tumor_brain_segmentation_pytorch_unet.csv\"\ndataset_2d_path = f\"{dataset_folder_path}2d_rsna_miccai_brain_tumor_brain_segmentation_pytorch_unet.csv\"","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-17T08:10:11.888607Z","iopub.execute_input":"2023-06-17T08:10:11.889001Z","iopub.status.idle":"2023-06-17T08:10:11.897204Z","shell.execute_reply.started":"2023-06-17T08:10:11.888961Z","shell.execute_reply":"2023-06-17T08:10:11.895787Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üõ† Global:</strong></h2>\n        <ul>\n            <li>Train folder of RSNA: <b>`train_folder_path`</b></li>\n            <li>Generated Train labels Segmentation 3D CSV: <b>`train_labels_path`</b></li>\n        </ul>\n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"### Constants","metadata":{}},{"cell_type":"code","source":"# ------------------------------------#   \n# Configuration\n# ------------------------------------#    \n    \n# ‚ÑπÔ∏è Flag to skip brain segmentation with PyTorch UNet\n# If set to True, we will import the dataset that has already been generated\nskip_brain_segmentation_pytorch_unet = True\n\n# ‚ÑπÔ∏è Set the dataset version to use when <skip_brain_segmentation_pytorch_unet = True>\ndataset_version =  DatasetVersion.V_3D\n\n#Color\ndefault_palette_color = \"deep\"","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-17T08:10:11.89881Z","iopub.execute_input":"2023-06-17T08:10:11.899166Z","iopub.status.idle":"2023-06-17T08:10:11.914392Z","shell.execute_reply.started":"2023-06-17T08:10:11.899121Z","shell.execute_reply":"2023-06-17T08:10:11.913476Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üõ† Global:</strong></h2>\n        <ul>\n            <li>Flag to skip brain segmentation with PyTorch UNet: <b>`skip_brain_segmentation_pytorch_unet`</b></li>\n            <li>Flag to set the dataset version 2D or 3D: <b>`dataset_version`</b></li>\n            <li>üëÜüèª Configuration of the project</li>\n        </ul>\n    </div>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"### Configure","metadata":{"_uuid":"228f3522-329f-4672-9102-108bfece2be9","_cell_guid":"2bc00e09-7471-4c5a-9608-0208530070ef","trusted":true}},{"cell_type":"code","source":"# ------------------------------------#\n# Configuration options\n# ------------------------------------#\n\n# Pandas configuration\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)\npd.set_option('display.max_rows', None)\n\n# Suppressing Warnings\nwarnings.filterwarnings('ignore')\n\n# Define \"reader\"\n# Read serie of image files into a SimpleTK image\nsitk_reader = sitk.ImageSeriesReader()\nsitk_reader.LoadPrivateTagsOn()\n\n# Suppressing warnings SimpleITK\nsitk.ProcessObject.SetGlobalWarningDisplay(False)\n\n# Default color\nsns.set_theme(palette=default_palette_color)\n\n# ------------\n# Segmentation\n# ------------\n\n# Load mateuszbuda/brain-segmentation-pytorch, U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI\nsegmentation_model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', \n                                    'unet', \n                                    in_channels=3, \n                                    out_channels=1, \n                                    init_features=32, \n                                    pretrained=True, \n                                    trust_repo=True)\n\n# ------------\n# Dataset\n# ------------\n# Dataset of the project, explanation in next section.    \npreview_dataset_df = pd.read_csv(train_labels_path)\nsamp_subm = pd.read_csv(submission_labels_path)","metadata":{"_uuid":"7caa653c-b826-4e99-852e-c22b28757274","_cell_guid":"ddf3ae13-9bc9-47e5-a6be-7c343074b818","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-06-17T08:10:11.916523Z","iopub.execute_input":"2023-06-17T08:10:11.917174Z","iopub.status.idle":"2023-06-17T08:10:12.27605Z","shell.execute_reply.started":"2023-06-17T08:10:11.91714Z","shell.execute_reply":"2023-06-17T08:10:12.274997Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üõ† Global:</strong></h2>\n        <ul>\n            <li>Dataset of the competition: <b>`preview_dataset_df`</b></li>\n        </ul>\n    </div>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"### Methods","metadata":{"_uuid":"5f027225-6e9c-4b3a-9bdb-9b32287648ad","_cell_guid":"4b803e1c-e1f6-46f4-8d39-a2b4eb2900d7","trusted":true}},{"cell_type":"code","source":"# ===============================================\n# Base methods\n# ===============================================\n\ndef is_even(nombre):\n    return nombre % 2 == 0\n\ndef map_dataframe_to_tuples(df, columns):\n    mapped_tuples = []\n    for row in df.itertuples(index=False):\n        mapped_tuple = tuple(getattr(row, col) for col in columns)\n        mapped_tuples.append(mapped_tuple)\n    return mapped_tuples\n\ndef extract_number_from_filename(filename):\n    # Extrayez les chiffres √† la fin du nom de fichier\n    number_str = ''.join(filter(str.isdigit, filename))\n    if number_str:\n        return int(number_str)\n    else:\n        return -1  # Si aucun chiffre n'est trouv√©, renvoyez une valeur par d√©faut\n    import os\n    \ndef remove_values(values_to_remove, source_array):\n    \"\"\"\n    Removes elements from the source_array that are present in the values_to_remove.\n    \n    Args:\n        values_to_remove (list): The array containing the values to be removed.\n        source_array (list): The source array containing the values.\n\n    \n    Returns:\n        list: The source array with the matching values removed.\n    \"\"\"\n    for value in values_to_remove:\n        if value in source_array:\n            source_array.remove(value)\n    return source_array\n\n# ===============================================\n# Images methods\n# ===============================================\n\ndef get_processed_image(patient_id, dataset_version):\n    \"\"\"\n    Retrieves and processes the images for a given patient, grouping them for segmentation.\n\n    Args:\n        patient_id (str): The ID of the patient (BraTS21ID).\n\n    Returns:\n        numpy.ndarray: A processed image composed of the different images of the patient.\n    \"\"\"\n    # SEGMENTATION MODEL LIMITED TO 3 LAYERS\n    # T2W SKIPPED\n    patient_id = int(patient_id)\n\n    # Paths for image sequences\n    t1w_path = f'{train_folder_path}/{str(patient_id).zfill(5)}/T1w'\n    #t2w_path = f'{train_folder_path}/{str(patient_id).zfill(5)}/T2w'\n\n    flair_path = f'{train_folder_path}/{str(patient_id).zfill(5)}/FLAIR'\n    t1wce_path = f'{train_folder_path}/{str(patient_id).zfill(5)}/T1wCE'\n\n    # Retrieve image sequences\n    if dataset_version == DatasetVersion.V_2D:\n        t1w_image = sequence_filenames(t1w_path)\n        flair_image = sequence_filenames(flair_path)\n        t1wce_image = sequence_filenames(t1wce_path)\n        t2w_image = None\n    else:\n        t1w_image = sequence_filenames(t1w_path)\n        flair_image = sequence_filenames(flair_path)\n        t1wce_image = sequence_filenames(t1wce_path)\n        t2w_image = None\n        \n    # Resampling\n    if dataset_version == DatasetVersion.V_2D:\n        re_sampled_flair = re_sample_image(flair_image, t1w_image)\n        re_sampled_t1wce = re_sample_image(t1wce_image, t1w_image)\n        t2w_array = None\n    else:\n        re_sampled_flair = re_sample_image(flair_image, t1w_image)\n        re_sampled_t1wce = re_sample_image(t1wce_image, t1w_image)\n        t2w_array = None\n\n    \n    # Normalization\n    if dataset_version == DatasetVersion.V_2D:\n        t1w_array = normalize(sitk.GetArrayFromImage(t1w_image))\n        re_sampled_flair_array = normalize(sitk.GetArrayFromImage(re_sampled_flair))\n        re_sampled_t1wce_array = normalize(sitk.GetArrayFromImage(re_sampled_t1wce))\n        re_sampled_t2w_array = None\n    else:\n        t1w_array = normalize(sitk.GetArrayFromImage(t1w_image))\n        re_sampled_flair_array = normalize(sitk.GetArrayFromImage(re_sampled_flair))\n        re_sampled_t1wce_array = normalize(sitk.GetArrayFromImage(re_sampled_t1wce))\n        re_sampled_t2w_array = None\n\n    # Stack sequences\n    if dataset_version == DatasetVersion.V_2D:\n        sequence_stacked = np.stack([t1w_array, re_sampled_flair_array, re_sampled_t1wce_array])\n        central_slice = t1w_array.shape[0] // 2\n    else:\n        sequence_stacked = np.stack([t1w_array, re_sampled_flair_array, re_sampled_t1wce_array])\n        central_slice = t1w_array.shape[0] // 2\n\n    rvb = sequence_stacked[:, central_slice, :, :].transpose(1, 2, 0)\n    image = Image.fromarray((rvb * 255).astype(np.uint8))\n    return np.array([np.moveaxis(np.array(image.resize((256, 256))), -1, 0)])\n\n\n    \ndef sequence_filenames(path) :\n    \"\"\"\n    Retrieves a sequence of images for a given directory.\n\n    Args:\n        path (str): The path to the directory containing the DICOM data set.\n\n    Returns:\n        SimpleITK.Image: A sequence of images corresponding to the DICOM files in the directory.\n\n    Raises:\n        FileNotFoundError: If the specified path does not exist.\n    \"\"\"\n    filenames = sitk_reader.GetGDCMSeriesFileNames(path)\n    sitk_reader.SetFileNames(filenames)\n    image = sitk_reader.Execute()\n    \n    return image    \n\ndef normalize(dataset) :\n    \"\"\"\n    Normalizes the data obtained from the images.\n\n    Args:\n        dataset (numpy.ndarray): The dataset to be normalized.\n\n    Returns:\n        numpy.ndarray: The normalized dataset.\n    \"\"\"\n    return (dataset - np.min(dataset)) / (np.max(dataset) - np.min(dataset))\n\n\ndef re_sample_image(image, ref_img):\n    \"\"\"\n    Resamples the image to match the dimensions and properties of the reference image.\n\n    Args:\n        image (SimpleITK.Image): The image to be resampled.\n        ref_img (SimpleITK.Image): The reference image used for resampling.\n\n    Returns:\n        SimpleITK.Image: The resampled image.\n    \"\"\"\n    re_sampler = sitk.ResampleImageFilter()\n    re_sampler.SetReferenceImage(ref_img)\n    re_sampler.SetDefaultPixelValue(image.GetPixelIDValue())\n    re_sampler.SetInterpolator(sitk.sitkLinear)\n    re_sampler.SetOutputSpacing(ref_img.GetSpacing())\n    re_sampler.SetOutputDirection(ref_img.GetDirection())\n    re_sampler.SetOutputOrigin(ref_img.GetOrigin())\n    re_sampler.SetSize(ref_img.GetSize())\n    re_sampler.SetTransform(sitk.AffineTransform(image.GetDimension()))\n    re_sampled_image = re_sampler.Execute(image)\n    \n    return re_sampled_image\n\ndef segmentation_process(image_resized):\n    \"\"\"\n    Obtains the segmented image.\n\n    Args:\n        image_resized (numpy.ndarray): The resized image.\n\n    Returns:\n        numpy.ndarray: The segmented image.\n    \"\"\"\n    segmentation = segmentation_model(torch.Tensor(image_resized))\n    return segmentation\n    \n# ===============================================\n# Dataset creation methods\n# ===============================================\n\ndef init_dataset_radiomics() :\n    \"\"\"\n    Initializes the DataFrame structures for radiomics data acquisition.\n\n    Returns:\n        None\n    \"\"\"\n    global df_shapes\n    global df_textures\n    global df_first_orders_features\n    \n    if dataset_version == DatasetVersion.V_3D:\n        df_shapes_columns = ['ID','BraTS21ID','MeshVolume','VoxelVolume','SurfaceArea','SurfaceVolumeRatio','Sphericity',\n                             'Compactness1','Compactness2','SphericalDisproportion','Maximum3DDiameter','Maximum2DDiameterRow',\n                             'Maximum2DDiameterColumn','MajorAxisLength','MinorAxisLenth','LeastAxisLength','Elongation','Flatness']\n\n    else :\n        df_shapes_columns = ['ID','BraTS21ID','MeshSurface','PixelSurface','Perimeter','PerimeterSurfaceRatio','Sphericity',\n                             'SphericalDisproportion','MaximumDiameter','MajorAxisLength','MinorAxisLenth','Elongation']\n\n    \n    df_shapes = pd.DataFrame(columns=df_shapes_columns) \n\n    # GLCM features\n    df_textures_columns = ['ID','Autocorrelation', 'ClusterProminence', 'ClusterShade', 'ClusterTendency', 'Contrast', \n                           'Correlation', 'DifferenceAverage', 'DifferenceEntropy', 'DifferenceVariance', 'Id', 'Idm', \n                           'Idmn', 'Idn', 'Imc1', 'Imc2', 'InverseVariance', 'JointAverage', 'JointEnergy', 'JointEntropy', \n                           'MCC', 'MaximumProbability', 'SumAverage', 'SumEntropy', 'SumSquares']\n    df_textures = pd.DataFrame(columns=df_textures_columns) \n\n\n    # FIRST ORDERS features\n    df_first_orders_features_columns=['ID','10Percentile', '90Percentile', 'Energy', 'Entropy', 'InterquartileRange', 'Kurtosis', \n                                      'Maximum', 'MeanAbsoluteDeviation', 'Mean', 'Median', 'Minimum', 'Range', 'RobustMeanAbsoluteDeviation', \n                                      'RootMeanSquared', 'Skewness', 'TotalEnergy', 'Uniformity', 'Variance']\n    df_first_orders_features = pd.DataFrame(columns=df_first_orders_features_columns) \n\n    # gzlm features\n    df_gzlm_features_columns = ['ID','SmallAreaEmphasis','LargeAreaEmphasis','GrayLevelNonUniformity','GrayLevelNonUniformityNormalized',\n                                'SizeZoneNonUniformity','SizeZoneNonUniformityNormalized','ZonePercentage','GrayLevelVariance','ZoneVariance',\n                                'ZoneEntropy','LowGrayLevelZoneEmphasis','HighGrayLevelZoneEmphasis','SmallAreaLowGrayLevelEmphasis',\n                                'SmallAreaHighGrayLevelEmphasis','LargeAreaLowGrayLevelEmphasis','LargeAreaHighGrayLevelEmphasis']\n    df_gzlm_features = pd.DataFrame(columns=df_gzlm_features_columns)\n    \n    #glrlm_features\n    df_glrlm_features_columns = ['ID','ShortRunEmphasis','LongRunEmphasis','GrayLevelNonUniformity','RunLengthNonUniformity','RunLengthNonUniformityNormalized'\n                                 ,'RunPercentage','GrayLevelVariance','RunVariance','RunEntropy','LowGrayLevelRunEmphasis','HighGrayLevelRunEmphasis'\n                                 ,'ShortRunLowGrayLevelEmphasis','ShortRunHighGrayLevelEmphasis','LongRunLowGrayLevelEmphasis','LongRunHighGrayLevelEmphasis']\n    df_glrlm_features = pd.DataFrame(columns=df_glrlm_features_columns)\n    # ngtdm features\n    df_ngtdm_features_columns = ['ID','Coarseness','Contrast','Busyness','Complexity','Strength']\n    df_ngtdm_features = pd.DataFrame(columns=df_ngtdm_features_columns)\n    \n    # gldm features\n    df_gldm_features_columns = ['ID','SmallDependenceEmphasis','LargeDependenceEmphasis','GrayLevelNonUniformity','GrayLevelNonUniformityNormalized','DependenceNonUniformity'\n                               , 'DependenceNonUniformityNormalized', 'GrayLevelVariance', 'DependenceVariance', 'DependenceEntropy', 'DependencePercentage'\n                                , 'LowGrayLevelEmphasis', 'HighGrayLevelEmphasis', 'SmallDependenceLowGrayLevelEmphasis', 'SmallDependenceHighGrayLevelEmphasis'\n                               , 'LargeDependenceLowGrayLevelEmphasis', 'LargeDependenceHighGrayLevelEmphasis']\n    df_gldm_features = pd.DataFrame(columns=df_gldm_features_columns)\n    \n\ndef add_patient_data2D(ID,img_resized,segmentation) :\n    \"\"\"\n    Adds data from the specified patient's images to the analysis dataset.\n\n    Args:\n        ID (int): The ID of the patient.\n        img_resized (numpy.ndarray): Resized image of the patient.\n        segmentation (torch.Tensor): Segmentation of the patient's image.\n\n    Returns:\n        None\n    \"\"\"\n    global df_shapes\n    global df_textures\n    global df_first_orders_features\n    \n    # shape\n    results = radiomics.shape2D.RadiomicsShape2D(\n        sitk.GetImageFromArray(img_resized), \n        sitk.GetImageFromArray(np.array([\n            segmentation[0][0].detach().cpu().numpy() > 0.5\n        ]).astype(np.uint8)),\n        force2D=True\n    )\n    \n    shape2D = {}\n    shape2D['ID'] = int(ID)\n    shape2D['BraTS21ID'] = int(ID)\n    shape2D['MeshSurface'] = results.getMeshSurfaceFeatureValue()\n    shape2D['PixelSurface'] = results.getPixelSurfaceFeatureValue()\n    shape2D['Perimeter'] = results.getPerimeterFeatureValue()\n    shape2D['PerimeterSurfaceRatio'] = results.getPerimeterSurfaceRatioFeatureValue()\n    shape2D['Sphericity'] = results.getSphericityFeatureValue()\n    shape2D['SphericalDisproportion'] = results.getSphericalDisproportionFeatureValue()\n    shape2D['MaximumDiameter'] = results.getMaximumDiameterFeatureValue()\n    shape2D['MajorAxisLength'] = results.getMajorAxisLengthFeatureValue()\n    shape2D['MinorAxisLenth'] = results.getMinorAxisLengthFeatureValue()\n    shape2D['Elongation'] = results.getElongationFeatureValue()\n    \n    df_shapes=df_shapes.append(shape2D,ignore_index=True)\n    \n    # GLCM\n    results=radiomics.glcm.RadiomicsGLCM(\n        sitk.GetImageFromArray(img_resized[0,0,:,:].reshape(1, 256, 256)), \n        sitk.GetImageFromArray(np.array([\n            segmentation[0][0].detach().cpu().numpy() > 0.5\n        ]).astype(np.uint8)),\n        force2D=True\n    )\n\n    results.enableAllFeatures()\n    res = results.execute()\n    res['ID']=int(ID)\n\n    df_textures=df_textures.append(res,ignore_index=True)\n    \n    # First-orders features\n    results =  radiomics.firstorder.RadiomicsFirstOrder(\n        sitk.GetImageFromArray(img_resized[0,0,:,:].reshape(1, 256, 256)), \n        sitk.GetImageFromArray(np.array([\n            segmentation[0][0].detach().cpu().numpy() > 0.5\n        ]).astype(np.uint8)),\n        force2D=True\n    )\n\n    results.enableAllFeatures()\n    res = results.execute()\n    res['ID']=int(ID)\n\n    df_first_orders_features=df_first_orders_features.append(res,ignore_index=True)\n    \n\ndef add_patient_data3D(ID,img_resized,segmentation) :\n    \"\"\"\n    Adds data from the specified patient's images to the analysis dataset.\n\n    Args:\n        ID (int): The ID of the patient.\n        img_resized (numpy.ndarray): Resized image of the patient.\n        segmentation (torch.Tensor): Segmentation of the patient's image.\n\n    Returns:\n        None\n    \"\"\"\n    global df_shapes\n    global df_textures\n    global df_first_orders_features\n    global df_gzlm_features\n    global df_glrlm_features\n    global df_ngtdm_features\n    global df_gldm_features\n\n    # shape\n    results = radiomics.shape.RadiomicsShape(\n        sitk.GetImageFromArray(img_resized), \n        sitk.GetImageFromArray(np.array([\n            segmentation[0][0].detach().cpu().numpy() > 0.5\n        ]).astype(np.uint8))#,\n        #force2D=True\n    )\n    \n    shape3D = {}\n    shape3D['ID'] = int(ID)\n    shape3D['BraTS21ID'] = int(ID)\n    shape3D['MeshVolume'] = results.getMeshVolumeFeatureValue()\n    shape3D['VoxelVolume'] = results.getVoxelVolumeFeatureValue()\n    shape3D['SurfaceArea'] = results.getSurfaceAreaFeatureValue()\n    shape3D['SurfaceVolumeRatio']=results.getSurfaceVolumeRatioFeatureValue()\n    shape3D['Sphericity'] = results.getSphericityFeatureValue()\n    shape3D['Compactness1']=results.getCompactness1FeatureValue()\n    shape3D['Compactness2']=results.getCompactness2FeatureValue()\n    shape3D['SphericalDisproportion']=results.getSphericalDisproportionFeatureValue()\n    shape3D['Maximum3DDiameter']=results.getMaximum3DDiameterFeatureValue()\n    shape3D['Maximum2DDiameterRow']=results.getMaximum2DDiameterRowFeatureValue()\n    shape3D['Maximum2DDiameterColumn']=results.getMaximum2DDiameterColumnFeatureValue()\n    shape3D['MajorAxisLength'] = results.getMajorAxisLengthFeatureValue()\n    shape3D['MinorAxisLenth'] = results.getMinorAxisLengthFeatureValue()\n    shape3D['LeastAxisLength'] = results.getLeastAxisLengthFeatureValue()\n    shape3D['Elongation'] = results.getElongationFeatureValue()\n    shape3D['Flatness'] = results.getFlatnessFeatureValue()\n  \n    df_shapes=df_shapes.append(shape3D,ignore_index=True)\n    \n    # GLCM\n    results=radiomics.glcm.RadiomicsGLCM(\n        sitk.GetImageFromArray(img_resized[0,0,:,:].reshape(1, 256, 256)), \n        sitk.GetImageFromArray(np.array([\n            segmentation[0][0].detach().cpu().numpy() > 0.5\n        ]).astype(np.uint8))#,\n        #force2D=True\n    )\n\n    results.enableAllFeatures()\n    res = results.execute()\n    res['ID']=int(ID)\n    \n    df_first_orders_features=df_first_orders_features.append(res,ignore_index=True)  \n    \n    # GLSZM features\n    results = radiomics.glszm.RadiomicsGLSZM(\n    \n        sitk.GetImageFromArray(img_resized[0,0,:,:].reshape(1, 256, 256)), \n        sitk.GetImageFromArray(np.array([\n            segmentation[0][0].detach().cpu().numpy() > 0.5\n        ]).astype(np.uint8))#,\n        #force2D=True\n    )\n    results.enableAllFeatures()\n    res = results.execute()\n    res['ID']=int(ID)\n    df_gzlm_features = df_gzlm_features.append(res,ignore_index=True)\n    \n    # GLRLM features\n    results = radiomics.glrlm.RadiomicsGLRLM(\n         sitk.GetImageFromArray(img_resized[0,0,:,:].reshape(1, 256, 256)), \n        sitk.GetImageFromArray(np.array([\n            segmentation[0][0].detach().cpu().numpy() > 0.5\n        ]).astype(np.uint8))#,\n        #force2D=True\n    )\n    results.enableAllFeatures()\n    res = results.execute()\n    res['ID']=int(ID)\n    df_glrlm_features = df_glrlm_features.append(res,ignore_index=True)\n    \n    # NGTDM features\n    results = radiomics.ngtdm.RadiomicsNGTDM(\n        sitk.GetImageFromArray(img_resized[0,0,:,:].reshape(1, 256, 256)), \n        sitk.GetImageFromArray(np.array([\n            segmentation[0][0].detach().cpu().numpy() > 0.5\n        ]).astype(np.uint8))#,\n        #force2D=True\n    )\n    results.enableAllFeatures()\n    res = results.execute()\n    res['ID']=int(ID)\n    df_ngtdm_features = df_ngtdm_features.append(res,ignore_index=True)\n    \n    # GLDM features\n    results=radiomics.gldm.RadiomicsGLDM(\n        sitk.GetImageFromArray(img_resized[0,0,:,:].reshape(1, 256, 256)), \n        sitk.GetImageFromArray(np.array([\n            segmentation[0][0].detach().cpu().numpy() > 0.5\n        ]).astype(np.uint8))#,\n        #force2D=True\n    )\n    results.enableAllFeatures()\n    res = results.execute()\n    res['ID']=int(ID)\n    df_gldm_features = df_gldm_features.append(res,ignore_index=True) \n\n    \n# ===============================================\n# Show methods\n# ===============================================\n    \ndef show_brains(patient_id_mgmt_tuple_list, figsize, dataset_version, scans_cmap=['red', 'yellow'], show_segmentation=True, segmentation_cmap=\"Reds\"):\n    \"\"\"\n    Display a preview of patients from the given dataset.\n\n    Args:\n        patient_id_mgmt_tuples (list): List of tuples containing patient IDs and MGMT values. (Id, MGMT)\n        dataset_version (str): Version of the dataset (2D or 3D).\n    \"\"\"\n    # D√©finir le th√®me seaborn avec la palette personnalis√©e\n    custom_palette = sns.color_palette(scans_cmap)\n    sns.set_palette(custom_palette)\n    \n    # Loading\n    loading_max = len(patient_id_mgmt_tuple_list)\n    progress_bar = widgets.IntProgress(min=0, max=loading_max, description='Processing patients:', bar_style='info')\n    display(progress_bar)\n\n    # Process and display images for each patient\n    for i, patient_id_mgmt_value in enumerate(patient_id_mgmt_tuple_list, start=1):\n        # Process images for patient with MGMT value\n        patient_id = patient_id_mgmt_value[0]\n        patient_mgmt = patient_id_mgmt_value[1]\n        img_resized = get_processed_image(patient_id, dataset_version)\n        \n        if dataset_version == DatasetVersion.V_3D:\n            titles = ['T1w', 'FLAIR', 'T1wce', 'Segmentation'] # T2w\n        else:\n            titles = ['T1w', 'FLAIR', 'T1wce', 'Segmentation'] # T2w\n        \n        # Create the main figure\n        fig = plt.figure(figsize = figsize)\n\n        # Adjust top margin for the main figure\n        fig.subplots_adjust(top=0.85)\n\n        # Set the global title\n        fig.suptitle(f\"Patient {patient_id} and MGMT = {patient_mgmt}\", y=0.75)\n\n        x_size_subplot = 4 if show_segmentation else 3  \n        # Iterate over the source images\n        for i in range(3):\n            ax = fig.add_subplot(1, x_size_subplot, 1+i, )\n            \n            ax.imshow(img_resized[0, i])\n            ax.set_title(titles[i])\n            ax.set_xticks([])\n            ax.set_yticks([])\n            \n        if show_segmentation:\n            segmentation = segmentation_process(img_resized)\n            # Add the segmentation image\n            ax_segmentation = fig.add_subplot(1, 4, 4)\n            ax_segmentation.imshow(segmentation.detach().numpy()[0, 0], cmap=segmentation_cmap)\n            ax_segmentation.set_title(titles[3])\n            ax_segmentation.set_xticks([])\n            ax_segmentation.set_yticks([])\n   \n        plt.tight_layout()\n        plt.show()\n        \n        progress_bar.value = i + 1\n\n    sns.set_theme(palette=default_palette_color)\n    progress_bar.close()\n\n    \ndef show_segmentation(title, figsize, img_src, dataset_version, segmentation):\n    \"\"\"\n    Displays the resized source images and the segmentation image in a single line.\n\n    Args:\n        title (str): Global title for the plot.\n        img_src (numpy.ndarray): Resized source images.\n        segmentation (torch.Tensor): Segmentation image.\n\n    Returns:\n        None\n    \"\"\"\n    \ndef show_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):\n    \"\"\"\n    Displays a download link for a DataFrame as a CSV file.\n\n    Args:\n        df (pandas.DataFrame): The DataFrame to be downloaded.\n        title (str): The title of the download link (default: \"Download CSV file\").\n        filename (str): The name of the downloaded file (default: \"data.csv\").\n\n    Returns:\n        None\n    \"\"\"\n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload, title=title, filename=filename)\n    display(HTML(html))\n    \ndef fonc_test_normality(df, quantitative_var) : \n    fig = plt.figure(figsize=(12, 4))\n\n    plt.subplot(1,2,1)\n    sns.histplot(data=df, x=quantitative_var, kde=True, hue='MGMT_value')\n    plt.title('Histogramme de '+quantitative_var,fontsize=8)\n    plt.xlabel('Valeur',fontsize=7)\n    plt.ylabel('Fr√©quence',fontsize=7)\n\n    plt.subplot(1,2,2)\n    sns.boxplot(data=df, y=quantitative_var, x='MGMT_value')\n    plt.title('Boxplot de '+quantitative_var,fontsize=8)\n    #plt.xlabel('Valeur')\n\n    plt.show()  \n    \ndef create_kde_mgmt_pos_neg(df, x, y) : \n    \"\"\"\n    Displays 3 KDE plots that show the bivariate density using the columns x and y from a dataframe.\n\n    Args:\n        df: DataFrame, the dataframe whose 2 variables are going to be used for the KDE plot.\n        x, y: String, the two columns\n        \n    \"\"\"\n    fig = plt.figure(figsize=(15, 3))\n    fig.suptitle(\"Densit√© bivari√©e de \"+y+\" par rapport √† \"+x, fontsize=10)\n    \n    plt.subplot(1,3,1)\n    sns.kdeplot(data=df, x=x, y=y, hue=\"MGMT_value\")\n\n    plt.subplot(1,3,2)\n    plt.title(\"Densit√© bivari√©e pour MGMT positif\", fontsize = 6)\n    sns.kdeplot(data=df[df[\"MGMT_value\"] == 1], x=x, y=y, color=\"orange\")\n\n    plt.subplot(1,3,3)\n    plt.title(\"Densit√© bivari√©e pour MGMT n√©gatif\", fontsize = 6)\n    sns.kdeplot(data=df[df[\"MGMT_value\"] == 0], x=x, y=y)\n    plt.show();\n\n\ndef show_explore_distribution_normality(df, quantitative_var, figsize=(20, 5)):\n    fig, axes = plt.subplots(nrows=1, ncols=6, figsize=figsize)\n\n    # Histogram with density estimation\n    histplot = sns.histplot(data=df, x=quantitative_var, kde=True, hue='MGMT_value', ax=axes[0])\n    axes[0].set_title('Distribution of Data')\n    axes[0].set_xlabel(quantitative_var)\n    axes[0].set_ylabel('Frequency')\n\n    legend = histplot.get_legend()\n    legend.set_title('MGMT_value')\n    legend.set_bbox_to_anchor((1, 1))\n\n    # Density plot with normal distribution\n    sns.kdeplot(data=df, x=quantitative_var, ax=axes[1], label='Density')\n    x = np.linspace(np.min(df[quantitative_var]), np.max(df[quantitative_var]), 100)\n    y = stats.norm.pdf(x, np.mean(df[quantitative_var]), np.std(df[quantitative_var]))\n    axes[1].plot(x, y, linewidth=2, label='Normal Distribution')\n    axes[1].set(title='Density Plot with Normal Distribution', xlabel=quantitative_var, ylabel='Density')\n    axes[1].legend(loc='upper right')\n\n    \n    # Boxplot\n    sns.boxplot(data=df, y=quantitative_var, x='MGMT_value', hue='MGMT_value', ax=axes[2])\n    axes[2].set_title('Comparison of Distributions')\n    axes[2].set_xlabel('MGMT_value')\n    axes[2].set_ylabel(quantitative_var)\n    axes[2].legend(title='MGMT_value', loc='upper center')\n\n    # Violin plot\n    sns.violinplot(data=df, y=quantitative_var, x='MGMT_value', hue='MGMT_value', ax=axes[3])\n    axes[3].set_title('Distribution and Density')\n    axes[3].set_xlabel('MGMT_value')\n    axes[3].set_ylabel(quantitative_var)\n    axes[3].legend(title='MGMT_value', loc='upper center')\n\n    # Q-Q plot\n    stats.probplot(df[quantitative_var], plot=axes[4])\n    axes[4].set_title('Quantile-Quantile (Q-Q) Comparison')\n    axes[4].set_xlabel('Theoretical Quantiles')\n    axes[4].set_ylabel('Ordered Values')\n    \n    # Scatter plot of values vs. mean\n    grouped = df.groupby('MGMT_value')\n    for group_name, group_data in grouped:\n        axes[5].scatter(range(len(group_data)), group_data[quantitative_var], label=group_name, alpha=0.5)\n    mean_value = np.mean(df[quantitative_var])\n    axes[5].axhline(y=mean_value, color='r', linestyle='--')\n    axes[5].set_title('Values vs. Mean')\n    axes[5].set_xlabel('Index')\n    axes[5].set_ylabel(quantitative_var)\n    axes[5].legend(title='MGMT_value', loc='upper right')\n\n    plt.suptitle(f'Exploratory Analysis of Data Distribution and Normality Assessment of {quantitative_var}', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n\ndef show_test_normality(df):\n    new_df = pd.DataFrame(index=['skewness', 'kurtosis', 'excess_kurtosis', 'shapiro_test', 'normalite'])\n\n    for col in df.columns:\n        skewness = stats.skew(df[col])\n        kurtosis = stats.kurtosis(df[col], fisher=False)\n        excess_kurtosis = stats.kurtosis(df[col])\n        shapiro_test = stats.shapiro(df[col])[1]\n        normalite = 'Yes' if shapiro_test > 0.05 else 'No'\n\n\n        new_df[col] = [0.000001 if skewness == 0 else skewness,\n                       0.000001 if kurtosis == 0 else kurtosis,\n                       0.000001 if excess_kurtosis == 0 else excess_kurtosis,\n                       shapiro_test,\n                       normalite]\n\n    return new_df\n\n\n\n\ndef show_3D_scatter_plots(\n    combinations_party,\n    dataset, \n    title=None,\n    figsize=(14, 25),\n    elev_angle=30, \n    azimuth_angle=15, \n    show_legend=False,\n    legend_loc='lower center',\n    cmap='coolwarm',\n    alpha=1.0\n):\n    \"\"\"\n    Display 3D scatter plots for each combination of variables in combinations_party using the given dataset.\n\n    Args:\n        combinations_party (list): List of combinations of variables to plot (x, y, z).\n        dataset (DataFrame): The dataset containing the variables.\n        figsize (tuple, optional): Figure size in inches. Defaults to (14, 25).\n        elev_angle (float, optional): Elevation angle in degrees. Defaults to 30.\n        azimuth_angle (float, optional): Azimuth angle in degrees. Defaults to 15.\n        title (str, optional): Title for the plot. Defaults to None.\n        show_legend (bool, optional): Whether to show the legend. Defaults to True.\n        cmap (str, optional): Color map for the MGMT values. Defaults to 'coolwarm'.\n        alpha (float, optional): Transparency of the data points. Defaults to 1.0.\n    \"\"\"\n    num_combinations = len(combinations_party)\n    num_rows = int(math.ceil(num_combinations / 2))\n    num_cols = min(2, num_combinations)\n\n    fig, axes = plt.subplots(num_rows, num_cols, subplot_kw={'projection': '3d'}, figsize=figsize)\n\n    # Loop to create scatter plots for each combination\n    for i, combo in enumerate(combinations_party):\n        if num_combinations == 1:\n            ax = axes\n        elif num_rows == 1:\n            ax = axes[i]\n        else:\n            row = i // num_cols\n            col = i % num_cols\n            ax = axes[row, col]\n\n        value_x = combo[0]\n        value_y = combo[1]\n        value_z = combo[2]\n        value_c = value_z\n        \n        x = dataset[value_x]\n        y = dataset[value_y]\n        z = dataset[value_z]\n        c = dataset[value_c]\n            \n\n        # Display 3D scatter plot\n        scatter = ax.scatter(x, y, z, c=c, cmap=cmap, alpha=alpha)\n\n        # Add labels to axes\n        ax.set_xlabel(value_x)\n        ax.set_ylabel(value_y)\n        ax.set_zlabel(value_z)\n        ax.view_init(elev=elev_angle, azim=azimuth_angle)\n        scatter.set_label(title)\n\n        if show_legend:\n            unique_values = np.unique(c)\n            colors = plt.get_cmap(cmap)(np.linspace(0, 1, len(unique_values)))\n            labels = [f\"{value_c} = {value}\" for value in unique_values]\n                    \n            # Ajouter une l√©gende personnalis√©e\n            custom_legend = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in colors]\n            ax.legend(custom_legend, labels, loc=legend_loc, ncol=1)\n\n\n\n    # Hide the last subplot if the number of combinations is odd\n    if num_combinations % 2 == 1 and num_combinations > 1:\n        axes[num_rows-1, num_cols-1].axis('off')\n\n    fig.suptitle(title)  # Set the overall title for the plot\n\n    fig.tight_layout()\n    plt.show()\n    plt.close()\n\n\ndef show_triangle_correlation_matrix(data, title=\"Correlation Matrix\", filter_threshold=0, figsize=(50, 20)):\n    corr_df = data.corr()\n    if filter_threshold > 0:\n        corr_df = filter_correlation_matrix(corr_df, filter_threshold)\n\n    plt.figure(figsize=figsize)\n    mask = np.triu(np.ones_like(corr_df))\n    ax = sns.heatmap(corr_df, annot=True, mask=mask, cmap=\"coolwarm\")  # Ajout de linewidths\n    plt.title(title)\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    \n    plt.show()\n    \ndef show_brain_slices(segmentation_path_folder, split_by = None, figsize=(50, 10)):\n    file_list = sorted(os.listdir(segmentation_path_folder), key=extract_number_from_filename)\n        \n    if split_by is not None:\n        file_list = file_list[::split_by]\n    \n    if len(file_list) == 0:\n        print(\"Aucune image √† afficher.\")\n        return\n    \n    slice_images = []\n    \n    for file_name in file_list:\n        if file_name.endswith('.dcm'):\n            segmentation_path = os.path.join(segmentation_path_folder, file_name)\n            segmented_image = sitk.ReadImage(segmentation_path)\n            segmented_array = sitk.GetArrayFromImage(segmented_image)\n            slice_image = segmented_array[0]\n            slice_images.append(slice_image)\n    \n    \n    num_rows = len(slice_images) // 10 \n    num_columns =  len(slice_images) // num_rows  \n    fig, axes = plt.subplots(num_rows, num_columns, figsize=figsize)\n    \n    for ax, image in zip(axes.ravel(), slice_images):\n        ax.imshow(image, cmap='gray')\n        ax.axis('off')\n\n    plt.show()\n# ===============================================\n# Best Features\n# ===============================================\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import RFECV\n\ndef select_best_features_RFE(df, target, models, features_scalers, reduction_step=0.001, cv=5, scoring='r2'):\n    \"\"\"\n    This function selects features using Recursive Feature Elimination (RFE) method.\n\n    Parameters:\n    df (DataFrame): The input dataframe.\n    features_and_scaler_tuple_list (list): List of tuples containing feature names and corresponding scaler.\n    target (str): The name of the target variable.\n    model (estimator): The estimator supports either fit_transform method or feature_importances_ attribute.\n    n_features_to_select (int, optional): The number of features to select. If None, half of the features are selected. Defaults to None.\n    reduction_step (float or int, optional): If greater than or equal to 1, then step corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then step corresponds to the percentage (rounded down) of features to remove at each iteration. Defaults to 0.001.\n\n    Returns:\n    list: The list of selected feature names ordered by importance.\n    \"\"\"\n    all_features = [feature for features, scaler in features_scalers for feature in features]\n\n    # Get the feature matrix and the target variable\n    X = df[all_features]\n    y = df[target]\n\n    # Iterate over the feature-scaler tuples\n    for features, scaler in features_scalers:\n        # Apply the scaler on the features\n        for feature in features:\n            X[feature] = scaler.fit_transform(X[feature].values.reshape(-1, 1))\n\n    results = []\n    for name, model in models:\n        rfe = RFECV(estimator=model, step=reduction_step, cv=cv, scoring=scoring)\n        rfe.fit(X, y)\n        selected_features = X.iloc[:, rfe.support_]\n        score = rfe.score(X, y)\n        results.append((name, score, selected_features))\n\n    return sorted(results, key=lambda x: x[1], reverse=True)\n\n\nfrom boruta import BorutaPy\nfrom sklearn.model_selection import cross_val_score\n\ndef select_best_features_Boruta(df, target, models, features_scalers, max_iter=100, n_estimators='auto', cv=5, scoring='r2', random_state=0):\n    \"\"\"\n    This function selects features using BorutaPy method.\n\n    Parameters:\n    df (DataFrame): The input dataframe.\n    target (str): The name of the target variable.\n    models (list): List of tuples containing model names and corresponding model instances.\n    features_scalers (list): List of tuples containing feature names and corresponding scaler instances.\n    max_iter (int, optional): Maximum number of iterations for Boruta. Defaults to 100.\n    cv (int or cross-validation generator, optional): Determines the cross-validation splitting strategy. Defaults to 5.\n    random_state (int, optional): Random seed for reproducibility. Defaults to 0.\n\n    Returns:\n    list: The list of selected feature names ordered by importance.\n    \"\"\"\n    all_features = [feature for features, scaler in features_scalers for feature in features]\n    \n    # Get the feature matrix and the target variable\n    X = df[all_features]\n    y = df[target]\n\n    # Iterate over the feature-scaler tuples\n    for features, scaler in features_scalers:\n        # Apply the scaler on the features\n        for feature in features:\n            X[feature] = scaler.fit_transform(X[feature].values.reshape(-1, 1))\n\n    results = []\n    for name, model in models:\n        boruta = BorutaPy(estimator=model, n_estimators=n_estimators, max_iter=max_iter, random_state=random_state)\n        # Perform cross-validation with BorutaPy\n        scores = cross_val_score(boruta, X.values, y.values, cv=cv, scoring=scoring)\n        boruta.fit(X.values, y.values)\n        selected_features = X.iloc[:, boruta.support_]\n        results.append((name, scores.mean(), selected_features.columns.tolist()))\n\n    return results\n\n\n# ===============================================\n# Correlation\n# ===============================================\n\ndef generate_correlated_features_table(dataset, correlation_thresholds):\n    \"\"\"\n    Generate a table of correlated features based on the given correlation thresholds.\n\n    Args:\n        dataset (pd.DataFrame): The dataset to analyze.\n        correlation_thresholds (list): List of correlation thresholds to consider.\n\n    Returns:\n        pd.DataFrame: The table of correlated features.\n    \"\"\"\n\n    groups_correlated = {}\n\n    for correlation_threshold in correlation_thresholds:\n        groups_correlated[correlation_threshold] = find_highly_correlated_groups(dataset.corr(), correlation_threshold)\n\n    # Pr√©parer les donn√©es pour le tableau\n    correlated_features = {}\n\n    for correlation_threshold, groups_correlated_threshold in groups_correlated.items():\n        correlated_features[correlation_threshold] = [\", \".join(group_correlated) for group_correlated in groups_correlated_threshold if len(group_correlated) > 1]\n\n    # V√©rifier et ajuster les longueurs des listes\n    max_length = max(len(features) for features in correlated_features.values())\n\n    for features in correlated_features.values():\n        features.extend([\"\"] * (max_length - len(features)))\n\n    # Cr√©er un DataFrame avec les donn√©es\n    df_correlated = pd.DataFrame(correlated_features)\n    # Ajouter un titre au DataFrame\n    df_correlated.columns.name = \"Correlation Thresholds\"\n\n    return df_correlated\n\ndef extract_correlated_features(dataset, correlation_thresholds):\n    \"\"\"\n    Extracts correlated features from a dataset based on given correlation thresholds.\n    \n    Args:\n        dataset (pandas.DataFrame): The input dataset.\n        correlation_thresholds (list): A list of correlation thresholds.\n    \n    Returns:\n        numpy.ndarray: An array of correlated features.\n    \"\"\"\n    correlation_features = generate_correlated_features_table(dataset.copy(), [correlation_thresholds])\n    correlation_features = correlation_features.T.to_numpy()\n    correlation_features = np.concatenate(correlation_features).ravel()\n    correlation_features = [x for x in correlation_features if x != \"\"]\n    correlation_features = [string.split(\", \") for string in correlation_features]\n    correlation_features = np.concatenate(correlation_features).ravel()\n    \n    return correlation_features\n\n\ndef filter_correlation_matrix(correlation_matrix, correlation_threshold):\n    \"\"\"\n    Filters a correlation matrix by keeping only the absolute values greater than or equal to the correlation threshold.\n\n    Args:\n        correlation_matrix (pd.DataFrame): The correlation matrix.\n        correlation_threshold (float): The correlation threshold for filtering the matrix.\n\n    Returns:\n        pd.DataFrame: The filtered correlation matrix.\n\n    \"\"\"\n    filtered_correlation_matrix = correlation_matrix.copy()\n    filtered_correlation_matrix[abs(filtered_correlation_matrix) < correlation_threshold] = np.nan\n\n    return filtered_correlation_matrix\n\n\ndef find_highly_correlated_groups(correlation_matrix, correlation_threshold = 0.8, filter_duplicated_group = True, convert_indices_to_column_names = True):\n    \"\"\"\n    Finds groups of highly correlated variables from a correlation matrix.\n\n    Args:\n        correlation_matrix (pd.DataFrame): The correlation matrix.\n        correlation_threshold (float): The correlation threshold to consider as highly correlated.\n        filter_duplicated_group (bool): Indicates whether to filter out duplicated values in the correlated groups.\n        convert_indices_to_column_names (bool): Indicates whether to convert indices to column names.\n\n    Returns:\n        List[List[str]]: A list of groups, where each group contains the names of variables that are highly correlated.\n\n    \"\"\"\n    n = correlation_matrix.shape[0]  # Number of variables in the correlation matrix\n    groups_correlated = []  # List to store the correlated groups\n    \n    # Retrieve column names\n    column_names = correlation_matrix.columns\n    \n    # Traverse each variable\n    for i in range(n):\n        if column_names[i] not in [column_names[v] for g in groups_correlated for v in g]:  # Check if the variable has already been added to a group\n            group = [i]  # Create a new group containing the current variable (i)\n            for j in range(i+1, n):\n                if column_names[j] not in [column_names[v] for g in groups_correlated for v in g]:  # Check if the variable has already been added to a group\n                    correlation = correlation_matrix.iloc[i, j]  # Retrieve the correlation between variables i and j\n                    if abs(correlation) >= correlation_threshold:  # Strong correlation condition (adjust as needed)\n                        group.append(j)  # Add variable j to the group\n            \n            groups_correlated.append(group)  # Add the group to the list of correlated groups\n    \n    # Filter out duplicated values in the correlated groups\n    if filter_duplicated_group:\n        filtered_groups_correlated = []\n        for group in groups_correlated:\n            filtered_group = list(set(group))  # Convert to a set to eliminate duplicates, then convert back to a list\n            filtered_groups_correlated.append(filtered_group)\n        groups_correlated = filtered_groups_correlated # Reset by new one\n    \n    # Convert indices to column names\n    if convert_indices_to_column_names:\n        groups_correlated = [[column_names[i] for i in group] for group in groups_correlated]\n    \n    return groups_correlated\n\n","metadata":{"_uuid":"5a760e1d-74f2-4c96-8949-7e6daf4bcff8","_cell_guid":"afc69598-2d6f-42f4-904d-7e836dc83f2f","_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-06-17T08:10:12.280099Z","iopub.execute_input":"2023-06-17T08:10:12.28078Z","iopub.status.idle":"2023-06-17T08:10:12.478454Z","shell.execute_reply.started":"2023-06-17T08:10:12.280741Z","shell.execute_reply":"2023-06-17T08:10:12.477381Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"<hr>","metadata":{"_uuid":"84472088-1904-44d6-8a09-05645dfdfc16","_cell_guid":"184c67d6-fe34-4d5a-ac6a-aa62a2f1bfca","trusted":true}},{"cell_type":"markdown","source":"# <a id='exploratory_data'>2. Exploratory Data</a>","metadata":{"_uuid":"2f518b02-410b-40f1-8287-8fdedd6945cd","_cell_guid":"7551c0dc-5900-4dd9-af62-b9489a79d556","trusted":true}},{"cell_type":"markdown","source":"The **\"train/\"** directory contains the training files for the competition. Each top-level directory represents a subject, and the **\"train_labels.csv\"** file contains the corresponding targets for each subject, indicating the presence of MGMT promoter methylation.","metadata":{}},{"cell_type":"code","source":"total = len(preview_dataset_df)  # Nombre total d'√©chantillons\ntitle = \"Total number of files\"\ndata = {title: [total]}\n\ndf = pd.DataFrame(data)\ndf.set_index(title, inplace=True)\ndf.head()","metadata":{"_uuid":"9e68198c-5bce-4c61-a33b-5ee3d6451821","_cell_guid":"b5bfcec0-ef1f-430c-829b-5524f221a045","_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-17T08:10:12.479952Z","iopub.execute_input":"2023-06-17T08:10:12.480537Z","iopub.status.idle":"2023-06-17T08:10:12.495878Z","shell.execute_reply.started":"2023-06-17T08:10:12.480503Z","shell.execute_reply":"2023-06-17T08:10:12.495041Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: []\nIndex: [585]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n    </tr>\n    <tr>\n      <th>Total number of files</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>585</th>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"preview_dataset_df.head(10)","metadata":{"_uuid":"dfabdc6d-078c-4d65-a86c-3cd1546c0127","_cell_guid":"7788dfe2-782c-4786-99e4-2dbc5a2083a2","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-17T08:10:12.497204Z","iopub.execute_input":"2023-06-17T08:10:12.497754Z","iopub.status.idle":"2023-06-17T08:10:12.507483Z","shell.execute_reply.started":"2023-06-17T08:10:12.497723Z","shell.execute_reply":"2023-06-17T08:10:12.506422Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"   BraTS21ID  MGMT_value\n0  0          1         \n1  2          1         \n2  3          0         \n3  5          1         \n4  6          1         \n5  8          1         \n6  9          0         \n7  11         1         \n8  12         1         \n9  14         1         ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BraTS21ID</th>\n      <th>MGMT_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>12</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>14</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nsns.countplot(data=preview_dataset_df, x=\"MGMT_value\")\nplt.title(\"Distribution of MGMT values\")\nplt.xlabel(\"MGMT Value\")\nplt.xticks([0, 1], [\"Not Present\", \"Present\"])\nplt.show()","metadata":{"_uuid":"cf02cf8f-6ceb-4e45-a61e-6ed420d78166","_cell_guid":"f0249ce2-c619-4dde-a7fb-5439377e9ec1","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-17T08:10:12.508588Z","iopub.execute_input":"2023-06-17T08:10:12.509618Z","iopub.status.idle":"2023-06-17T08:10:12.81679Z","shell.execute_reply.started":"2023-06-17T08:10:12.509587Z","shell.execute_reply":"2023-06-17T08:10:12.815519Z"},"trusted":true},"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"The **\"test/\"** directory contains the test files. For each subject in the test data, there is no file containing the methylation targets, so these values must be predicted. The **\"sample_submission.csv\"** file is an example of a correctly formatted submission file, with MGMT values of **0.5** for each subject.\n\nOverall, the task of the competition is to predict the presence of MGMT (%) promoter methylation for each subject in the test data.","metadata":{"_uuid":"0c274f99-0b27-40e3-8383-70327d2d3398","_cell_guid":"df694fbf-9457-4a20-8dfd-43508944fe97","trusted":true}},{"cell_type":"code","source":"samp_subm.head(1)","metadata":{"_uuid":"e5d32f7a-462f-4b99-bee3-5997ca8fe212","_cell_guid":"9abe5753-0094-4f8a-b46b-c27c0fd39c15","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-17T08:10:12.823799Z","iopub.execute_input":"2023-06-17T08:10:12.827162Z","iopub.status.idle":"2023-06-17T08:10:12.842424Z","shell.execute_reply.started":"2023-06-17T08:10:12.827128Z","shell.execute_reply":"2023-06-17T08:10:12.841538Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"   BraTS21ID  MGMT_value\n0  1          0.5       ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BraTS21ID</th>\n      <th>MGMT_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"The dataset we will be working with consists of MRI data provided by the Radiological Society of North America (RSNA¬Æ) and the Medical Imaging Computation and Computer Assistance Society (MICCAI Society). \nThe images are provided in DICOM format and are accompanied by a CSV file containing radiomic features extracted from the images.","metadata":{}},{"cell_type":"code","source":"first_folder = str(preview_dataset_df.loc[0, 'BraTS21ID']).zfill(5) + \"/\"\ntitle = \"Folders content for all patients\"\n# Folders content\nfolder_path = train_folder_path + first_folder  # Replace train_folder_path with the actual path\nfolder_content = os.listdir(folder_path)\n\ndf = pd.DataFrame({\n        title: folder_content\n    })\ndf.set_index(title, inplace=True)\ndf.head()","metadata":{"_uuid":"16b496d4-fea4-4af0-84c5-5fd26747fbd1","_cell_guid":"25a41fda-4818-4491-b61e-b6841540cf70","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-17T08:10:12.847568Z","iopub.execute_input":"2023-06-17T08:10:12.848875Z","iopub.status.idle":"2023-06-17T08:10:12.870845Z","shell.execute_reply.started":"2023-06-17T08:10:12.848842Z","shell.execute_reply":"2023-06-17T08:10:12.870015Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: []\nIndex: [T2w, T1wCE, T1w, FLAIR]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n    </tr>\n    <tr>\n      <th>Folders content for all patients</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>T2w</th>\n    </tr>\n    <tr>\n      <th>T1wCE</th>\n    </tr>\n    <tr>\n      <th>T1w</th>\n    </tr>\n    <tr>\n      <th>FLAIR</th>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"In the first Dataset of the patient (ID: \"00000\") , we will explore the images contained in **['T2w', 'T1wCE', 'T1w', 'FLAIR']** of the first patient.","metadata":{}},{"cell_type":"code","source":"first_folder = str(preview_dataset_df.loc[0, 'BraTS21ID']).zfill(5) + \"/\"\nfolder_path = train_folder_path + first_folder  # Replace train_folder_path with the actual path\n\ntitle = 'Image Type'\nflair_count = len(os.listdir(folder_path + 'FLAIR'))\nt1w_count = len(os.listdir(folder_path + 'T1w'))\nt1wce_count = len(os.listdir(folder_path + 'T1wCE'))\nt2w_count = len(os.listdir(folder_path + 'T2w'))\n\ndata = {\n    title: ['FLAIR', 'T1w', 'T1wCE', 'T2w'],\n    'Count': [flair_count, t1w_count, t1wce_count, t2w_count]\n}\n\n\ndf = pd.DataFrame(data)\ndf.set_index(title, inplace=True)\n\n\ndf.head()","metadata":{"_uuid":"b4582e6f-229c-4e4b-8118-ea10eaef37db","_cell_guid":"76951d5c-e505-4e36-af00-f70a897a26fb","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-17T08:10:12.875744Z","iopub.execute_input":"2023-06-17T08:10:12.878879Z","iopub.status.idle":"2023-06-17T08:10:12.904049Z","shell.execute_reply.started":"2023-06-17T08:10:12.878846Z","shell.execute_reply":"2023-06-17T08:10:12.903245Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"            Count\nImage Type       \nFLAIR       400  \nT1w         33   \nT1wCE       129  \nT2w         408  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Count</th>\n    </tr>\n    <tr>\n      <th>Image Type</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>FLAIR</th>\n      <td>400</td>\n    </tr>\n    <tr>\n      <th>T1w</th>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>T1wCE</th>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>T2w</th>\n      <td>408</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## <a id='exploratory_data_2_1'>2.1. MRI (FLAIR, T1w, T1wCE, T2w)</a>","metadata":{}},{"cell_type":"markdown","source":"\n\nThe files are mpMRI scans, this includes:\n\n- **Fluid Attenuated Inversion Recovery (FLAIR)**\n    * **What it is:** These are images that detect brain abnormalities, such as edema and inflammatory lesions. These images are sensitive to the detection of anomalies related to inflammatory and infectious diseases of the central nervous system.\n    * **What it highlights:** It helps to detect anomalies in the brain that might not be visible in other MRI sequences.\n    * These images allow for the detection of brain abnormalities related to inflammatory and infectious diseases of the central nervous system.","metadata":{}},{"cell_type":"code","source":"show_brain_slices(train_folder_path + \"00000/FLAIR\", split_by = 13)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-17T08:10:12.907765Z","iopub.execute_input":"2023-06-17T08:10:12.910641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **T1-weighted pre-contrast (T1w)**\n    * **What it is:** These are images that highlight soft tissues, such as muscles and nerves, and are useful for visualizing normal brain structures.\n    * **What it highlights:** It allows the visualization of the normal brain structures and also helps in the detection of tumors and lesions.\n    * These images allow for the detection of brain tumors and lesions.\n","metadata":{}},{"cell_type":"code","source":"show_brain_slices(train_folder_path + \"00000/T1w\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-  **T1-weighted post-contrast (T1wCE or T1Gd)**\n    * **What it is:** These are images that use a contrast agent to detect vascular anomalies, such as tumors and lesions, which are more visible after contrast agent administration.\n    * **What it highlights:** It enhances the visibility of vascular anomalies, such as tumors and lesions, making it easier to detect them.\n    * These images allow for the detection of vascular anomalies, such as tumors and lesions.\n","metadata":{}},{"cell_type":"code","source":"show_brain_slices(train_folder_path + \"00000/T1wCE\", split_by = 4)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **T2-weighted (T2w or T2)**\n    * **What it is:** These images detect abnormalities related to demyelination, such as multiple sclerosis, as well as brain tumors and lesions.\n    * **What it highlights:** It helps in the detection of anomalies related to cerebrospinal fluid, such as cysts and brain tumors.\n    * These images allow for the detection of anomalies related to demyelination, brain tumors, lesions, and cerebrospinal fluid.","metadata":{}},{"cell_type":"code","source":"show_brain_slices(train_folder_path + \"00000/T2w\", split_by = 13)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MRI sequences: **FLAIR, T1wCE, T1w, T2w**","metadata":{}},{"cell_type":"code","source":"image_path = \"https://github.com/YanSteph/RSNA-MICCAI-Brain-Tumor-Classification-AI/blob/main/img/scan1.png?raw=true\"\nhtml_code = f'<img src=\"{image_path}\" style=\"width: 700px;\" />'\ndisplay(HTML(html_code))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id='exploratory_data_2_1'>2.2. Highlight features or areas of interest MRI</a>\n\nThe \"Hot\" palette palette with Red and Yellow is commonly used in medicine to highlight areas of high activity or high intensity in brain images.\n\nThis palette uses warm colors, ranging from black to bright red, to visually represent regions of intense brain activity or high intensity values. These warm colors help highlight areas of interest and clearly distinguish them from areas of low activity or intensity.\n\nIn medicine, this can be particularly useful for visualizing aspects such as neuronal activity, areas of blood flow concentration, the intensity of certain characteristics or measurements, or any other relevant information related to activity or intensity in the brain.\n\nUsing the \"Hot\" palette makes it easier to quickly identify regions of high activity or intensity, which can be critical for healthcare professionals in interpreting brain images and detecting features or areas of interest.\n\n## <a id='exploratory_data_2_2'>2.3. Exploratory quality and interpretation of the MRI</a>\n\n\nHere is the list of examples of arguments supporting that an MRI is well performed and that the scans are appropriate for the study:\n\n1. **Image resolution and quality:** A well-executed MRI will produce high-resolution images with clear image quality. This enables healthcare professionals to observe anatomical structures accurately and detect any potential abnormalities.\n\n2.  **Proper contrast:** The use of contrast agents during an MRI can enhance the visualization of certain structures or pathologies. When a scan is correctly performed, contrast will be applied appropriately, facilitating the identification of areas of interest.\n\n3.  **Positioning and immobilization:** High-quality MRI requires precise patient positioning and good immobilization to prevent unwanted movements during the procedure. When the patient is properly positioned and kept still, the obtained images will be more reliable and interpretable.\n\n4.  **Appropriate acquisition protocol:** Each type of MRI requires a specific acquisition protocol based on the body part being studied and the purpose of the examination. When the protocol is followed correctly, the images will provide the necessary information for diagnosis or study.\n\n5.  **Accurate interpretation:** Lastly, for an MRI to be well-executed and the scans ready for study, it is essential that interpretation is carried out by a qualified professional, such as a radiologist. Accurate interpretation of the images allows for the identification of potential abnormalities, proper diagnosis, and the recommendation of necessary treatments.\n\nIt is important to note that these arguments are general and that there are many more.","metadata":{"_uuid":"50bad42c-2162-40a7-a22a-c8c7740b3894","_cell_guid":"1fadb689-cb08-4fa2-afb3-0da0b576ade8","trusted":true}},{"cell_type":"code","source":"number_of_extraction = 3\n\n\n# Filtre df mght 1 and 0\npatient_without_mgmt = preview_dataset_df.loc[preview_dataset_df[\"MGMT_value\"] == 0].sort_values('BraTS21ID')\npatient_with_mgmt = preview_dataset_df.loc[preview_dataset_df[\"MGMT_value\"] == 1].sort_values('BraTS21ID')\n\n# Collonne selected\npatient_with_mgmt_folder_ids = patient_with_mgmt[[\"BraTS21ID\", \"MGMT_value\"]].iloc[:number_of_extraction]\npatient_without_mgmt_folder_ids = patient_without_mgmt[[\"BraTS21ID\", \"MGMT_value\"]].iloc[:number_of_extraction]\n\n# map df into tupple\npatient_with_mgmt_mapped_tuples = map_dataframe_to_tuples(patient_with_mgmt_folder_ids, [\"BraTS21ID\", \"MGMT_value\"])\npatient_without_mgmt_mapped_tuples = map_dataframe_to_tuples(patient_without_mgmt_folder_ids, [\"BraTS21ID\", \"MGMT_value\"])\n\n# Merge\npatient_combined_with_and_without_mgmt_ids = list(chain.from_iterable(zip(patient_with_mgmt_mapped_tuples, patient_without_mgmt_mapped_tuples)))\n\n# Show\nshow_brains(patient_combined_with_and_without_mgmt_ids, figsize = (8, 5), dataset_version = dataset_version)","metadata":{"_uuid":"6fcd178a-3d53-43b7-bac0-03d2c70a3730","_cell_guid":"f90ac8c9-9fc4-4b35-bc2b-20e67bed7ef5","_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When an MRI is performed improperly or the scan is of poor quality, several issues can arise, potentially leading to adverse consequences for the patient. \n\nHere potential problems:\n\n1.  **Image artifacts:** A poorly performed MRI can result in artifacts, which are distortions or errors in the image. This can make interpretation difficult or even impossible, leading to incorrect or incomplete diagnoses.\n\n2.  **Insufficient image quality:** Improper execution of the MRI can lead to insufficient image quality. Inadequate resolution or excessive blurring can make it difficult to accurately identify anatomical structures or lesions, compromising the reliability of the diagnosis.\n\n3.  **Positioning errors:** Proper patient positioning is crucial during an MRI to obtain precise and consistent images. Incorrect positioning, such as poor immobilization or improper device calibration, can cause deformations and significant loss of information.\n\n4.  **Motion artifacts:** MRI often requires the patient to remain still during image acquisition. Any involuntary movement, such as tremors or respiratory motion, can introduce motion artifacts into the image. These artifacts can mask abnormalities or lead to incorrect interpretation of the results.\n\n5.  **Parameter setting errors:** The parameters used in an MRI, such as the acquisition sequence, echo time, or repetition time, need to be properly defined based on the clinical objective. Parameter setting errors can result in poor visualization of certain structures or inappropriate sensitivity to detect certain pathologies.\n\n6.  **Region of interest not covered:** When an MRI is poorly positioned or planning errors occur, it is possible that the region of interest may not be fully covered by the image. This can lead to a loss of essential information and incomplete evaluation of the area under examination, thereby compromising the accuracy of the diagnosis.\n\n\nThese are just an example. It is important to note that these problems are not exclusive to a single MRI scan. They can occur in exceptional circumstances or due to human error.\n\nHere a preview of wrong IRM:","metadata":{}},{"cell_type":"code","source":"excluded_patient_ids = [109, 123, 709] \n\npatient_with_wrong_mri_ids = preview_dataset_df.loc[(preview_dataset_df[\"BraTS21ID\"].isin(excluded_patient_ids))].sort_values('BraTS21ID')\npatient_with_wrong_mri_ids_tuples = map_dataframe_to_tuples(patient_with_wrong_mri_ids, [\"BraTS21ID\", \"MGMT_value\"])\n\nshow_brains(patient_with_wrong_mri_ids_tuples, figsize = (8, 5), dataset_version = dataset_version)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üìù Note:</strong></h2>\n        <ul>\n            <li>Our training dataset consists of <b>585 patients</b>, which is relatively limited for machine learning. A larger dataset would provide more robust results.</li>\n            <li>Submissions are evaluated on the area under the <b>ROC curve</b> between the predicted probability and the observed target. <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" style=\"color: white;\">üîó Link: ROC (Receiver operating characteristic)</a>.</li>\n            <li>For the submission file each BraTS21ID in the test set, we must predict a <b>probability (%)</b> for the <b>target MGMT_value.</b></li>\n            <li>We need to split the given <b>train sets into two parts: one for train and the other for test.</b></li>\n            <li>Report on the competition page, there are unexpected issues with the following cases in the training dataset, <b>we will exclude these patient IDs:</b>\n                <ul>\n                    <li><b>[109, 123, 709]</b></li>\n                </ul>\n            </li>\n            <li>To ensure accurate prediction results, <b>we exclude the \"/test\" folder</b>, it only contains thresholded values of <b>0.5</b> and this is the part of submission to the competition.</li>\n        </ul>\n    </div>\n</div>\n</br>\n<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üõ† Global:</strong></h2>\n        <ul>\n            <li>Excluded patient IDs: <b>`excluded_patient_ids`</b></li>\n            <li>We excluded this patient Ids for the rest of the project.</li>\n                <ul>\n                    <li><b>[109, 123, 709]</b></li>\n                </ul>\n        </ul>\n    </div>\n</div>","metadata":{"_uuid":"53d42ad4-da06-4b10-a7ef-0eb25337ebd5","_cell_guid":"821b1528-ae86-4ee8-bb98-f81bf996f3a3","trusted":true}},{"cell_type":"markdown","source":"----","metadata":{"_uuid":"55a7f1f8-7d14-4b9a-8f55-f9160654c033","_cell_guid":"9f7f6fc7-7504-47f3-ae9d-70690ad96912","trusted":true}},{"cell_type":"markdown","source":"# <a id='brain_segmentation'>3. Brain segmentation</a>\nThe \"mateuszbuda_brain-segmentation-pytorch_unet\" library was chosen to facilitate brain segmentation from medical images in our project. This section outlines the reasons behind selecting this library and its contribution to achieving accurate brain segmentation results.\n\nSource: [mateuszbuda_brain-segmentation-pytorch_unet on PyTorch Hub](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/)\n\n## <a id='brain_segmentation_3_1'>3.1. Importance of Brain Segmentation</a>\nBrain segmentation is a crucial task in medical imaging as it enables the extraction of precise information about different regions or classes within the brain structures. Accurate segmentation plays a vital role in various medical applications, including tumor detection, anatomical analysis, and treatment planning.\n\n## <a id='brain_segmentation_3_2'>3.2. Selection Criteria</a>\nThe \"mateuszbuda_brain-segmentation-pytorch_unet\" library was chosen based on the following criteria:\n\n* **Exceptional Performance:** The library has demonstrated exceptional performance in brain segmentation tasks, providing accurate and reliable results.\nUnet Architecture: It is specifically designed based on the Unet neural network architecture, which has a proven track record of success in biomedical image segmentation.\n* **User-Friendly Nature:** The library offers a user-friendly interface and efficient implementation, allowing researchers and practitioners to easily integrate it into their projects.\n\n## <a id='brain_segmentation_3_3'>3.3. Contributions of the Library</a>\nThe utilization of the \"mateuszbuda_brain-segmentation-pytorch_unet\" library significantly contributed to the acquisition of accurate brain segmentation data for our dataset. By leveraging the library's capabilities, we were able to efficiently segment medical images and extract valuable information for further analysis and research purposes.\n\n<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üìù Note:</strong></h2>\n        <p style=\"color: white;\">üëâüèø In summary, the selection of the <b>\"mateuszbuda_brain-segmentation-pytorch_unet\"</b> library was based on its exceptional performance, user-friendly nature, and utilization of the powerful Unet architecture. This library played a pivotal role in achieving accurate brain segmentation results and provided a solid foundation for our project's objectives.</p>\n    </div>\n</div>\n\n\n   \n# <a id='utilizing_unet_4'>4. Utilizing the U-Net for Brain MRI Model and RadiomicsShape3D Class</a>\nIn order to leverage 3D capabilities for brain segmentation, we will utilize the U-Net for Brain MRI model and the RadiomicsShape3D class. This section provides an overview of the model's architecture and the usage instructions for performing 3D brain segmentation and shape analysis.\n\n## <a id='utilizing_unet_4_1'>4.1. Architecture</a>\nThe U-Net for Brain MRI model in 3D extends the 2D architecture to process volumetric brain images. It follows a similar U-shaped architecture with skip connections, but the convolutional layers operate in a 3D manner. The model consists of encoder and decoder paths, with each path containing multiple levels of blocks. The number of filters in the convolutional layers varies across the levels, allowing the model to capture different levels of details in the volumetric data.\n\n## <a id='utilizing_unet_4_2'>4.2. Usage Instructions</a>\nTo utilize the 3D U-Net for Brain MRI model, follow these steps:\n\nPrepare a brain MRI volume with three channels corresponding to pre-contrast, FLAIR, and post-contrast sequences.\nPreprocess the volume by resizing it to an appropriate spatial resolution and normalizing the intensity values.\nFeed the preprocessed volume to the U-Net model for segmentation. The model will output a probability map representing the likelihood of abnormal regions in the input volume.\nApply a suitable threshold to the probability map to obtain a binary segmentation mask.\n\n## <a id='utilizing_unet_4_3'>4.3. RadiomicsShape3D Class</a>\nTo perform shape analysis on the segmented brain regions, we will utilize the \"radiomics.shape3D.RadiomicsShape3D\" class. This class provides functionalities for analyzing the 3D shape characteristics of segmented regions in medical images.\n\n## <a id='utilizing_unet_4_4'>4.4. Usage Instructions</a>\nTo utilize the RadiomicsShape3D class, follow these steps:\n\nObtain the segmented regions or masks from the 3D U-Net model.\nInstantiate the RadiomicsShape3D class.\n\nSource: [Radiomics 3D](https://pyradiomics.readthedocs.io/en/latest/features.html#module-radiomics.shape3D)\n\n<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üìù Note:</strong></h2>\n        <ul>\n            <li>By incorporating the 3D U-Net for Brain MRI model and the RadiomicsShape3D class, we have extended the capabilities of our project to volumetric brain image analysis. This allows for more comprehensive segmentation and shape analysis, facilitating advanced medical research and clinical applications.</li>\n            <li>Initially, we worked with 2D data, where we only considered the central slice of the images. However, given the size of our dataset and the limitation of 2D visualization in providing a complete understanding of the brain structure, it seemed appropriate to transition to a 3D approach. By working with 3D image volumes, we were able to increase the richness of our data and obtain a more comprehensive representation of the brain's features. This transition to 3D offers new perspectives for more detailed analysis and more accurate results.</li>\n        </ul>\n    </div>\n    <p style=\"color: white;\">üëâüèø Thus, by leveraging the 3D U-Net model and the RadiomicsShape3D class, we are able to fully harness the benefits offered by volumetric image analysis.</p>\n</div>\n\n\n# <a id='dataset_creation_5'>5. Dataset Preparation</a>\n\nIn order to proceed, we need to partition the given training dataset into two sets: training and testing. \n\nHowever, it is important to note that there are certain issues with three specific cases in the training dataset, namely [00109, 00123, 00709], as reported on the main contest page. \n\nTherefore, we will exclude these cases from the dataset.","metadata":{"_uuid":"2ecfbb0e-3a96-4742-9b28-1c526f0bb86b","_cell_guid":"f3106509-4ab2-4874-968f-4b40af192238","trusted":true}},{"cell_type":"code","source":"if skip_brain_segmentation_pytorch_unet:\n    dataset_df = pd.read_csv(dataset_3d_path if dataset_version == DatasetVersion.V_3D else dataset_2d_path)\n    \nelse:\n    loader = widgets.IntProgress(min=0, max=len(preview_dataset_df), description='Loading:')\n    display(loader)\n    \n    # Empty creation of datasets\n    init_dataset_radiomics()\n\n    for i in preview_dataset_df.BraTS21ID :\n        loader.value += 1\n        img_resized = get_processed_image(i, dataset_version)\n        segmentation = segmentation_process(img_resized)\n\n        if dataset_version == DatasetVersion.V_3D:\n            add_patient_data3D(i, img_resized, segmentation)\n        else : \n            add_patient_data2D(i, img_resized, segmentation)\n\n    # Join the 7 datasets\n    df_shapes = df_shapes.set_index('ID')\n    df_textures = df_textures.set_index('ID')\n    df_first_orders_features = df_first_orders_features.set_index('ID')\n\n    df_gzlm_features = df_gzlm_features.set_index('ID').add_prefix('gzlm_')\n    df_glrlm_features = df_glrlm_features.set_index('ID').add_prefix('glrlm_')\n    df = df_shapes.join(df_textures).join(df_first_orders_features)\n    df_ngtdm_features = df_ngtdm_features.set_index('ID').add_prefix('ngtdm_')\n    df_gldm_features = df_gldm_features.set_index('ID').add_prefix('gldm_')\n    \n    df = df_shapes.join(df_textures).join(df_first_orders_features).join(df_gzlm_features).join(df_glrlm_features).join(df_ngtdm_features).join(df_gldm_features)\n\n    # Define 'BraTS21ID' column as integer IDs\n    df['BraTS21ID'] = df['BraTS21ID'].astype(int)\n    \n    # Merge the old dataset with the new one\n    dataset_df = pd.merge(preview_dataset_df, df, left_on='BraTS21ID', right_on='BraTS21ID')\n    dataset_df.rename(columns={'BraTS21ID': 'ID'}, inplace=True)\n\n# Patient BraTS21ID now is ID, and ID of Dataset\ndataset_df = dataset_df.set_index('ID')\n# Drop patient\ndataset_df = dataset_df.drop(index=excluded_patient_ids)\n\nshow_download_link(dataset_df)","metadata":{"_uuid":"3c7d4169-1b44-4576-bdbd-3482098642c6","_cell_guid":"98df2c38-fc59-4868-9f32-dda82317efca","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üõ† Global:</strong></h2>\n        <ul>\n            <li>Segmentation Dataset 2D or 3D: <b>`dataset_df`</b></li> \n            <li>We excluded patient Ids with wrong data for the rest of the project.</li>\n        </ul>\n    </div>\n</div>","metadata":{"_uuid":"0de17d8c-17d0-4577-a91d-5bec55cbb907","_cell_guid":"bfb66090-3f88-47db-ae66-67a103dae18c","trusted":true}},{"cell_type":"markdown","source":"<hr>","metadata":{"_uuid":"980927a2-dfdd-490b-bd2d-7e41a0afb890","_cell_guid":"5f1b7849-eaaf-45c1-a502-0e7621a496f4","trusted":true}},{"cell_type":"markdown","source":"# <a id='exploratory_dataset'>6. Exploratory Dataset</a>\n\nThese features provide information about various properties of brain MRI images, such as shape, texture, and grayscale statistics. They are commonly used for analysis and classification of medical images to aid in the detection and characterization of brain pathologies.\n\nHere is the requested list of features extracted from brain MRI images:\n\n<details>\n  <summary style=\"font-weight: bold; font-size: 1.2em;\">‚ñ∂Ô∏è General Features:</summary>\n  <ul>\n    <li><strong>ID:</strong> Sample identifier (Index).</li>\n    <li><strong>MGMT_value:</strong> Presence of MGMT.</li>\n  </ul>\n</details>\n\n<details>\n  <summary style=\"font-weight: bold; font-size: 1.2em;\"><strong>‚ñ∂Ô∏è First Order Statistics Features:</strong></summary>\n  <ul>\n    <li><strong>10Percentile:</strong> The 10th percentile of the image array.</li>\n    <li><strong>90Percentile:</strong> The 90th percentile of the image array.</li>\n    <li><strong>Energy:</strong> A measure of the magnitude of voxel values in an image. A larger value implies a greater sum of the squares of these values.</li>\n    <li><strong>Entropy:</strong> Specifies the uncertainty/randomness in the image values. It measures the average amount of information required to encode the image values.</li>\n    <li><strong>InterquartileRange:</strong> The difference between the 75th and 25th percentile of the image array.</li>\n    <li><strong>Kurtosis:</strong> A measure of the ‚Äòpeakedness‚Äô of the distribution of values in the image ROI. A higher kurtosis implies that the mass of the distribution is concentrated towards the tail(s) rather than towards the mean. A lower kurtosis implies the reverse: that the mass of the distribution is concentrated towards a spike near the Mean value.</li>\n    <li><strong>Maximum:</strong> The maximum gray level intensity within the ROI.</li>\n    <li><strong>MeanAbsoluteDeviation:</strong> The mean distance of all intensity values from the Mean Value of the image array.</li>\n    <li><strong>Mean:</strong> The average gray level intensity within the ROI.</li>\n    <li><strong>Median:</strong> The median gray level intensity within the ROI.</li>\n    <li><strong>Minimum:</strong> The minimum gray level intensity within the ROI.</li>\n    <li><strong>Range:</strong> The range of gray values in the ROI.</li>\n    <li><strong>RobustMeanAbsoluteDeviation:</strong> The mean distance of all intensity values from the Mean Value calculated on the subset of image array with gray levels in between, or equal to the 10th and 90th percentile.</li>\n    <li><strong>RootMeanSquared:</strong> The square-root of the mean of all the squared intensity values. It is another measure of the magnitude of the image values.</li>\n    <li><strong>Skewness:</strong> Measures the asymmetry of the distribution of values about the Mean value. Depending on where the tail is elongated and the mass of the distribution is concentrated, this value can be positive or negative.</li>\n    <li><strong>TotalEnergy:</strong> The value of Energy feature scaled by the volume of the voxel in cubic mm.</li>\n    <li><strong>Uniformity:</strong> A measure of the sum of the squares of each intensity value. This is a measure of the homogeneity of the image array, where a greater uniformity implies a greater homogeneity or a smaller range of discrete intensity values.</li>\n    <li><strong>Variance:</strong> The mean of the squared distances of each intensity value from the Mean value. This is a measure of the spread of the distribution about the mean.</li>\n  </ul>\n</details>\n\n<details>\n  <summary style=\"font-weight: bold; font-size: 1.2em;\"><strong>‚ñ∂Ô∏è Shape (3D) Features:</strong></summary>\n  <ul>\n    <li><strong>MeshVolume:</strong> The volume of the ROI is calculated from the triangle mesh of the ROI. For each face in the mesh, the (signed) volume of the tetrahedron defined by that face and the origin of the image is calculated. The total volume of the ROI is obtained by taking the sum of all these volumes.</li>\n    <li><strong>VoxelVolume:</strong> The volume of the ROI is approximated by multiplying the number of voxels in the ROI by the volume of a single voxel. This is a less precise approximation of the volume and is not used in subsequent features.</li>\n    <li><strong>SurfaceArea:</strong> To calculate the surface area, first the surface area of each triangle in the mesh is calculated. The total surface area is then obtained by taking the sum of all calculated sub-areas.</li>\n    <li><strong>SurfaceVolumeRatio:</strong> This is the ratio of the surface area of the tumor region to the volume of the tumor region. A lower value indicates a more compact (sphere-like) shape.</li>\n    <li><strong>Sphericity:</strong> Sphericity is a measure of the roundness of the shape of the tumor region relative to a sphere. It is a dimensionless measure, independent of scale and orientation.</li>\n    <li><strong>Compactness1:</strong> Similar to Sphericity, Compactness 1 is a measure of how compact the shape of the tumor is relative to a sphere (most compact).</li>\n    <li><strong>Compactness2:</strong> Similar to Sphericity and Compactness 1, Compactness 2 is a measure of how compact the shape of the tumor is relative to a sphere (most compact).</li>\n    <li><strong>SphericalDisproportion:</strong> Spherical Disproportion is the ratio of the surface area of the tumor region to the surface area of a sphere with the same volume as the tumor region, and by definition, the inverse of Sphericity.</li>\n    <li><strong>Maximum3DDiameter:</strong> Maximum 3D diameter is defined as the largest pairwise Euclidean distance between tumor surface mesh vertices.</li>\n    <li><strong>Maximum2DDiameterSlice:</strong> Maximum 2D diameter (Slice) is defined as the largest pairwise Euclidean distance between tumor surface mesh vertices in the row-column (generally the axial) plane.</li>\n    <li><strong>Maximum2DDiameterColumn:</strong> Maximum 2D diameter (Column) is defined as the largest pairwise Euclidean distance between tumor surface mesh vertices in the row-slice (usually the coronal) plane.</li>\n    <li><strong>Maximum2DDiameterRow:</strong> Maximum 2D diameter (Row) is defined as the largest pairwise Euclidean distance between tumor surface mesh vertices in the column-slice (usually the sagittal) plane.</li>\n    <li><strong>MajorAxisLength:</strong> This feature yields the largest axis length of the ROI-enclosing ellipsoid and is calculated using the largest principal component.</li>\n    <li><strong>MinorAxisLength:</strong> This feature yields the second-largest axis length of the ROI-enclosing ellipsoid and is calculated using the second largest principal component.</li>\n    <li><strong>LeastAxisLength:</strong> This feature yields the smallest axis length of the ROI-enclosing ellipsoid and is calculated using the smallest principal component.</li>\n    <li><strong>Elongation:</strong> Elongation shows the relationship between the two largest principal components in the ROI shape.</li>\n    <li><strong>Flatness:</strong> Flatness shows the relationship between the largest and smallest principal components in the ROI shape.</li>\n    <li><strong>MinorAxisLength:</strong> Not found.</li>\n  </ul>\n</details>\n\n<details>\n  <summary style=\"font-weight: bold; font-size: 1.2em;\"><strong>‚ñ∂Ô∏è Gray Level Co-occurrence Matrix (GLCM) Features:</strong></summary>\n  <ul>\n    <li><strong>Autocorrelation</strong>: This is a measure of the magnitude of the fineness and coarseness of texture.</li>\n    <li><strong>ClusterProminence</strong>: A measure of the asymmetry and complexity of the texture.</li>\n    <li><strong>ClusterShade</strong>: A measure of skewness and uniformity of the texture.</li>\n    <li><strong>ClusterTendency</strong>: A measure of groupings of voxels with similar gray-level values.</li>\n    <li><strong>Contrast</strong>: A measure of the local variations in the gray-level co-occurrence matrix.</li>\n    <li><strong>Correlation</strong>: A measure of the joint probability occurrence of the specified pixel pairs.</li>\n    <li><strong>DifferenceAverage</strong>: Provides a measure of contrast in an image.</li>\n    <li><strong>DifferenceEntropy</strong>: A measure of randomness or variation in an image.</li>\n    <li><strong>DifferenceVariance</strong>: A measure of heterogeneity that places higher weights on differing intensity level pairs.</li>\n    <li><strong>Id</strong>: Inverse Difference (ID) is a measure of local homogeneity.</li>\n    <li><strong>Idm</strong>: Inverse Difference Moment (IDM) is another measure of local homogeneity.</li>\n    <li><strong>Idmn</strong>: Inverse Difference Moment Normalized (IDMN) is a measure of local homogeneity with a higher emphasis on smaller gray-level differences.</li>\n    <li><strong>Idn</strong>: Inverse Difference Normalized (IDN) is a measure of local homogeneity with a higher emphasis on larger gray-level differences.</li>\n    <li><strong>Imc1</strong>: Informational Measure of Correlation 1 (IMC1) is a measure of complexity.</li>\n    <li><strong>Imc2</strong>: Informational Measure of Correlation 2 (IMC2) is another measure of complexity.</li>\n    <li><strong>InverseVariance</strong>: A measure of the sum of squares of each intensity value.</li>\n    <li><strong>JointAverage</strong>: The average value of the joint histogram of the image.</li>\n    <li><strong>JointEnergy</strong>: A measure of similarity of the distribution of the joint histogram to that of the image.</li>\n    <li><strong>JointEntropy</strong>: A measure of uncertainty or randomness.</li>\n    <li><strong>MCC</strong>: Maximal Correlation Coefficient (MCC) is a measure of correlation between elements in the GLCM.</li>\n    <li><strong>MaximumProbability</strong>: The maximum probability value in the GLCM.</li>\n    <li><strong>SumAverage</strong>: The average sum of gray-level pairs in the GLCM.</li>\n    <li><strong>SumEntropy</strong>: A measure of randomness or complexity in the GLCM.</li>\n    <li><strong>SumSquares</strong>: A measure of the variance in the GLCM.</li>\n  </ul>\n</details>\n\n<details>\n  <summary style=\"font-weight: bold; font-size: 1.2em;\"><strong>‚ñ∂Ô∏è Gray Level Size Zone Matrix (GLSZM) Features:</strong></summary>\n  <ul>\n    <li><strong>SmallAreaEmphasis:</strong> Measures the proportion of small size zones, with a higher value indicating a greater proportion of small size zones.</li>\n    <li><strong>LargeAreaEmphasis:</strong> Measures the proportion of large size zones, with a higher value indicating a greater proportion of large size zones.</li>\n    <li><strong>GrayLevelNonUniformity:</strong> Measures the similarity of gray-level intensity values in the image, with a lower value indicating a greater similarity in intensity values.</li>\n    <li><strong>GrayLevelNonUniformityNormalized:</strong> Measures the similarity of gray-level intensity values in the image, with a lower value indicating a greater similarity in intensity values. This is normalized to account for differences in the size of the region of interest.</li>\n    <li><strong>SizeZoneNonUniformity:</strong> Measures the variability of size-zone lengths throughout the image, with a lower value indicating more homogeneity in size-zone lengths.</li>\n    <li><strong>SizeZoneNonUniformityNormalized:</strong> Measures the variability of size-zone lengths throughout the image, with a lower value indicating more homogeneity in size-zone lengths. This is normalized to account for differences in the size of the region of interest.</li>\n    <li><strong>ZonePercentage:</strong> Measures the number of size-zones per number of voxels in the region of interest.</li>\n    <li><strong>GrayLevelVariance:</strong> Measures the variance in gray-level intensity values.</li>\n    <li><strong>ZoneVariance:</strong> Measures the variance of size-zone lengths.</li>\n    <li><strong>ZoneEntropy:</strong> Measures the uncertainty/randomness of the size-zone length distribution.</li>\n    <li><strong>LowGrayLevelZoneEmphasis:</strong> Measures the proportion of low gray-level size zones.</li>\n    <li><strong>HighGrayLevelZoneEmphasis:</strong> Measures the proportion of high gray-level size zones.</li>\n    <li><strong>SmallAreaLowGrayLevelEmphasis:</strong> Measures the proportion of small size zones with lower gray-level values.</li>\n    <li><strong>SmallAreaHighGrayLevelEmphasis:</strong> Measures the proportion of small size zones with higher gray-level values.</li>\n    <li><strong>LargeAreaLowGrayLevelEmphasis:</strong> Measures the proportion of larger size zones with lower gray-level values.</li>\n    <li><strong>LargeAreaHighGrayLevelEmphasis:</strong> Measures the proportion of larger size zones with higher gray-level values.</li>\n  </ul>\n</details>\n\n<details>\n  <summary style=\"font-weight: bold; font-size: 1.2em;\"><strong>‚ñ∂Ô∏è Gray Level Run Length Matrix (GLRLM) Features:</strong></summary>\n  <ul>\n    <li><strong>ShortRunEmphasis:</strong> Measures the distribution of short runs, with a lower value indicating a greater prevalence of longer runs.</li>\n    <li><strong>LongRunEmphasis:</strong> Measures the distribution of long runs, with a higher value indicating a greater prevalence of longer runs.</li>\n    <li><strong>GrayLevelNonUniformity:</strong> Measures the similarity of run lengths throughout the image, with a lower value indicating a greater similarity in run lengths.</li>\n    <li><strong>RunLengthNonUniformity:</strong> Measures the similarity of gray-level intensity values in the image, with a lower value indicating more homogeneity.</li>\n    <li><strong>RunLengthNonUniformityNormalized:</strong> This is the normalized version of the Run Length Non-Uniformity feature.</li>\n    <li><strong>RunPercentage:</strong> This is a measure of the coarseness of the texture, with a higher value indicating a coarser texture.</li>\n    <li><strong>GrayLevelVariance:</strong> Measures the variance in gray-level intensity values in the image.</li>\n    <li><strong>RunVariance:</strong> Measures the variance of runs lengths in the image.</li>\n    <li><strong>RunEntropy:</strong> A measure of complexity or information that can be extracted from the GLRLM matrix.</li>\n    <li><strong>LowGrayLevelRunEmphasis:</strong> Measures the proportion of runs with lower gray-level values.</li>\n    <li><strong>HighGrayLevelRunEmphasis:</strong> Measures the proportion of runs with higher gray-level values.</li>\n    <li><strong>ShortRunLowGrayLevelEmphasis:</strong> Measures the joint distribution of shorter run lengths and lower gray-level values.</li>\n    <li><strong>ShortRunHighGrayLevelEmphasis:</strong> Measures the joint distribution of shorter run lengths and higher gray-level values.</li>\n    <li><strong>LongRunLowGrayLevelEmphasis:</strong> Measures the joint distribution of longer run lengths and lower gray-level values.</li>\n    <li><strong>LongRunHighGrayLevelEmphasis:</strong> Measures the joint distribution of longer run lengths and higher gray-level values.</li>\n    <li><strong>RunPercentage:</strong> The Run Percentage measures the proportion of runs in the image that have a length longer than the specified run length. It quantifies the coarseness or texture of the image, with a higher value indicating a coarser texture.</li>\n  </ul>\n</details>\n\n<details>\n  <summary style=\"font-weight: bold; font-size: 1.2em;\"><strong>‚ñ∂Ô∏è Gray Level Dependence Matrix (GLDM) Features:</strong></summary>\n  <ul>\n    <li><strong>SmallDependenceEmphasis (SDE):</strong> Measures the distribution of small dependencies with lower values indicating a greater proportion of small dependencies in the image. Small dependencies are considered those that are shorter than the specified dependence length.</li>\n    <li><strong>LargeDependenceEmphasis (LDE):</strong> Measures the distribution of large dependencies with higher values indicating a greater proportion of large dependencies in the image. Large dependencies are considered those that are longer than the specified dependence length.</li>\n    <li><strong>GrayLevelNonUniformity (GLN):</strong> Measures the similarity of gray-level intensity values in the image, with a lower value indicating a greater similarity in intensity values.</li>\n    <li><strong>GrayLevelNonUniformityNormalized (GLNN):</strong> Measures the similarity of gray-level intensity values in the image, with a lower value indicating a greater similarity in intensity values. This is a normalized version of the Gray Level Non-Uniformity feature.</li>\n    <li><strong>DependenceNonUniformity (DN):</strong> Measures the similarity of dependencies in the image, with a lower value indicating a greater similarity in dependencies.</li>\n    <li><strong>DependenceNonUniformityNormalized (DNN):</strong> Measures the similarity of dependencies in the image, with a lower value indicating a greater similarity in dependencies. This is a normalized version of the Dependence Non-Uniformity feature.</li>\n    <li><strong>GrayLevelVariance (GLV):</strong> Measures the variance in gray-level intensity values.</li>\n    <li><strong>DependenceVariance (DV):</strong> Measures the variance of dependencies in the image.</li>\n    <li><strong>DependenceEntropy (DE):</strong> Measures the uncertainty/randomness of the distribution of dependencies in the image. It measures the average amount of information required to encode the dependencies.</li>\n    <li><strong>DependencePercentage (DP):</strong> Measures the percentage of dependencies in the image that are longer than the specified dependence length.</li>\n    <li><strong>LowGrayLevelEmphasis (LGLE):</strong> Measures the proportion of low gray-level intensity values.</li>\n    <li><strong>HighGrayLevelEmphasis (HGLE):</strong> Measures the proportion of high gray-level intensity values.</li>\n    <li><strong>SmallDependenceLowGrayLevelEmphasis (SDLGLE):</strong> Measures the joint distribution of small dependencies and low gray-level values.</li>\n    <li><strong>SmallDependenceHighGrayLevelEmphasis (SDHGLE):</strong> Measures the joint distribution of small dependencies and high gray-level values.</li>\n    <li><strong>LargeDependenceLowGrayLevelEmphasis (LDLGLE):</strong> Measures the joint distribution of large dependencies and low gray-level values.</li>\n    <li><strong>LargeDependenceHighGrayLevelEmphasis (LDHGLE):</strong> Measures the joint distribution of large dependencies and high gray-level values.</li>\n    <li><strong>DependenceNonUniformity (DN):</strong> Measures the similarity of dependencies in the image, with a lower value indicating a greater similarity in dependencies.</li>\n    <li><strong>DependenceNonUniformityNormalized (DNN):</strong> Measures the similarity of dependencies in the image, with a lower value indicating a greater similarity in dependencies. This is a normalized version of the Dependence Non-Uniformity feature.</li>\n    <li><strong>DependenceVariance (DV):</strong> Measures the variance of dependencies in the image.</li>\n    <li><strong>DependenceEntropy (DE):</strong> Measures the uncertainty/randomness of the distribution of dependencies in the image. It measures the average amount of information required to encode the dependencies.</li>\n    <li><strong>DependencePercentage (DP):</strong> Measures the percentage of dependencies in the image that are longer than the specified dependence length.</li>\n    <li><strong>SmallDependenceLowGrayLevelEmphasis (SDLGLE):</strong> Measures the joint distribution of small dependencies and low gray-level values.</li>\n    <li><strong>SmallDependenceHighGrayLevelEmphasis (SDHGLE):</strong> Measures the joint distribution of small dependencies and high gray-level values.</li>\n    <li><strong>LargeDependenceLowGrayLevelEmphasis (LDLGLE):</strong> Measures the joint distribution of large dependencies and low gray-level values.</li>\n    <li><strong>LargeDependenceHighGrayLevelEmphasis (LDHGLE):</strong> Measures the joint distribution of large dependencies and high gray-level values.</li>\n    <li><strong>DependenceNonUniformity (DN):</strong> Measures the similarity of dependencies in the image, with a lower value indicating a greater similarity in dependencies. It quantifies the variation in the distribution of dependencies.</li>\n    <li><strong>DependenceNonUniformityNormalized (DNN):</strong> Measures the similarity of dependencies in the image, with a lower value indicating a greater similarity in dependencies. This is a normalized version of the Dependence Non-Uniformity feature that accounts for differences in the size of the region of interest.</li>\n    <li><strong>DependencePercentage (DP):</strong> Measures the percentage of dependencies in the image that are longer than the specified dependence length. It quantifies the proportion of dependencies in the image.</li>\n  </ul>\n</details>\n\n","metadata":{"_uuid":"146734cf-c101-4223-82ab-cbe6b5bcd05d","_cell_guid":"288af7d4-080b-43eb-9be2-4a979a3ecd07","trusted":true}},{"cell_type":"code","source":"dataset_df.head()","metadata":{"_uuid":"5ccde63a-9288-44ea-995f-3d04a83c3fea","_cell_guid":"b32ba107-fb49-45a4-a3bc-6dc4c7ea990d","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_df.info()","metadata":{"_uuid":"0ad26915-cc1f-4626-a0e7-65d4adac4646","_cell_guid":"7b82d05f-9b77-46f1-bb7c-1318037cf75b","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_df.describe()","metadata":{"_uuid":"c2528cda-f551-4897-8130-28f51a447494","_cell_guid":"644a88a8-43b1-4faf-88b9-5180fd04fd4d","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n   <div class=\"card-body\">\n      <h2 class=\"card-title\" style=\"color: white;\"><strong>üìù Note:</strong></h2>\n      <p style=\"color: white;\">Based on the provided analysis, we can draw the following conclusions:</p>\n      <ul>\n         <li>\n            <strong>Distribution & Outliers:</strong>\n            <ul>\n               <li>Most have means that are significantly higher than the median, suggesting positive skew. Variables like <strong>'LeastAxisLength', 'Flatness', 'TotalEnergy', 'Maximum3DDiameter', 'Energy', 'Variance'</strong>, etc.</li>\n               <li>The variables have positive skewness with large values. This can complicate data analysis.</li>\n               <li>See also in the previous summary, have some wrong data to remove.</li>\n               <ul>\n                   <li>Apply feature scaling techniques to address the wide range of values and improve machine learning algorithm performance.</li>\n               </ul>\n            </ul>\n         </li>\n         <li>\n            <strong>Zero Values:</strong>\n            <ul>\n               <li>Variables such as <strong>\"LeastAxisLength\", \"Flatness\", \"gldm_GrayLevelNonUniformityNormalized\", \"gldm_DependencePercentage\"</strong> show a minimum value of 0 or NaN, which might indicate missing or incorrect data.</li>\n               <ul>\n                   <li>We now remove these variables for the rest of the project.</li>\n               </ul>\n            </ul>\n         </li>\n         <li>\n            <strong>Absence of Negative Values:</strong>\n            <ul>\n               <li>Most variables do not contain negative values, which makes sense considering they seem to represent physical measurements. (Brain MRI).</li>\n            </ul>\n         </li>\n         <li>\n            <strong>Potential Redundancy:</strong>\n            <ul>\n               <li>The <strong>'Id'</strong>, <strong>'Idm'</strong>, <strong>'Idmn'</strong>, and <strong>'Idn'</strong>, etc., variables appear to be very similar in terms of their statistical properties, which might indicate redundancy.</li>\n               <ul>\n                   <li>Potential redundancy, so high correlation. Potential variables to remove.</li>\n               </ul>\n            </ul>\n         </li>\n         <li>\n            <strong>Data Correlations:</strong>\n            <ul>\n               <li>This basic statistical summary doesn't provide any information about possible correlations between <strong>'MeshVolume'</strong>, <strong>'VoxelVolume'</strong>, and <strong>'MGMT_value'</strong>.</li>\n               <ul>\n                   <li>The brain size does not affect MGMT values.</li>\n               </ul>\n            </ul>\n         </li>\n         <li>\n            <strong>Statistical Variables:</strong>\n            <ul>\n               <li>The statistical variables included in the table, such as <strong>'Mean'</strong>, <strong>'Median'</strong>, <strong>'Maximum'</strong>, etc., are derived summaries or calculations based on other variables.</li>\n               <ul>\n                   <li>Potential for analysis and perhaps potential exclusion.</li>\n               </ul>\n            </ul>\n         </li>\n      </ul>\n      <p style=\"color: white;\">üëâüèø It's important to keep in mind that potential errors, biases, or other anomalies in how the data was gathered could influence these statistical properties.</p>\n   </div>\n</div>","metadata":{}},{"cell_type":"code","source":"# First Orders features\nfirst_orders_features_columns_names =['10Percentile', '90Percentile', 'Energy', 'Entropy', 'InterquartileRange', 'Kurtosis', \n                                      'Maximum', 'MeanAbsoluteDeviation', 'Mean', 'Median', 'Minimum', 'Range', 'RobustMeanAbsoluteDeviation', \n                                      'RootMeanSquared', 'Skewness', 'TotalEnergy', 'Uniformity', 'Variance']\n\n# Shape 3D  \nshapes_features_columns_names = ['MeshVolume','VoxelVolume','SurfaceArea','SurfaceVolumeRatio','Sphericity',\n                             'Compactness1','Compactness2','SphericalDisproportion','Maximum3DDiameter','Maximum2DDiameterRow',\n                             'Maximum2DDiameterColumn','MajorAxisLength','MinorAxisLenth','Elongation']\n\n# GLCM features\ntextures_features_columns_names = ['Autocorrelation', 'ClusterProminence', 'ClusterShade', 'ClusterTendency', 'Contrast', \n                           'Correlation', 'DifferenceAverage', 'DifferenceEntropy', 'DifferenceVariance', 'Id', 'Idm', \n                           'Idmn', 'Idn', 'Imc1', 'Imc2', 'InverseVariance', 'JointAverage', 'JointEnergy', 'JointEntropy', \n                           'MCC', 'MaximumProbability', 'SumAverage', 'SumEntropy', 'SumSquares']\n \n# GZLM features\ngzlm_features_columns_names = ['gzlm_SmallAreaEmphasis','gzlm_LargeAreaEmphasis','gzlm_GrayLevelNonUniformity','gzlm_GrayLevelNonUniformityNormalized',\n                                'gzlm_SizeZoneNonUniformity','gzlm_SizeZoneNonUniformityNormalized','gzlm_ZonePercentage','gzlm_GrayLevelVariance','gzlm_ZoneVariance',\n                                'gzlm_ZoneEntropy','gzlm_LowGrayLevelZoneEmphasis','gzlm_HighGrayLevelZoneEmphasis','gzlm_SmallAreaLowGrayLevelEmphasis',\n                                'gzlm_SmallAreaHighGrayLevelEmphasis','gzlm_LargeAreaLowGrayLevelEmphasis','gzlm_LargeAreaHighGrayLevelEmphasis']\n\n# GLRLM\nglrlm_features_columns_names = ['glrlm_ShortRunEmphasis','glrlm_LongRunEmphasis','glrlm_GrayLevelNonUniformity','glrlm_RunLengthNonUniformity','glrlm_RunLengthNonUniformityNormalized'\n                                 ,'glrlm_RunPercentage','glrlm_GrayLevelVariance','glrlm_RunVariance','glrlm_RunEntropy','glrlm_LowGrayLevelRunEmphasis','glrlm_HighGrayLevelRunEmphasis'\n                                 ,'glrlm_ShortRunLowGrayLevelEmphasis','glrlm_ShortRunHighGrayLevelEmphasis','glrlm_LongRunLowGrayLevelEmphasis','glrlm_LongRunHighGrayLevelEmphasis']\n\n# NGTM features\nngtdm_features_columns_names = ['ngtdm_Coarseness','ngtdm_Contrast','ngtdm_Busyness','ngtdm_Complexity','ngtdm_Strength']\n\n    \n# GLDM features\ngldm_features_columns_names = ['gldm_SmallDependenceEmphasis','gldm_LargeDependenceEmphasis','gldm_GrayLevelNonUniformity','gldm_DependenceNonUniformity'\n                               , 'gldm_DependenceNonUniformityNormalized', 'gldm_GrayLevelVariance', 'gldm_DependenceVariance', 'gldm_DependenceEntropy'\n                                , 'gldm_LowGrayLevelEmphasis', 'gldm_HighGrayLevelEmphasis', 'gldm_SmallDependenceLowGrayLevelEmphasis', 'gldm_SmallDependenceHighGrayLevelEmphasis'\n                               , 'gldm_LargeDependenceLowGrayLevelEmphasis', 'gldm_LargeDependenceHighGrayLevelEmphasis']\n\nall_features_column_names = {\n    'first': first_orders_features_columns_names,\n    'shapes': shapes_features_columns_names,\n    'textures': textures_features_columns_names,\n    'gzlm': gzlm_features_columns_names,\n    'glrlm': glrlm_features_columns_names,\n    'ngtdm': ngtdm_features_columns_names,\n    'gldm': gldm_features_columns_names,\n}\n\n# Drop\nexcluded_features_column_names = [\"LeastAxisLength\", \"Flatness\", \"gldm_GrayLevelNonUniformityNormalized\", \"gldm_DependencePercentage\"]\ndataset_df.drop(excluded_features_column_names, axis=1, inplace=True)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üõ† Global:</strong></h2>\n        <ul>\n            <li>First orders Features: <b>`first_orders_features_columns_names`</b></li>\n            <li>Shape 3D Features: <b>`shapes_features_columns_names`</b></li>\n            <li>textures Features: <b>`textures_features_columns_names`</b></li>      \n            <li>gzlm Features: <b>`gzlm_features_columns_names`</b></li>      \n            <li>glrlm Features: <b>`glrlm_features_columns_names`</b></li>    \n            <li>ngtdm Features: <b>`ngtdm_features_columns_names`</b></li>\n            <li>gldm Features: <b>`ngtdm_features_columns_names`</b></li>\n            <li>All Features: <b>`all_features_column_names`</b></li>\n            <li>We exluded features with <b>zero or NaN:</b></li>\n            <ul>\n                <li><b>[\"LeastAxisLength\", \"Flatness\", \"gldm_GrayLevelNonUniformityNormalized\", \"gldm_DependencePercentage]</b></li>\n            </ul>\n            </li>\n        </ul>\n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"----","metadata":{"_uuid":"05279de4-831c-44d8-9ba6-b1604811e822","_cell_guid":"025282e8-5a1a-4d14-9b16-f4165ab875ff","trusted":true}},{"cell_type":"markdown","source":"# <a id='analysis'>7. Analysis</a>\n\nIn the analysis section, we are going to look at various features to discover their characteristics and identify significant differences.\n\n\n","metadata":{"_uuid":"9a500165-8624-4966-9706-0d8993b923d8","_cell_guid":"cc2fbfa2-c63b-44e6-af99-8928aa29c48e","trusted":true}},{"cell_type":"markdown","source":"## <a id='analysis_7_1'>7.1. Hierarchical by Clustering Dendrogram</a>","metadata":{}},{"cell_type":"markdown","source":"The Hierarchical Clustering Dendrogram method is used to group brain images based on their structural and functional similarities. \n\nThe dendrogram obtained makes it possible to visualize the hierarchical structure of the groupings and to identify the subgroups and the similarities between the images. \nThis helps to understand patterns and relationships between different brain regions in brain images. It is a valuable approach for analyzing and understanding brain patterns.","metadata":{}},{"cell_type":"code","source":"# Extract features from the DataFrame\nfeatures = dataset_df.drop(\"MGMT_value\", axis=1)\n\n# Calculate the distances between features\ndistances = sch.distance.pdist(features)\n\n# Perform hierarchical clustering and obtain the linkage matrix\nlinkage_matrix = sch.linkage(distances, method='ward')\n\n# Plot the dendrogram\nplt.figure(figsize=(30, 8))\ndendrogram = sch.dendrogram(linkage_matrix, no_labels=True)\n# Adjust the spacing of y-axis tick labels\n\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\n\nplt.title('Hierarchical Clustering Dendrogram of Sample Index')\n\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_clusters = 3\nlabels = sch.fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n\n# Create a dictionary to group features by label\ncluster_three_groups = {}\nfor i, label in enumerate(labels):\n    if label not in cluster_three_groups:\n        cluster_three_groups[label] = []\n    cluster_three_groups[label].append(features.index[i])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the dentogram we can clearly emerge 3 distinct groups","metadata":{}},{"cell_type":"code","source":"def check_and_display_group(key, patients, anomaly_type):\n    if np.isin(patients, cluster_three_groups[key]).any():\n        display(HTML(f\"<h3>Group {key} ({anomaly_type})</h3>\"))\n        patient_with_wrong_mri_ids = preview_dataset_df.loc[preview_dataset_df[\"BraTS21ID\"].isin(patients)]\n        patient_with_wrong_mri_ids_tuples = map_dataframe_to_tuples(patient_with_wrong_mri_ids, [\"BraTS21ID\", \"MGMT_value\"])\n        show_brains(patient_with_wrong_mri_ids_tuples, figsize=(8, 5), dataset_version=dataset_version)\n\nkey = 1\npatients = [111, 456]\ncheck_and_display_group(key, patients, \"ROI with compact shape\")\n\nkey = 2\npatients = [35, 380]\ncheck_and_display_group(key, patients, \"ROI with compact and dispersed shape with noisy\")\n\npatients = [305, 11, 70, 121, 353]\ncheck_and_display_group(key, patients, \"Imperfect, anomaly\")\n\nkey = 3\npatients = [810, 483]\ncheck_and_display_group(key, patients, \"ROI with dispersed shape\")\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üìù Note:</strong></h2>        \n        <p style=\"color: white;\">From analyzing the results of the dendrogram, here are some conclusions that can be drawn:</p>\n        <ul>\n            <li><b>Group 1:</b> Shows a group with potential scans with ROI with compact shape.</li>\n            <li><b>Group 2:</b> Shows a group with two potential types.</li>\n            <ul>\n                <li>A group where the segmentation is more dispersed, compact and with artifacts, the scans are potentially less perfect, but still useful.</li>\n                <li>A group of so-called imperfect, anomaly scans that can be removed (Does not respect the quality advice seen upstream, Positioning errors, Parameter setting errors, Region of interest not covered, Insufficient image quality).</li>\n            </ul>\n            <li><b>Group 3:</b> Shows a group with potential scans with ROI with dispersed shape.</li>            \n        </ul>\n        <p>We do not see any significant values or correlations with group and MGMT.</p>\n    </div>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üõ† Global:</strong></h2>\n        <ul>\n            <li>Cluster group level 3: <b>`cluster_three_groups`</b></li>\n        </ul>\n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# üöß TEAM Delete anamoly","metadata":{}},{"cell_type":"markdown","source":"## <a id='analysis_7_1'>7.2. Average Patient MGMT Comparison</a>\n\nHere is a summary of the differences in values between an average patient with MGMT 1 (Mean) and an average patient with MGMT 0 (Mean) with the indication of the increase or decrease and indicating the potential impact in perentage.","metadata":{}},{"cell_type":"code","source":"def fun_significant(x):\n    if float(x[:-1]) >= 1:\n        return True\n    elif float(x[:-1]) <= 1:\n        return True\n    else:\n        return False\n    \ndef fun_significant_symbol(x):\n    value = float(x[:-1])\n    if value > 1:\n        return \"‚¨ÜÔ∏è\"\n    elif value < -1:\n        return \"‚¨áÔ∏è\"\n    else:\n        return \"-\"\n\n# Select the first row for patients with MGMT_value = 1\npatient_with_mgmt = dataset_df.loc[dataset_df[\"MGMT_value\"] == 1].mean()\n\n# Select the first row for patients with MGMT_value = 0\npatient_without_mgmt = dataset_df.loc[dataset_df[\"MGMT_value\"] == 0].mean()\n\n# Transpose the dataframes\npatient_with_mgmt = patient_with_mgmt.T\npatient_without_mgmt = patient_without_mgmt.T\n\n# Concatenate the transposed dataframes horizontally\nsignificant_values_df = pd.concat([patient_without_mgmt, patient_with_mgmt, patient_without_mgmt - patient_with_mgmt], axis=1)\n\n# Rename the columns\nsignificant_values_df.columns = ['Patient MGMT 0 (Mean)', 'Patient MGMT 1 (Mean)', 'Difference']\n\n# Calculate the percentage difference\nsignificant_values_df['Difference (%)'] = (significant_values_df['Patient MGMT 1 (Mean)'] - significant_values_df['Patient MGMT 0 (Mean)']) / significant_values_df['Patient MGMT 0 (Mean)'] * 100\n\nsignificant_values_df = significant_values_df.sort_values(by=['Difference (%)'], ascending=False)\n\n# Set the format of the 'Difference (%)' column\nsignificant_values_df['Difference (%)'] = np.where((np.isinf(significant_values_df['Difference (%)'])) | (significant_values_df['Difference (%)'].isna()), '0%', significant_values_df['Difference (%)'].apply(lambda x: f\"{x:.2f}%\"))\n\n#Drop MGMT\nsignificant_values_df = significant_values_df.drop(significant_values_df.index[0])\n\n# Add the 'Significant' and 'No Significant' columns\nsignificant_values_df['Significant'] = significant_values_df['Difference (%)'].apply(fun_significant_symbol)\n\n\nsignificant_values_df","metadata":{"_uuid":"c94296ca-9434-4f8a-9db9-c5807654a5fa","_cell_guid":"0d6d20ca-3061-436a-8b21-195682b1e9f4","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üìù Note:</strong></h2>        \n        <p style=\"color: white;\">From analyzing the results of the table, here are some conclusions that can be drawn:</p>\n        <ul>\n        <li>Among other variables, some also show significant differences between patients with and without the MGMT value:</li>\n            <ul>\n                <li>Such as <b>\n                                'ClusterShade', 'Kurtosis', 'ngtdm_Strength', 'gzlm_LargeAreaLowGrayLevelEmphasis',\n                                'Skewness' etc..\n                             </b>.\n                    </li> \n                <li>These differences suggest that these variables are potentially important in distinguishing patients with and without the MGMT value.</li>\n            </ul>\n        <li>Some variables do not show significant differences:\n            <ul>\n                <li>Such as \n                    <b>'Maximum', 'ngtdm_Contrast', 'Idmn' etc..</b>.\n                </li> \n                <li>This suggests that these variables have no significant impact on the presence or absence of the MGMT value.</li>\n            </ul>\n        <li>It is important to note that the percentage differences can vary significantly across variables. Some variables show relatively small differences\n            <ul>\n                <li>Such as <b>\"SurfaceVolumeRatio\"</b> with 0.01%, while others exhibit more substantial differences.</li> \n                <li>Such as <b>\"ClusterShade\"</b> with 65.77% or <b>\"Kurtosis\"</b> with 38.62%.</li>\n                <li>These percentage variations indicate the magnitude of the impact of each variable on the presence or absence of the MGMT value.</li>\n            </ul>\n        </ul>        \n        <p style=\"color: white;\">üëâüèø Overall, these results suggest that some variables have a significant influence on the MGMT value, while others do not have a substantial impact. This information can be valuable in understanding the factors associated with the MGMT value and in developing predictive models or appropriate treatment approaches.</p>\n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üõ† Global:</strong></h2>\n        <ul>\n            <li>Significant values: <b>`significant_values_df`</b></li>            \n        </ul>\n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"## <a id='analysis_7_3'>7.3. Univariate analysis</a>\n\nThis univariate analysis was carried out with the aim of trying to identify the variables potentially close to a normal law in terms of distribution of values.\n\n### <a id='analysis_7_3'>7.3.1 Univariate Analysis: Assessing Normality and Distributions</a>\n\nThese visualizations provide a comprehensive analysis of the normality of the quantitative variable, comparing distributions, and assessing departures from normality. These graphics are commonly used in statistical analysis, exploratory data analysis, and modeling to gain insights into data distributions, identify outliers, assess goodness-of-fit.\n\n<details>\n  <summary style=\"font-weight: bold; font-size: 1.2em;\">‚ñ∂Ô∏è Histogram with Density Estimation:</summary>\n  <p>\n    A histogram is a graphical representation of the distribution of data. It is commonly used to visualize the frequency or count of observations falling into different intervals or bins. In addition to the histogram bars, a probability density curve can be overlaid on the histogram. This curve represents the theoretical distribution that the data may follow. It provides insights into the shape and characteristics of the data distribution.\n  </p>\n</details>\n\n<details>\n  <summary style=\"font-weight: bold; font-size: 1.2em;\">‚ñ∂Ô∏è Density Plot with Normal Distribution:</summary>\n  <p>\n    The density plot with a normal distribution overlays a kernel density estimation of the data along with a curve representing the normal distribution. It allows for visual comparison between the data distribution and the theoretical normal distribution. Deviations between the two curves can indicate departures from normality in the data.\n  </p>\n</details>\n\n<details>\n  <summary style=\"font-weight: bold; font-size: 1.2em;\">‚ñ∂Ô∏è Boxplot:</summary>\n  <p>\n    A boxplot, also known as a box-and-whisker plot, provides a visual summary of the distribution of the data through quartiles. The box represents the interquartile range (IQR) and the line inside the box represents the median. The whiskers extend to the minimum and maximum values, excluding outliers, which are often plotted as individual points. It is useful for comparing multiple datasets or analyzing the spread and skewness of a single dataset.\n  </p>\n</details>\n\n<details>\n  <summary style=\"font-weight: bold; font-size: 1.2em;\">‚ñ∂Ô∏è Violin Plot:</summary>\n  <p>\n    A violin plot combines aspects of a boxplot and a kernel density plot. It displays the distribution of data by featuring a rotated kernel density plot on each side of a central axis. The width of the violin at a specific point represents the density or concentration of data at that value. The violin plot provides insights into both the summary statistics (like a boxplot) and the underlying distribution of the data.\n  </p>\n</details>\n\n<details>\n  <summary style=\"font-weight: bold; font-size: 1.2em;\">‚ñ∂Ô∏è Q-Q Plot (Quantile-Quantile Plot):</summary>\n  <p>\n    A Q-Q plot is a graphical tool used to assess how well a given sample of data matches a theoretical distribution. It compares the quantiles of the data to the quantiles of a theoretical distribution. If the points on the plot roughly follow a straight line, it indicates a good fit between the data and the theoretical distribution. Deviations from the straight line suggest departures from the assumed distribution. Q-Q plots are especially useful for assessing normality assumptions.\n  </p>\n</details>\n\n<details>\n  <summary style=\"font-weight: bold; font-size: 1.2em;\">‚ñ∂Ô∏è Scatter Plot of Values vs. Mean:</summary>\n  <p>\n    The scatter plot shows the relationship between individual values of the quantitative variable and the mean value. Each group defined by the \"MGMT_value\" category is represented by different colors or markers. The horizontal line represents the mean value of the variable. This plot provides insights into how the individual values are distributed around the mean.\n  </p>\n</details>\n<br>\n\nWe took the first 5 and the last 5 relevant:\n","metadata":{"_uuid":"9b9a51f2-5847-4cde-afe0-417787afd0fa","_cell_guid":"e47622c2-7b4b-498a-bcc0-b66eae7641fb","trusted":true}},{"cell_type":"code","source":"display(HTML(\"<h3>3 first relevant ‚¨ÜÔ∏è</h3>\"))\nfirst_5 = significant_values_df.head(3)\nfor column_name in first_5.index:\n    show_explore_distribution_normality(dataset_df, column_name)    \n\n\ndisplay(HTML(\"<h3>3 last relevant ‚¨áÔ∏è</h3>\"))\nlast_5 = significant_values_df.tail(3)\nfor column_name in last_5.index:\n    show_explore_distribution_normality(dataset_df, column_name)  \n    \ndisplay(HTML(\"<h3>Sample Follow normaly law (Correct skewness) ‚è∫</h3>\"))    \nshow_explore_distribution_normality(dataset_df, \"MeshVolume\")\nshow_explore_distribution_normality(dataset_df, \"SurfaceArea\")\n\ndisplay(HTML(\"<h3>Sample Follow normaly law (Good kurtosis) ‚è∫</h3>\"))\nshow_explore_distribution_normality(dataset_df, \"Sphericity\")\nshow_explore_distribution_normality(dataset_df, \"Compactness1\")\n\n\ndisplay(HTML(\"<h3>Sample Follow normaly law (Good Q-Q) ‚è∫</h3>\"))\nshow_explore_distribution_normality(dataset_df, \"Elongation\")\nshow_explore_distribution_normality(dataset_df, \"DifferenceAverage\")\n\n","metadata":{"_uuid":"8355d298-9bce-470c-9e09-6267a5bad61a","_cell_guid":"94d3fc65-ecb3-423a-932b-9a630d304bda","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üìù Note:</strong></h2>\n        <ul>\nThe dataset shows that the target values are homogeneous.\nMany variables do not respond to the Shapiro-Wilk test indicating that the explanatory variables do not follow a normal law. However, some are close.\n            <li>Variables with correct skewness: <b>MeshVolume, SurfaceArea, MajorAxisLength, Contrast etc.</b></li>\n            <li>Variables with good kurtosis: <b>Sphericity,Compactness1, Elongation, DifferenceAverage, DifferenceEntropy, Idn, JointAverage, SumAverage, Mean etc.</b></li>\n            <li>Analyzing the graphs, variables looking good on the Q-Q plot without too many extreme values: <b>Elongation, DifferenceAverage, DifferenceEntropy, JointEntropy, SumEntropy, Entropy, Maximum, Range etc..</b></li>  \n            <li>The distribution of values is generally similar, but several differences can be observed between patients with different MGMT values.</li>\n        </ul>\n    <p style=\"color: white;\">But this does not mean that the dataset is bad, it allows to bring out the variables with extreme values.</p>\n    </div>\n </div>\n   \n\n","metadata":{"_uuid":"694e0daa-0f72-4c41-b32a-fb872b492b89","_cell_guid":"a6816578-a68f-4031-93c4-5849e45d1745","trusted":true}},{"cell_type":"markdown","source":"### <a id='analysis_7_3'>7.3.2 Univariate Analysis: Understanding Normality, Skewness, and Kurtosis Tests</a>\n\n<ul>\n  <li>\n    <u>Normality test</u> used: <b>Shapiro-Wilk</b> - a powerful and precise test recommended for small sample sizes. It evaluates whether the dataset follows a normal distribution. The test assesses the hypothesis that the data is normally distributed based on the sample data and provides a p-value for the evaluation.\n  </li>\n  <li>\n    <u>Test of skewness</u>: measures the asymmetry of a series. A skewness value of 0 indicates a perfectly symmetric distribution following the normal law. However, non-zero skewness values indicate the presence and direction of asymmetry. Positive skewness indicates a longer right tail, while negative skewness indicates a longer left tail.\n  </li>\n  <li>\n    <u>Kurtosis test</u>: measures the shape and flatness of the distribution. A kurtosis value of 3 corresponds to Laplace's normal law, indicating a distribution with the same tail thickness as a standard normal distribution. However, the excess kurtosis is also considered. \n    <ul>\n      <li>If the kurtosis is greater than 3, the dataset is leptokurtic, meaning it has thicker tails than a normal distribution. This suggests a grouping of outliers or extreme values.</li>\n      <li>If the kurtosis is less than 3, the dataset is platykurtic, indicating thinner tails than a normal distribution. This implies a negative excess of outliers, with most of the data clustering around the mean.</li>\n      <li>When the kurtosis is equal to 3, the dataset is mesokurtic, signifying that the tails of the distribution have the same thickness as a normal distribution.</li>\n    </ul>\n    The excess kurtosis is calculated by subtracting 3 from the kurtosis value, providing further insights into the deviation from a normal distribution.\n  </li>\n</ul>\n\n\n","metadata":{}},{"cell_type":"code","source":"describe = show_test_normality(dataset_df.drop('MGMT_value',axis=1))\ndisplay(describe)\n\ndescribe_transposed = describe.loc[['skewness','kurtosis','excess_kurtosis']].transpose()\ndescribe_transposed['absolute_value'] = np.abs(describe_transposed['excess_kurtosis']).astype('float')\ntop_50_closest = describe_transposed.nsmallest(50, 'absolute_value').sort_values('absolute_value')\n\ndescribe_transposed = describe.loc[['skewness', 'kurtosis', 'excess_kurtosis']].transpose()\ndescribe_transposed['absolute_value'] = np.abs(describe_transposed['excess_kurtosis']).astype(float)\n\n# Calculer la diff√©rence entre la colonne 'skewness' et la valeur id√©ale de 1\ndescribe_transposed['skewness_diff'] = np.abs(describe_transposed['skewness'] - 1).astype(float)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# S√©lectionner les variables avec les valeurs les plus proches de 1 pour 'skewness' et les plus faibles 'absolute_value'\nunivariate_analysis_normality_skewness_kurtosis_tests_top_50_df = describe_transposed.nsmallest(50, ['absolute_value', 'skewness_diff'])\nunivariate_analysis_normality_skewness_kurtosis_tests_top_50_df = univariate_analysis_normality_skewness_kurtosis_tests_top_50_df.drop(['skewness_diff', 'absolute_value'], axis=1)\n\ndisplay(HTML(\"<h3>Top 50 normal distribution:</h3>\"))\ndisplay(univariate_analysis_normality_skewness_kurtosis_tests_top_50_df)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üìù Note:</strong></h2>\nSome variables have a better skewness with an MGMT of 1 than compared to the global dataset and vice versa, these same variables are therefore sensitive to markers. Similarly Kurtosis is sensitive too. Depending on the variables, the correlation is positive or negative between skewness and kurtosis.\n        <ul>\n            <li>None of the variables pass the Shapiro-Wilk test, indicating that the explanatory variables do not follow a normal distribution.</li>\n            <li>Some variables exhibit different skewness and kurtosis values between MGMT 1 and the global dataset, indicating sensitivity to markers.</li>\n            <li>The correlation between skewness and kurtosis varies depending on the variables.</li>\n            <li>The estimated density function tends to have a higher maximum value for patients with MGMT value 1, except for the kurtosis variable.</li>\n            <li>The estimated density function for patients with MGMT value 1 tends to overlap with the one for patients with MGMT value 0.</li>\n            <li>The most normal variables are <b>`MajorAxisLength`,`glrlm_RunLengthNonUniformity`, `Elongation`</b></li>\n        </ul>\n    </div>\n</div>","metadata":{"_uuid":"17fe8764-e4e4-4619-b907-45fa39b3984a","_cell_guid":"8b102231-f537-4e52-a935-aef3f8b36f27","trusted":true}},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üõ† Global:</strong></h2>\n        <ul>\n            <li>Top 50 of Univariate Analysis: Understanding Normality, Skewness, and Kurtosis Tests <b>`univariate_analysis_normality_skewness_kurtosis_tests_top_50_df`</b></li>            \n        </ul>\n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"### <a id='analysis_7_3'>7.3.3 Univariate Analysis: Comparison of Kurtosis Values for Different Datasets</a>","metadata":{}},{"cell_type":"code","source":"describe_MGMT_1=show_test_normality(dataset_df[dataset_df.MGMT_value == 1].drop('MGMT_value',axis=1))\ndescribe_MGMT_0=show_test_normality(dataset_df[dataset_df.MGMT_value == 0].drop('MGMT_value',axis=1))\n\nT_describe = describe.drop(['Kurtosis'], axis=1).T\nT_MGMT_1 = describe_MGMT_1.drop(['Kurtosis', 'Maximum'], axis=1).T\nT_MGMT_0 = describe_MGMT_0.drop(['Kurtosis', 'Maximum'], axis=1).T\n\nplt.figure(figsize=(20, 5))\nT_describe[T_describe['kurtosis'] < 10]['kurtosis'].plot(label='Complet', color='blue')\nT_MGMT_1[T_MGMT_1['kurtosis'] < 10]['kurtosis'].plot(label='MGMT_1', color='red', linestyle='-.')\nT_MGMT_0[T_MGMT_0['kurtosis'] < 10]['kurtosis'].plot(label='MGMT_0', color='tab:orange', linestyle='--')\n\nplt.title('Comparison of Kurtosis Values for Different Datasets')\nplt.ylabel('Kurtosis')\nplt.xticks(range(len(T_describe.index)), T_describe.index, rotation=90)  # Afficher toutes les lignes sur l'axe X avec une rotation de 90 degr√©s\nplt.legend()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üìù Note:</strong></h2>\n        <ul>\n            <li>A first conclusion could be that a glioma with MGMT value = 0 would have variables containing more outliers.</li>\n            <li>There are exceptions with seemingly absurd variations, such as the excess of kurtosis for the MajorAxisLength variable which increases by almost 1232%.</li>\n            <li>These variations could indicate that certain variables have a greater impact compared to the target value. For example, kurtosis shows fewer outliers with MGMT value of 1, indicating better normality of Idn.</li>\n        </ul>\n    </div>\n</div>\n","metadata":{"_uuid":"96c47c41-d28e-44e0-8e42-f376b80a028b","_cell_guid":"a9e13f73-2cd5-47cc-b7f1-e1a7f20a7500","trusted":true}},{"cell_type":"markdown","source":"## <a id='analysis_7_3'>7.4 Outliers detection</a>","metadata":{"_uuid":"6af89121-b7cc-4add-be33-4d193ef03e55","_cell_guid":"d96a66c3-9af1-4598-9cd0-4f5fdf15953d","trusted":true}},{"cell_type":"code","source":"# Calculate quartiles for each column\nquartiles = dataset_df.quantile([0.25, 0.75])\n\n# Calculate the interquartile range (IQR) for each column\niqr = quartiles.loc[0.75] - quartiles.loc[0.25]\n\n# Set the threshold to detect outliers\nthreshold = 1.5  # You can adjust this threshold as needed\n\ndataset_copy_df = dataset_df.copy()\n\ndataset_copy_df.drop(\"MGMT_value\", axis=1, inplace=True)\n# Find columns with outliers\noutlier_columns = dataset_copy_df.apply(lambda x: any((x > quartiles.loc[0.75, x.name] + threshold * iqr[x.name]) | (x < quartiles.loc[0.25, x.name] - threshold * iqr[x.name])), axis=0)\n\n# Create a DataFrame with columns containing outliers\ndf_outliers = pd.DataFrame({'Columns with outliers': outlier_columns.index, 'Outliers present': outlier_columns.values})\n\n# Set the index of the DataFrame\ndf_outliers.set_index(\"Columns with outliers\", inplace=True)\n\noutlier_feature_names = df_outliers.index[df_outliers['Outliers present']].tolist()\nnon_outlier_feature_names = df_outliers.index[~df_outliers['Outliers present']].tolist()\n\n\n# Display the DataFrame of columns with outliers\ndf_outliers\n","metadata":{"_uuid":"8a44ad53-ee36-4edf-947f-05ce076d7a16","_cell_guid":"7a047191-7bf8-4cf2-893a-694f98bb57e7","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üìù Note:</strong></h2>\n        <ul>\n            <li>More than half of the population contains outliers.</li>\n            <li>A possible conclusion is that gliomas with MGMT value = 0 may have variables with more outliers.</li>\n    </ul>\n    </div>\n</div>\n","metadata":{"_uuid":"92fe41c8-b200-4c41-b994-efdde6b86265","_cell_guid":"73ba0475-b1a9-4b23-9489-605caae0aefe","trusted":true}},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üõ† Global:</strong></h2>\n        <ul>\n            <li>Variables represents a collection of column names that are associated with outlier detected in a dataset. : <b>`outlier_feature_names`</b></li>      \n            <li>Variables represents a collection of column names that are associated without outlier detected in a dataset. : <b>`non_outlier_feature_names`</b></li> \n        </ul>\n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"---\n","metadata":{}},{"cell_type":"markdown","source":"## <a id='analysis_7_4'>7.3. Bivariate Analysis</a>\n\nIn the bivariate analysis, we explore the relationships and interactions between two variables. \n\n### <a id='analysis_7_4'>7.3.1 Bivariate Analysis: Correlation Matrix</a>\n\nAs a rule of correlation threshold:\n* 0.00-0.19: very weak.\n* 0.20-0.39: weak.\n* 0.40-0.59: moderate.\n* 0.60-0.79: strong.\n* 0.80-1.00: very strong.","metadata":{"_uuid":"e92549fe-6384-46de-bba5-0b3023871410","_cell_guid":"b1e57578-e79f-4df0-be9d-98971dbd65e1","trusted":true}},{"cell_type":"code","source":"show_triangle_correlation_matrix(dataset_df, figsize=(100, 50))","metadata":{"_uuid":"69352ff6-51b4-44a2-81ff-fe3cf02ff276","_cell_guid":"1204c4cd-dcc5-4c94-8574-a3818ac72c38","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target variable is not correlated with the traits, this means that there is no obvious linear relationship between the target variable and the explanatory variables.","metadata":{"execution":{"iopub.status.busy":"2023-06-12T13:22:37.668834Z","iopub.execute_input":"2023-06-12T13:22:37.669264Z","iopub.status.idle":"2023-06-12T13:22:37.705268Z","shell.execute_reply.started":"2023-06-12T13:22:37.669229Z","shell.execute_reply":"2023-06-12T13:22:37.702949Z"}}},{"cell_type":"markdown","source":"### Correlation group with threshold strength\n\nFor correlation threshold equal to **0.7, strong.**","metadata":{"_uuid":"60639114-c5b0-49dd-8607-00c011e62c62","_cell_guid":"d1d98454-739c-411c-958f-168af7652473","execution":{"iopub.status.busy":"2023-05-29T09:22:54.2922Z","iopub.execute_input":"2023-05-29T09:22:54.292577Z","iopub.status.idle":"2023-05-29T09:22:54.299079Z","shell.execute_reply.started":"2023-05-29T09:22:54.29255Z","shell.execute_reply":"2023-05-29T09:22:54.29756Z"},"trusted":true}},{"cell_type":"code","source":"# Filtrer by correlation threshold\nshow_triangle_correlation_matrix(dataset_df,filter_threshold = 0.7, figsize=(100, 50))","metadata":{"_uuid":"b22be0c4-edcf-46d5-abd8-503c6fa95531","_cell_guid":"63d56950-1566-4c45-99cb-0fdaae2b4982","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation group with threshold strength\n\nFor correlation threshold equal to **0.9, very strong.**","metadata":{"_uuid":"d73597f6-88e2-4e57-addc-7db9766c65e1","_cell_guid":"bd92a22b-976c-4a58-9bf0-0363f64a655b","trusted":true}},{"cell_type":"code","source":"show_triangle_correlation_matrix(dataset_df,filter_threshold = 0.9, figsize=(100, 50))","metadata":{"_uuid":"4cccb58e-3d39-4662-8ca4-a444931be6f3","_cell_guid":"d55e4a9d-6e91-444c-be13-80d879c324b3","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the provided analysis with 3D, we can valided the conclusions:","metadata":{}},{"cell_type":"code","source":"hight_correlated_features_tupples = [\n    ('JointAverage', 'SumAverage', 'MGMT_value'),\n    ('TotalEnergy', 'Energy', 'MGMT_value'),\n    ('MeshVolume', 'VoxelVolume', 'MGMT_value'),\n    ('MajorAxisLength', 'MinorAxisLenth', 'MGMT_value')\n]\n\n\nshow_3D_scatter_plots(\n    hight_correlated_features_tupples, \n    dataset_df, \n    title=\"Samples Correlation Thresholds >0.99 without angles\",\n    figsize=(10, 10), \n    elev_angle=90,\n    azimuth_angle=90, \n    show_legend=True,\n    alpha=0.2\n)\n\nshow_3D_scatter_plots(\n    hight_correlated_features_tupples, \n    dataset_df, \n    figsize=(10, 10), \n    elev_angle=45, \n    azimuth_angle=90, \n    title=\"Samples Correlation Thresholds >0.99\", \n    show_legend=True,\n    legend_loc=\"upper right\",\n    alpha=0.2\n)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üìù Note:</strong></h2>        \n        <p style=\"color: white;\">Based on the provided analysis, we can draw the following correlation conclusions:</p>  \n        <h2 class=\"card-title\" style=\"color: white;\"><strong>Note START üëáüèª</strong></h2> \n    </div>\n</div>\n","metadata":{"_uuid":"2341fefa-7b12-4c1e-af09-eca26c60f0a1","_cell_guid":"7e32f71b-cd6b-42c3-aaf9-812ce203e819","trusted":true}},{"cell_type":"code","source":"correlation_thresholds = [0.5, 0.6, 0.7, 0.8, 0.9, 0.99, 1]\ncorrelated_features_table = generate_correlated_features_table(dataset_df, correlation_thresholds)\n\ncorrelated_features_table","metadata":{"_uuid":"5d3fa7f9-e02e-468e-ad16-17359cc55d3e","_cell_guid":"5e774ae9-f7ce-4790-891f-f87907f2552d","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n    <h2 class=\"card-title\" style=\"color: white;\"><strong>Note END üëÜ</strong></h2>       \n    </div>\n</div>\n","metadata":{"execution":{"iopub.status.busy":"2023-06-07T12:28:49.623244Z","iopub.execute_input":"2023-06-07T12:28:49.623708Z","iopub.status.idle":"2023-06-07T12:28:49.631426Z","shell.execute_reply.started":"2023-06-07T12:28:49.623673Z","shell.execute_reply":"2023-06-07T12:28:49.62988Z"}}},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üõ† Global:</strong></h2>\n        <ul>\n            <li>Table listing the features correlated by thresholds: <b>`correlated_features_table`</b></li>\n        </ul>\n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"----\n----\n----\n----\n----\n\n","metadata":{}},{"cell_type":"markdown","source":"# üöß TODO","metadata":{}},{"cell_type":"markdown","source":"## <a id='analysis_7_4'>8. Best feas Analysis</a>","metadata":{}},{"cell_type":"markdown","source":"### <a id='analysis_7_3'>8...\n    Univariate Analysis: RFE Best features selection</a>","metadata":{}},{"cell_type":"markdown","source":"We performs feature selection using the Recursive Feature Elimination (RFE) method to select the most important features in our medical dataset based on brain scans for predicting survival. \n\nWe explored two main configurations while taking into account feature correlations:\n\n- **Random Forest Regressor** model (Less s\nensitive to correlation and Outliers) with **RobustScaler** to handle outliers. We reduced the number of features by 10% at each iteration.\n- **Gradient Boosting Regressor** model (Less sensitive to correlation and Outliers) with **RobustScaler** to handle outliers. We reduced the number of features by 5% at each iteration.\n- We wanted to use **Lasso** with **RobustScaler**, but found many correlated features (Sensitive to correlation). (We may apply the following with the correlation filter.)\n\nThese approaches enable us to select the most informative features while considering potential correlations among them, thereby improving the performance of predictive models.","metadata":{}},{"cell_type":"code","source":"n_outliers_features_values = [40, 30, 20, 15, 10, 5]\nn_without_outliers_features_values = [10, 7, 6, 4, 3 ,2]\nrandom_state=42","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import make_regression\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfeature_importance_with_outliers = {}\n\nmax_length = 0\n\nfeatures_and_scaler_tuple_list = [\n    (outlier_feature_names, RobustScaler()),\n    (non_outlier_feature_names, StandardScaler()),\n]\n\n\nmodels = [\n    (\"Lasso Regression\", Lasso(alpha=0.8)),\n    (\"Random Forest Regressor\", RandomForestRegressor(random_state=random_state)),\n    (\"Gradient Boosting Regressor\", GradientBoostingRegressor(random_state=random_state)),\n    (\"AdaBoost Regressor\", AdaBoostRegressor(random_state=random_state)),\n    (\"XGBoost Regressor\", XGBRegressor(random_state=random_state)),\n    (\"LightGBM Regressor\", LGBMRegressor(random_state=random_state)),\n    (\"Support Vector Regressor\", SVR())\n]\n\ncounter_df = pd.DataFrame(outlier_feature_names + non_outlier_feature_names, columns=[\"features\"])\n\ncounter_df[\"count\"] = 0\n\n\ndef check_feature_importance(feature):\n     return feature in selected_features\n\n    \ncv = KFold(n_splits=5)    \n\nresults = select_best_features_RFE(\n        dataset_df, \n        \"MGMT_value\",\n        models, \n        features_and_scaler_tuple_list,\n        reduction_step=0.001,\n        cv=cv\n    )\n    \nfor name, score, selected_features in results:\n    model_name = f\"{name} - {score}\" \n    counter_df[model_name] = counter_df[\"features\"].apply(check_feature_importance)\n    counter_df[\"count\"] += counter_df[model_name].astype(int) \n    \nsorted_counter_df = counter_df.sort_values(by=\"count\", ascending=False)\ndisplay(HTML(\"<h3>Best features with outliers (RandomForestRegressor and RobustScaler):</h3>\"))\ndisplay(sorted_counter_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üìù Note:</strong></h2>\n        <ul>\n            <li>We perform feature selection using RFECV (Recursive Feature Elimination with Cross-Validated Selection): RFECV is a method that combines recursive feature selection with cross-validation. It assesses the importance of features by gradually eliminating them and using cross-validation to estimate model performance. RFECV is a robust approach for feature selection, as it takes correlations into account and is able to handle outliers. It also helps to find the optimal number of features to select using cross-validation.</li>\n            <li>The selection of the best features is performed under two scenarios:</li>\n            <ul>\n                <li>With outliers:</li>\n                <ul>\n                    <li>We utilize the Random Forest Regressor model in combination with the RobustScaler for handling outliers. This approach helps to mitigate the impact of outliers on feature importance estimation. The reduction step of 0.01 is applied, meaning 1% of features are eliminated at each iteration. We explore different numbers of features to select (40, 30, 20, 15, 10, and 5) to understand their impact on predictive performance.</li>\n                    <li>We also apply the Gradient Boosting Regressor model with the RobustScaler for feature selection in the presence of outliers. The reduction step of 0.05 is used, meaning 5% of features are eliminated at each iteration. Similarly, we investigate various numbers of features to select (40, 30, 20, 15, 10, and 5).</li>\n                </ul>\n                <li>Without outliers:</li>\n                <ul>\n                    <li>In the absence of outliers, we employ the RandomForestRegressor model with the StandardScaler for feature selection. The reduction step of 0.1 is applied, meaning 10% of features are eliminated at each iteration. We consider different numbers of features to select (15, 10, and 5) to understand their impact on model performance.</li>\n                </ul>\n            </ul>\n            <li>This analysis helps us identify the most relevant features for predicting the target variable, taking into account the presence or absence of outliers. By exploring different models, scalers, and reduction steps, we can gain insights into the impact of outliers on feature importance and optimize feature selection for our predictive models.</li>\n        </ul>\n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"card\" style=\"background-color: #007bff; border-radius: 8px; padding: 16px; color: white;\">\n    <div class=\"card-body\">\n        <h2 class=\"card-title\" style=\"color: white;\"><strong>üõ† Global:</strong></h2>\n        <ul>\n            <li>Array of important Selected Features (with outliers):<b>`feature_importance_with_outliers_rfr_df` and `feature_importance_with_outliers_gbr_df`</b></li>     \n            <li>Array of important Selected Features (no outliers):<b>`feature_importance_without_outliers_rfr_df`</b></li>               \n        </ul>\n    </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"----\n----","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\ntarget = dataset_df[\"MGMT_value\"]\nfeatures = dataset_df.drop(\"MGMT_value\", axis=1)\n\n# Supposons que vous ayez votre jeu de donn√©es dans une variable X et les √©tiquettes correspondantes dans une variable y\nX, X_test, y, y_test = train_test_split(features, target, test_size=0.2, random_state=123)\n# R√©duire la dimension en utilisant PCA\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)  # Mise √† l'√©chelle des caract√©ristiques\n\npca = PCA(n_components=0.9)  # S√©lectionner le nombre de composantes qui capturent 95% de la variance\nX_reduced = pca.fit_transform(X_scaled)\n\n# S√©lectionner les caract√©ristiques les plus importantes en utilisant la r√©gression F\nselector = SelectKBest(score_func=f_regression, k=2)  # S√©lectionner les 10 meilleures caract√©ristiques\nX_selected = selector.fit_transform(X_reduced, y)\n\n# Obtenir les indices des caract√©ristiques s√©lectionn√©es\nselected_feature_indices = selector.get_support(indices=True)\n\n# Obtenir les noms des caract√©ristiques s√©lectionn√©es\nselected_feature_names = np.array(list(X.columns))[selected_feature_indices]\n\n# Supprimer les caract√©ristiques hautement corr√©l√©es\ncorrelation_matrix = np.corrcoef(X_selected, rowvar=False)\ncorrelation_mask = np.abs(correlation_matrix) < 0.9  # D√©finir le seuil de corr√©lation ici (0.9 dans cet exemple)\ncorrelation_mask = np.all(correlation_mask, axis=0)  # S√©lectionner les caract√©ristiques qui satisfont le seuil de corr√©lation\n\nX_final = X_selected[:, correlation_mask]\n\n# Utiliser X_final pour entra√Æner votre mod√®le\n\nselected_feature_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='clean_preprocessing_8'>8. TODO</a>\n","metadata":{"_uuid":"7468dfb1-9ac7-4e0e-86c7-2f334194a4b6","_cell_guid":"c1b7ef0d-92d8-4980-b6c2-9bfee2feb700","trusted":true}},{"cell_type":"markdown","source":"# üöß TODO TEST","metadata":{}},{"cell_type":"markdown","source":"----\n----","metadata":{}},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"markdown","source":"### Copy","metadata":{}},{"cell_type":"code","source":"dataset_copy = dataset_df.copy() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_copy.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.metrics import classification_report\nimport pandas as pd\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn_pandas import gen_features, DataFrameMapper\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning","metadata":{}},{"cell_type":"code","source":"#Contrast\n#DifferenceVariance \n#DifferenceEntropy\n#DifferenceVariance \n#DifferenceEntropy\n#DifferenceAverage\n\n#LargeAreaHighGrayLevelEmphasis\n#LargeDependenceHighGrayLevelEmphasis\n#LargeAreaHighGrayLevelEmphasis\n#Kurtosis \n#Idmn\n\n#Correlation\n#LargeDependenceHighGrayLevelEmphasis\n#SumEntropy\n#HighGrayLevelEmphasis\n#LargeAreaHighGrayLevelEmphasis\n#JointAverage\n\nfeature_outliers = [\n    'Kurtosis',\n    'glrlm_LongRunHighGrayLevelEmphasis',\n    'gzlm_SizeZoneNonUniformityNormalized',\n    'MajorAxisLength',\n    'Imc2',\n    'TotalEnergy',\n    'gzlm_LargeAreaLowGrayLevelEmphasis',\n    'Idn',\n    'gzlm_SmallAreaLowGrayLevelEmphasis',\n    'gzlm_GrayLevelVariance'\n]\n\nfeature_non_outliers = ['InverseVariance',\n    'Range',\n    'MaximumProbability',\n    'Maximum3DDiameter',\n    'Maximum2DDiameterColumn',\n    'Entropy',\n    'Maximum2DDiameterRow']\n\nfeature_to_keep = [\"MGMT_value\"]\n\ndataset_copy = dataset_copy[feature_outliers + feature_non_outliers + feature_to_keep]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop features","metadata":{}},{"cell_type":"code","source":"dataset_copy.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Slipt train test (Features & Targets)","metadata":{}},{"cell_type":"code","source":"target = dataset_copy[\"MGMT_value\"]\nfeatures = dataset_copy.drop(\"MGMT_value\", axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=123)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipeline","metadata":{}},{"cell_type":"code","source":"pipeline_steps = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"code","source":"#-------------------------------#\n# Name\n#-------------------------------#\npreprocessing_name = \"preprocessing\"\n\n#-------------------------------#\n# Map\n#-------------------------------#\n\ntransformers = []\n#-------------------------------#\n# Maps all features into transformers based on their categories\n#-------------------------------#\n#transformers = map_all_features_into_transformers(all_features)\ntransformers.append((f\"ze_a_transformer\", RobustScaler(), feature_outliers))\ntransformers.append((f\"ze_b_transformer\", StandardScaler(), feature_non_outliers))\n\n#-------------------------------#\n# Create ColumnTransformer\n#-------------------------------#\npreprocessor = ColumnTransformer(transformers)\n\n#-------------------------------#\n# Pipeline first step\n#-------------------------------#\npipeline_steps.append((preprocessing_name, preprocessor))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"#-------------------------------#\n# Name\n#-------------------------------#\nmodel_name = \"model\"\n\n#-------------------------------#\n# Model\n#-------------------------------#\n# KNeighborsClassifier\n#from sklearn.neighbors import KNeighborsClassifier\n#model = KNeighborsClassifier()\n\n# DecisionTreeClassifier\n#from sklearn.tree import DecisionTreeClassifier\n#model = DecisionTreeClassifier()\n\n# SVC\n#from sklearn.svm import SVC\n#model = SVC()\n\n# DBSCAN\n#from sklearn.cluster import DBSCAN\n#model = DBSCAN()\n\n# RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n#model = RandomForestClassifier()\n\n# XGBoost\n#from xgboost import XGBClassifier\n#model = XGBClassifier()\n\n# LabelSpreading\n#from sklearn.semi_supervised import LabelSpreading\n#model = LabelSpreading()\n\n# LabelSpreading\n#from sklearn.cluster import KMeans\n#model = KMeans()\n\n# AgglomerativeClustering\n#from sklearn.cluster import AgglomerativeClustering\n#model = AgglomerativeClustering()\n\n# BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nmodel = BaggingClassifier(base_estimator=RandomForestClassifier())\n\n#-------------------------------#\n# Pipeline second step\n#-------------------------------#\npipeline_steps.append((model_name, model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline(steps=pipeline_steps)\npipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter tuning, cross-validation, optimization","metadata":{}},{"cell_type":"code","source":"# Define the parameter grid for GridSearchCV\n#param_grid = {\n#    'model__nu': [0.1, 0.2, 0.3],  # Values to try for the 'nu' parameter\n#    'model__gamma': [0.1, 0.2, 0.3]  # Values to try for the 'gamma' parameter, if applicable\n#}\n\n# KNeighborsClassifier\n#param_grid = {\n#    f'{model_name}__n_neighbors': list(range(1, 200)),  # Valeurs possibles pour le nombre de voisins\n#    f'{model_name}__weights': ['uniform', 'distance']  # Valeurs possibles pour les poids\n#}\n\n# DecisionTreeClassifier\n#param_grid = {\n#    f'{model_name}__criterion': ['gini', 'entropy'],  # Crit√®re pour mesurer la qualit√© des splits\n#    f'{model_name}__max_depth': [None, 5, 10],  # Profondeur maximale de l'arbre\n#    f'{model_name}__min_samples_split': [2, 5, 10],  # Nombre minimal d'√©chantillons requis pour effectuer un split\n#    f'{model_name}__min_samples_leaf': [1, 2, 3],  # Nombre minimal d'√©chantillons requis dans une feuille\n#    f'{model_name}__max_features': ['auto', 'sqrt', 'log2'],  # Nombre maximal de caract√©ristiques √† consid√©rer pour chaque split\n#    f'{model_name}__min_impurity_decrease': [0.0, 0.1, 0.2],  # Seuil minimal pour effectuer un split bas√© sur l'impuret√©\n#}\n\n# SVC\n#param_grid = {\n#    f'{model_name}__C': [0.1, 1.0, 10.0],  # Param√®tre de r√©gularisation C\n#    f'{model_name}__kernel': ['linear', 'rbf'],  # Noyau du mod√®le SVC\n#    f'{model_name}__gamma': ['scale', 'auto']  # Param√®tre gamma pour les noyaux rbf et poly\n#}\n\n\n#RandomForestClassifier\n#param_grid = {\n#    f\"{model_name}__criterion\": [\"gini\", \"entropy\"],\n#    f\"{model_name}__n_estimators\":  [5, 100, 200, 300],  # Nombre d'estimateurs dans le RandomForestClassifier\n#    f\"{model_name}__max_depth\":  [1, 5, 15, 50],  # Profondeur maximale des arbres dans le RandomForestClassifier\n#}\n\n# XGBClassifier\n#param_grid = {\n#    f\"{model_name}__n_estimators\": [100, 200, 300, 400, 500, 600, 700],  # Nombre d'estimateurs\n#    f\"{model_name}__max_depth\": [3, 4, 5, 6],  # Profondeur maximale des arbres\n#    f\"{model_name}__learning_rate\": [0.3, 0.2, 0.1, 0.01, 0.001]  # Taux d'apprentissage\n#}\n#param_grid = {\n    # Nombre d'estimateurs (nombre d'arbres dans le mod√®le)\n   # f\"{model_name}__n_estimators\": [50, 100, 200, 300, 350],\n\n    # Profondeur maximale des arbres\n   # f\"{model_name}__max_depth\": [3, 4, 5, 6],\n\n    # Taux d'apprentissage (contr√¥le la contribution de chaque arbre dans le mod√®le)\n    #f\"{model_name}__learning_rate\": [0.3, 0.2, 0.1, 0.01, 0.001],\n\n    # Sous-√©chantillonnage des exemples (contr√¥le la proportion d'√©chantillons utilis√©e pour la construction de chaque arbre)\n   # f\"{model_name}__subsample\": [0.6, 0.8, 1.0],\n\n    # Sous-√©chantillonnage des colonnes (features) (contr√¥le la proportion de features utilis√©e pour la construction de chaque arbre)\n    #f\"{model_name}__colsample_bytree\": [0.6, 0.8, 1.0],\n\n    # Valeur de p√©nalit√© pour les coupes dans l'arbre (contr√¥le la complexit√© de l'arbre)\n    #f\"{model_name}__gamma\": [0, 0.1, 0.2],\n\n    # Valeur de p√©nalit√© L1 pour les poids de l'arbre (contr√¥le la r√©gularisation L1)\n   # f\"{model_name}__reg_alpha\": [0, 0.1, 0.5],\n\n    # Valeur de p√©nalit√© L2 pour les poids de l'arbre (contr√¥le la r√©gularisation L2)\n    #f\"{model_name}__reg_lambda\": [0, 0.1, 0.5]\n#}\n\n\n# LabelSpreading\n#param_grid = {\n#    f\"{model_name}__kernel\": ['knn', 'rbf'],\n#    f\"{model_name}__gamma\": [0.1, 1.0, 2, 3, 4, 5, 10, 15, 20 , 25, 30],\n#    f\"{model_name}__n_neighbors\": [3, 5, 7, 5, 10, 15, 30, 50, 60, 100],\n#    f\"{model_name}__max_iter\": [40, 50, 100, 200, 300],\n#}\n\n\n# kmeans\n#param_grid = {\n#    f\"{model_name}__n_clusters\": [2],  # Nombre de clusters √† essayer\n#    f\"{model_name}__init\": ['k-means++', 'random'],  # M√©thode d'initialisation des centroides\n#    f\"{model_name}__max_iter\": [100, 200, 300, 400, 500 , 600]  # Nombre maximum d'it√©rations\n#}\n\n# agglomerative\n#param_grid = {\n#    f\"{model_name}__n_clusters\": [2],  # Nombre de clusters √† essayer\n   # f\"{model_name}__affinity\": ['euclidean', 'l1', 'l2', 'manhattan', 'cosine'],  # M√©trique de distance utilis√©e\n   # f\"{model_name}__linkage\": ['ward', 'complete', 'average', 'single'],  # M√©thode de liaison pour former les clusters\n   # 'agglomerative__connectivity': [None, nearest_neighbors_graph],  # Matrice de connectivit√© pour contraindre les regroupements\n   # 'agglomerative__distance_threshold': [None, 0.5, 1.0, 1.5]  # Seuil de distance pour former les clusters\n#}\n\n\n# f\"{model_name}__n_estimators\": [50, 100, 200],\n# Nombre d'estimateurs dans le BaggingClassifier.\n# Plus le nombre d'estimateurs est √©lev√©, plus le mod√®le est robuste, mais cela augmente le temps d'entra√Ænement.\n\n# f\"{model_name}__max_samples\": [0.5, 0.8, 1.0],\n# La proportion des √©chantillons d'entra√Ænement √† utiliser pour chaque estimateur.\n# Une valeur inf√©rieure √† 1.0 cr√©e des √©chantillons d'entra√Ænement bootstrap.\n\n# f\"{model_name}__max_features\": [0.5, 0.8, 1.0],\n# La proportion des caract√©ristiques √† utiliser pour chaque estimateur.\n# Une valeur inf√©rieure √† 1.0 effectue une s√©lection al√©atoire des caract√©ristiques.\n\n# f\"{model_name}__base_estimator__max_depth\": [None, 5, 10],\n# La profondeur maximale de chaque arbre de d√©cision de RandomForestClassifier.\n# Une valeur None signifie que les arbres sont d√©velopp√©s jusqu'√† ce que toutes les feuilles soient pures ou jusqu'√† ce que toutes les feuilles contiennent un nombre minimum d'√©chantillons.\n\n# f\"{model_name}__base_estimator__min_samples_split\": [2, 5, 10],\n# Le nombre minimum d'√©chantillons requis pour scinder un n≈ìud interne dans chaque arbre de d√©cision.\n# Une valeur plus √©lev√©e peut emp√™cher le surapprentissage en obligeant les arbres √† avoir un nombre minimum d'√©chantillons dans chaque n≈ìud.\n\n# f\"{model_name}__base_estimator__min_samples_leaf\": [1, 2, 4]\n# Le nombre minimum d'√©chantillons requis dans une feuille d'arbre.\n# Une valeur plus √©lev√©e peut √©galement aider √† pr√©venir le surapprentissage en limitant la croissance de l'arbre.\n\n# Define the parameter grid for GridSearchCV\nparam_grid = {\n    f\"{model_name}__n_estimators\": [50, 100, 200],\n    f\"{model_name}__max_samples\": [0.5, 0.8, 1.0],\n    f\"{model_name}__max_features\": [0.5, 0.8, 1.0],\n    f\"{model_name}__base_estimator__max_depth\": [None, 5, 10],\n    f\"{model_name}__base_estimator__min_samples_split\": [2, 5, 10],\n    f\"{model_name}__base_estimator__min_samples_leaf\": [1, 2, 4]\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Search Hyperparame","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\ncross_validator = KFold(n_splits=10)\n\ngrid_search = GridSearchCV(pipeline, param_grid, cv=cross_validator, error_score='raise')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.fit(X_train , y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Score","metadata":{}},{"cell_type":"code","source":"# Get the best parameters and best score from GridSearchCV\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\nprint(best_params, \"\\n\")\nprint(best_score, \"\\n\")\n \n# Obtenez les pr√©dictions du meilleur mod√®le sur X_train\ny_train_pred = grid_search.best_estimator_.predict(X_train)\n# G√©n√©rez le rapport de classification pour les pr√©dictions sur X_train\nclassification_rep = classification_report(y_train, y_train_pred)\nprint(classification_rep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict\n","metadata":{}},{"cell_type":"code","source":"# Evaluate the predictions\naccuracy = grid_search.score(X_test, y_test)\n# Affichez le score d'exactitude\nprint(\"Accuracy Score on Test Set:\", accuracy, \"\\n\")\n\n# Use the best model for predictions\nbest_model = grid_search.best_estimator_\npredictions = best_model.predict(X_test)\n\n# Evaluate the predictions\nprint(classification_report(y_test, predictions))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"code","source":"import itertools\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adamax\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import KFold\nfrom keras.utils import to_categorical\n\n'''\ndef build_model(layer_size, activation, dropout):\n    model = Sequential()\n    model.add(Dense(layer_size, activation=activation, input_shape=(input_dim,)))\n    model.add(Dropout(dropout))\n    model.add(Dense(num_classes, activation='softmax'))\n    return model\n'''\n\ndef build_model(layer_size, activation, dropout):\n    model = Sequential()\n    model.add(Dense(layer_size, activation=activation, input_shape=(input_dim,)))\n    model.add(Dropout(dropout))\n    model.add(BatchNormalization())\n    model.add(Dense(layer_size, activation=activation))\n    model.add(Dropout(dropout))\n    model.add(BatchNormalization())\n    #model.add(Dense(num_classes, activation='softmax'))\n    model.add(Dense(num_classes, activation='sigmoid'))\n    return model\n\nnum_classes = 2\n\ndataset_copy = dataset_df.copy() \n\nfeature_to_keep = [\"MGMT_value\"]\n\nfeature_outliers = ['glrlm_LongRunHighGrayLevelEmphasis','MajorAxisLength','gzlm_SizeZoneNonUniformityNormalized','Kurtosis','Idn',\n'Idmn','gldm_LargeDependenceHighGrayLevelEmphasis','Mean','MinorAxisLenth','glrlm_RunPercentage']\nfeature_non_outliers = ['InverseVariance','Maximum3DDiameter']\n\ndataset_copy = dataset_copy[feature_outliers + feature_non_outliers + feature_to_keep]\n\ntarget = dataset_copy[\"MGMT_value\"]\nfeatures = dataset_copy.drop(\"MGMT_value\", axis=1)\n\n# Encodage One-Hot des √©tiquettes\nonehot_encoder = OneHotEncoder(sparse=False)\ntarget_encoded = onehot_encoder.fit_transform(target.values.reshape(-1, 1))\n\n# Convertir le codage one-hot en classe binaire\ny_binary = np.argmax(target_encoded, axis=1)\n\n# Compter les occurrences de chaque classe dans y_binary\nclass_counts = np.bincount(y_binary)\n\n# Afficher les r√©partitions des classes\nfor cls, count in enumerate(class_counts):\n    print(f\"Classe {cls}: {count} √©chantillons\")\n\n# Diviser les donn√©es en ensembles d'entra√Ænement et de test\nX_train, X_test, y_train, y_test = train_test_split(\n    features, y_binary, test_size=0.2, random_state=42\n)\n\n\n# Cr√©ation d'une instance du sur-√©chantillonneur RandomOverSampler\noversampler = RandomOverSampler(random_state=42)\n\n# Appliquer le sur-√©chantillonnage sur vos donn√©es\nX_train, y_train = oversampler.fit_resample(X_train, y_train)\n\n# Normaliser les donn√©es d'entra√Ænement et de test\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Dimension d'entr√©e du mod√®le\ninput_dim = X_train.shape[1]\n\n# Liste des param√®tres √† tester\n# Meilleurs param√®tres : (512, 'relu', 0.2, 256, 0.01)\n# Meilleure pr√©cision : 0.598313745856285\n\n# AUTRE\n#Accuracy: 0.5555555555555556 | Parameters: 512, relu, 1e-05, 512, 0.001\n#Best Parameters: {'layer_size': 512, 'activation': 'relu', 'dropout': 1e-05, 'batch_size': 512, 'learning_rate': 0.001}\n#Confusion Matrix:\n#array([[ 3, 49],\n#       [ 3, 62]])\n\n#Meilleurs param√®tres : (512, 'relu', 1e-05, 256, 0.001)\n#Meilleure pr√©cision : 0.5630980551242828\n#Confusion Matrix:\n#array([[122, 132],\n#       [138, 116]])\n'''\nlayer_sizes = [512]\nactivations = ['relu']\ndropouts = [0.2]\nbatch_sizes = [256]\nlearning_rates = [0.01]\n'''\n'''\nlayer_sizes = [512]\nactivations = ['relu']\ndropouts = [1e-05]\nbatch_sizes = [512]\nlearning_rates = [0.001]\n'''\n\nlayer_sizes = [64]#, 128, 256, 512]\nactivations = ['relu','sigmoid']#,'leakyrelu']\ndropouts = [1e-05, 0.0001,0.001]#, 0.01, 0.1, 0.2,0.5,0.8]\nbatch_sizes = [16,32,64]#,128,256,512]\nlearning_rates = [0.0001,0.001,0.01]#, 0.1,0.2]\n\nepochs = 10\n\n# Effectuer une recherche par grille\nbest_accuracy = 0.0\nbest_params = {}\nconfusion_matrix_best = None\n\nkf = KFold(n_splits=10, shuffle=True)  # Utilisation de la validation crois√©e avec 10 plis\n\nfor params in itertools.product(layer_sizes, activations, dropouts, batch_sizes, learning_rates):\n    layer_size, activation, dropout, batch_size, learning_rate = params\n\n    accuracies = []  # Stocker les pr√©cisions pour chaque pli\n    y_pred_fold = []  # Stocker les pr√©dictions pour chaque pli\n\n\n    for train_index, val_index in kf.split(X_train):\n        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n\n        # Convertir les √©tiquettes en encodage one-hot\n        y_train_fold = to_categorical(y_train_fold)\n        y_val_fold = to_categorical(y_val_fold)\n        \n        # Construire et entra√Æner le mod√®le avec les param√®tres actuels\n        model = build_model(layer_size, activation, dropout)\n        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n        #model.compile(optimizer=Adamax(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n        \n        history = model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, validation_data=(X_val_fold, y_val_fold), verbose=0)\n\n        # √âvaluer les performances du mod√®le sur l'ensemble de validation actuel\n        accuracy = model.evaluate(X_val_fold, y_val_fold)[1]\n        accuracies.append(accuracy)\n        \n        # Obtenir les pr√©dictions sur l'ensemble de validation actuel\n        y_pred = model.predict(X_val_fold)\n        y_pred = np.argmax(y_pred, axis=1)\n        y_pred_fold.extend(y_pred)\n\n    # Calculer la moyenne des pr√©cisions sur tous les plis\n    mean_accuracy = sum(accuracies) / len(accuracies)\n\n    # V√©rifier si les performances actuelles sont les meilleures\n    if mean_accuracy > best_accuracy:\n        best_accuracy = mean_accuracy\n        best_params = params\n        y_pred_best = np.array(y_pred_fold)\n        confusion_matrix_best = confusion_matrix(y_train, y_pred_best)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Afficher les meilleurs param√®tres et la meilleure pr√©cision\nprint(\"Meilleurs param√®tres :\", best_params)\nprint(\"Meilleure pr√©cision :\", best_accuracy)\n\nlayer_size, activation, dropout, batch_size, learning_rate = best_params\n# Construire et entra√Æner le mod√®le avec les param√®tres actuels\nmodel = build_model(layer_size, activation, dropout)\nmodel.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train, to_categorical(y_train), batch_size=batch_size, epochs=100)\n\ny_pred = np.argmax(model.predict(X_test), axis=1)\nprint(\"Confusion Matrix:\")\ndisplay(confusion_matrix(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}